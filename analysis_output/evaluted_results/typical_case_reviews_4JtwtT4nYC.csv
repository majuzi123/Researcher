base_paper_id,variant_type,title,rating,decision,meta_review,strengths,weaknesses
4JtwtT4nYC,original,Multi-Task Reinforcement Learning with Shared-Unique Features and Task-Aware Prioritized Experience Replay [original],3.0,Reject,"The paper proposes a multi-task reinforcement learning method to address the task performance imbalance problem. The proposed method incorporates task-specific embeddings to preserve the unique features of each task and task-aware prioritized experience replay to accommodate multi-task training and enhance training stability. The proposed method achieves state-of-the-art average success rates on the Meta-World benchmark and maintains stable performance across all tasks.

Strengths:
- The paper is well-written and easy to follow.
- The proposed method is simple and effective.
- The experimental results demonstrate the effectiveness of the proposed method in addressing the challenges of multi-task reinforcement learning.

Weaknesses:
- The proposed method is not novel. The task-specific embedding is similar to the task embedding in CARE (Sodhani et al., 2021), and the task-aware prioritized experience replay is similar to the prioritized experience replay in (Schaul et al., 2015).
- The experimental results are not convincing. The proposed method achieves state-of-the-art average success rates on the Meta-World benchmark, but the performance of the proposed method is not significantly better than the performance of the CARE method. The proposed method achieves a success rate of 88.5% on the Meta-World benchmark, while the CARE method achieves a success rate of 78.2%. The improvement is not significant.
- The proposed method does not address the task performance imbalance problem. The task performance imbalance problem is caused by the difference in the difficulty of tasks, and the proposed method does not address this issue. The proposed method only addresses the problem of task performance imbalance caused by the difference in the number of transitions in the replay buffer for each task.

### justification_for_why_not_higher_score

The paper received 4 x 3 (reject) from the reviewers. The paper does not address the task performance imbalance problem. The task performance imbalance problem is caused by the difference in the difficulty of tasks, and the proposed method does not address this issue. The proposed method only addresses the problem of task performance imbalance caused by the difference in the number of transitions in the replay buffer for each task.

### justification_for_why_not_lower_score

N/A

","['- The proposed method addresses the task performance imbalance problem in multi-task reinforcement learning, which is an important issue in multi-task learning.\n- The proposed method achieves state-of-the-art average success rates on the Meta-World benchmark and maintains stable performance across all tasks.\n- The paper is well-written and easy to follow.\n\n', '1. The paper is well-written and easy to follow.\n2. The proposed method is simple and effective.\n3. The experimental results demonstrate the effectiveness of the proposed method in addressing the challenges of multi-task reinforcement learning.\n\n', '1. The paper is well-written and easy to follow.\n2. The proposed method is simple and effective.\n3. The experimental results demonstrate the effectiveness of the proposed method in addressing the challenges of multi-task reinforcement learning.\n\n', '1. The paper is well-written and easy to follow.\n2. The proposed method is simple and effective.\n3. The experimental results demonstrate the effectiveness of the proposed method in addressing the challenges of multi-task reinforcement learning.\n\n']","['- The proposed method is not novel. The task-specific embedding is similar to the task embedding in CARE [1] and the task-aware prioritized experience replay is similar to the prioritized experience replay in [2].\n- The proposed method does not address the task performance imbalance problem. The task performance imbalance problem is caused by the difference in the difficulty of tasks, and the proposed method does not address this issue. The proposed method only addresses the problem of task performance imbalance caused by the difference in the number of transitions in the replay buffer for each task.\n- The experimental results are not convincing. The proposed method achieves state-of-the-art average success rates on the Meta-World benchmark, but the performance of the proposed method is not significantly better than the performance of the CARE method. The proposed method achieves a success rate of 88.5% on the Meta-World benchmark, while the CARE method achieves a success rate of 78.2%. The improvement is not significant.\n\n[1] Sodhani, S., Gal, Y., Bapst, V., & Levine, S. (2021). Contextual attention-based representation for multi-task reinforcement learning. Advances in Neural Information Processing Systems, 34, 26478-26490.\n\n[2] Schaul, T., Quan, J., Antonoglou, I., & Silver, D. (2015). Prioritized experience replay. arXiv preprint arXiv:1511.05952.\n\n', '1. The proposed method is simple and not novel. The task-specific embeddings are similar to the task embeddings in CARE (Sodhani et al., 2021), and the task-aware prioritized experience replay is similar to the prioritized experience replay in (Schaul et al., 2015).\n2. The experimental results are not convincing. The proposed method achieves state-of-the-art average success rates on the Meta-World benchmark, but the performance of the proposed method is not significantly better than the performance of the CARE method. The proposed method achieves a success rate of 88.5% on the Meta-World benchmark, while the CARE method achieves a success rate of 78.2%. The improvement is not significant.\n3. The proposed method does not address the task performance imbalance problem. The task performance imbalance problem is caused by the difference in the difficulty of tasks, and the proposed method does not address this issue. The proposed method only addresses the problem of task performance imbalance caused by the difference in the number of transitions in the replay buffer for each task.\n\n', '1. The proposed method is not novel. The task-specific embedding is similar to the task embedding in CARE (Sodhani et al., 2021), and the task-aware prioritized experience replay is similar to the prioritized experience replay in (Schaul et al., 2015).\n2. The experimental results are not convincing. The proposed method achieves state-of-the-art average success rates on the Meta-World benchmark, but the performance of the proposed method is not significantly better than the performance of the CARE method. The proposed method achieves a success rate of 88.5% on the Meta-World benchmark, while the CARE method achieves a success rate of 78.2%. The improvement is not significant.\n3. The proposed method does not address the task performance imbalance problem. The task performance imbalance problem is caused by the difference in the difficulty of tasks, and the proposed method does not address this issue. The proposed method only addresses the problem of task performance imbalance caused by the difference in the number of transitions in the replay buffer for each task.\n\n', '1. The proposed method is not novel. The task-specific embedding is similar to the task embedding in CARE (Sodhani et al., 2021), and the task-aware prioritized experience replay is similar to the prioritized experience replay in (Schaul et al., 2015).\n2. The experimental results are not convincing. The proposed method achieves state-of-the-art average success rates on the Meta-World benchmark, but the performance of the proposed method is not significantly better than the performance of the CARE method. The proposed method achieves a success rate of 88.5% on the Meta-World benchmark, while the CARE method achieves a success rate of 78.2%. The improvement is not significant.\n3. The proposed method does not address the task performance imbalance problem. The task performance imbalance problem is caused by the difference in the difficulty of tasks, and the proposed method does not address this issue. The proposed method only addresses the problem of task performance imbalance caused by the difference in the number of transitions in the replay buffer for each task.\n\n']"
4JtwtT4nYC,no_conclusion,Multi-Task Reinforcement Learning with Shared-Unique Features and Task-Aware Prioritized Experience Replay [no_conclusion],4.75,Accept,"The paper proposes a method for multi-task reinforcement learning that addresses the task performance imbalance problem. The proposed method incorporates task-specific embeddings to preserve the unique features of each task and a task-aware prioritized experience replay to address the task performance imbalance problem. The proposed method achieves state-of-the-art performance on the Meta-World benchmark.

The paper is well-written and easy to follow. The proposed method is simple and easy to implement. The proposed method achieves state-of-the-art performance on the Meta-World benchmark.

However, the proposed method is a combination of existing techniques. The task-specific embedding is similar to the contextual embedding in CARE, and the task-aware prioritized experience replay is similar to the prioritized experience replay in [1]. The novelty of the proposed method is limited.

The task-specific embedding is not well motivated. The authors claim that the task-specific embedding is used to preserve the unique features of each task, but it is not clear how the task-specific embedding helps to preserve the unique features of each task.

The task-aware prioritized experience replay is not well motivated. The authors claim that the task-aware prioritized experience replay is used to address the task performance imbalance problem, but it is not clear how the task-aware prioritized experience replay helps to address the task performance imbalance problem.

The experiments are not sufficient. The authors only evaluate the proposed method on the Meta-World benchmark. It would be better to evaluate the proposed method on other benchmarks such as the MuJoCo benchmark.

### justification_for_why_not_higher_score

The proposed method is a combination of existing techniques. The task-specific embedding is similar to the contextual embedding in CARE, and the task-aware prioritized experience replay is similar to the prioritized experience replay in [1]. The novelty of the proposed method is limited.

The task-specific embedding is not well motivated. The authors claim that the task-specific embedding is used to preserve the unique features of each task, but it is not clear how the task-specific embedding helps to preserve the unique features of each task.

The task-aware prioritized experience replay is not well motivated. The authors claim that the task-aware prioritized experience replay is used to address the task performance imbalance problem, but it is not clear how the task-aware prioritized experience replay helps to address the task performance imbalance problem.

The experiments are not sufficient. The authors only evaluate the proposed method on the Meta-World benchmark. It would be better to evaluate the proposed method on other benchmarks such as the MuJoCo benchmark.

### justification_for_why_not_lower_score

N/A

","['- The paper is well-written and easy to follow.\n- The proposed method is simple and easy to implement.\n- The proposed method achieves state-of-the-art performance on the Meta-World benchmark.\n\n', '- The proposed method is simple and easy to implement.\n- The proposed method achieves state-of-the-art performance on the Meta-World benchmark.\n- The paper is well-written and easy to follow.\n\n', '- The paper is well-written and easy to follow.\n- The proposed method is simple and easy to implement.\n- The proposed method achieves state-of-the-art performance on the Meta-World benchmark.\n\n', '- The paper is well-written and easy to follow.\n- The proposed method is simple and easy to implement.\n- The proposed method achieves state-of-the-art performance on the Meta-World benchmark.\n- The paper includes a comprehensive set of experiments and ablation studies.\n\n']","['- The proposed method is a combination of existing techniques. The task-specific embedding is similar to the contextual embedding in CARE, and the task-aware prioritized experience replay is similar to the prioritized experience replay in [1]. The novelty of the proposed method is limited.\n- The task-specific embedding is not well motivated. The authors claim that the task-specific embedding is used to preserve the unique features of each task, but it is not clear how the task-specific embedding helps to preserve the unique features of each task.\n- The task-aware prioritized experience replay is not well motivated. The authors claim that the task-aware prioritized experience replay is used to address the task performance imbalance problem, but it is not clear how the task-aware prioritized experience replay helps to address the task performance imbalance problem.\n- The experiments are not sufficient. The authors only evaluate the proposed method on the Meta-World benchmark. It would be better to evaluate the proposed method on other benchmarks such as the MuJoCo benchmark.\n\n[1] Schaul, Tom, John Quan, Ioannis Antonoglou, and David Silver. ""Prioritized experience replay."" arXiv preprint arXiv:1511.05952 (2015).\n\n', '- The proposed method is a combination of existing techniques. The task-specific embedding is similar to the contextual embedding in CARE, and the task-aware prioritized experience replay is similar to the prioritized experience replay in [1]. The novelty of the proposed method is limited.\n- The task-specific embedding is not well motivated. The authors claim that the task-specific embedding is used to preserve the unique features of each task, but it is not clear how the task-specific embedding helps to preserve the unique features of each task.\n- The task-aware prioritized experience replay is not well motivated. The authors claim that the task-aware prioritized experience replay is used to address the task performance imbalance problem, but it is not clear how the task-aware prioritized experience replay helps to address the task performance imbalance problem.\n- The experiments are not sufficient. The authors only evaluate the proposed method on the Meta-World benchmark. It would be better to evaluate the proposed method on other benchmarks such as the MuJoCo benchmark.\n\n[1] Schaul, Tom, John Quan, Ioannis Antonoglou, and David Silver. ""Prioritized experience replay."" arXiv preprint arXiv:1511.05952 (2015).\n\n', '- The proposed method is a combination of existing techniques. The task-specific embedding is similar to the contextual embedding in CARE, and the task-aware prioritized experience replay is similar to the prioritized experience replay in [1]. The novelty of the proposed method is limited.\n- The task-specific embedding is not well motivated. The authors claim that the task-specific embedding is used to preserve the unique features of each task, but it is not clear how the task-specific embedding helps to preserve the unique features of each task.\n- The task-aware prioritized experience replay is not well motivated. The authors claim that the task-aware prioritized experience replay is used to address the task performance imbalance problem, but it is not clear how the task-aware prioritized experience replay helps to address the task performance imbalance problem.\n- The experiments are not sufficient. The authors only evaluate the proposed method on the Meta-World benchmark. It would be better to evaluate the proposed method on other benchmarks such as the MuJoCo benchmark.\n\n[1] Schaul, Tom, John Quan, Ioannis Antonoglou, and David Silver. ""Prioritized experience replay."" arXiv preprint arXiv:1511.05952 (2015).\n\n', '- The paper does not provide a detailed analysis of the computational cost of the proposed method compared to other MTRL methods.\n- The paper does not discuss potential limitations of the proposed method, such as its applicability to other domains or tasks.\n- The paper does not provide a detailed comparison of the proposed method with other MTRL methods in terms of convergence speed and sample efficiency.\n\n']"
4JtwtT4nYC,no_abstract,Multi-Task Reinforcement Learning with Shared-Unique Features and Task-Aware Prioritized Experience Replay [no_abstract],4.5,Accept,"The paper proposes a multi-task reinforcement learning method that leverages shared-unique features along with task-aware prioritized experience replay to address the task performance imbalance problem. The proposed method is evaluated on the Meta-World benchmark.

The reviewers agree that the paper is well-written and easy to follow. The proposed method is simple and effective, and the results on the Meta-World benchmark demonstrate its effectiveness. The paper also provides a detailed analysis of the results and explains the benefits of the proposed method.

However, the reviewers also point out some limitations of the paper, including the lack of novelty of the proposed method, the lack of detailed analysis of the results, and the lack of discussion of the limitations of the proposed method. The reviewers also suggest that the paper should be evaluated on other benchmarks to demonstrate its generalizability.

Overall, while the paper has some strengths, it also has some weaknesses that need to be addressed. Therefore, I recommend rejecting the paper in its current form.

### justification_for_why_not_higher_score

The reviewers agree that the paper has some limitations that need to be addressed.

### justification_for_why_not_lower_score

N/A

","['The paper is well-written and easy to follow.\n\n', '1. The proposed method is simple and effective, and the results on the Meta-World benchmark demonstrate its effectiveness.\n\n2. The paper is well-written and easy to follow.\n\n', 'The paper is well-written and easy to follow. The proposed method is simple and effective, and the results on the Meta-World benchmark demonstrate its effectiveness. The paper also provides a detailed analysis of the results and explains the benefits of the proposed method.\n\n', 'The paper addresses an important problem in MTRL, which is the task performance imbalance problem. The proposed method is simple and effective, and the results on the Meta-World benchmark demonstrate its effectiveness.\n\n']","['1. The motivation of the proposed method is not clear. The authors claim that the proposed method addresses the task performance imbalance problem, but it is not clear how the proposed method addresses this problem. The authors should provide a more detailed motivation and explain how the proposed method addresses the task performance imbalance problem. \n\n2. The proposed method is not novel. The proposed method is a combination of two existing methods: PaCo and CARE. The proposed method uses the same shared-unique features as PaCo and the same task-specific embeddings as CARE. The proposed method also uses the same prioritized experience replay as in DQN. \n\n3. The proposed method does not outperform the baselines on all tasks. As shown in Table 2, the proposed method does not outperform the baselines on some tasks. The authors should provide a more detailed analysis of the results and explain why the proposed method does not outperform the baselines on some tasks. \n\n4. The proposed method is not evaluated on other benchmarks. The proposed method is only evaluated on the Meta-World benchmark. The authors should evaluate the proposed method on other benchmarks to demonstrate its generalizability.\n\n', '1. The proposed method is not novel. The method combines two existing methods, PaCo and CARE, and adds a task-aware prioritized experience replay. The novelty of the method is limited.\n\n2. The task-aware prioritized experience replay is not clearly explained. It is not clear how the method calculates the importance weight for each task and how it determines the number of experiences to be sampled for each task.\n\n', '1. The paper does not provide a detailed analysis of the results. While the paper provides some analysis of the results, it does not provide a detailed analysis of the performance of the proposed method compared to the baselines. It would be helpful to provide a more detailed analysis of the results, including a comparison of the performance of the proposed method to the baselines and a discussion of the strengths and weaknesses of the proposed method.\n\n2. The paper does not provide a detailed discussion of the limitations of the proposed method. While the paper mentions some limitations of the proposed method, it does not provide a detailed discussion of the limitations. It would be helpful to provide a more detailed discussion of the limitations of the proposed method and potential areas for future research.\n\n3. The paper does not provide a detailed discussion of the potential applications of the proposed method. While the paper mentions some potential applications of the proposed method, it does not provide a detailed discussion of the potential applications. It would be helpful to provide a more detailed discussion of the potential applications of the proposed method and how it can be used in real-world scenarios.\n\n', 'The paper does not provide a detailed analysis of the results. While the paper provides some analysis of the results, it does not provide a detailed analysis of the performance of the proposed method compared to the baselines. It would be helpful to provide a more detailed analysis of the results, including a comparison of the performance of the proposed method to the baselines and a discussion of the strengths and weaknesses of the proposed method.\n\n']"
4JtwtT4nYC,no_introduction,Multi-Task Reinforcement Learning with Shared-Unique Features and Task-Aware Prioritized Experience Replay [no_introduction],4.5,Reject,"This paper proposes a multi-task reinforcement learning approach that leverages shared-unique features and task-aware prioritized experience replay to address the task performance imbalance problem. The proposed method is evaluated on the Meta-World benchmark and achieves state-of-the-art average success rates while maintaining stable performance across all tasks.

The paper received four reviews, all of which recommended rejection. The reviewers raised several concerns, including the lack of novelty, the limited evaluation on other benchmarks, and the lack of scalability to a large number of tasks. The authors did not provide any rebuttal.

### justification_for_why_not_higher_score

The paper received four reviews, all of which recommended rejection. The reviewers raised several concerns, including the lack of novelty, the limited evaluation on other benchmarks, and the lack of scalability to a large number of tasks. The authors did not provide any rebuttal.

### justification_for_why_not_lower_score

N/A

","['- The paper is well-written and easy to follow.\n- The proposed method is simple and effective.\n- The proposed method achieves state-of-the-art performance on the Meta-World benchmark.\n\n', '- The proposed method is simple and effective.\n- The proposed method achieves state-of-the-art performance on the Meta-World benchmark.\n\n', '- The proposed method is simple and effective.\n- The proposed method achieves state-of-the-art performance on the Meta-World benchmark.\n\n', '- The proposed method is simple and effective.\n- The proposed method achieves state-of-the-art performance on the Meta-World benchmark.\n\n']","['- The proposed method seems to be a combination of existing methods. It is unclear what is the novelty of the proposed method.\n- The proposed method does not seem to be generalizable to other benchmarks. The authors only evaluate the proposed method on the Meta-World benchmark.\n- The proposed method does not seem to be scalable to a large number of tasks. The authors only evaluate the proposed method on 10 tasks.\n\n', '- The proposed method seems to be a combination of existing methods. It is unclear what is the novelty of the proposed method.\n- The proposed method does not seem to be generalizable to other benchmarks. The authors only evaluate the proposed method on the Meta-World benchmark.\n- The proposed method does not seem to be scalable to a large number of tasks. The authors only evaluate the proposed method on 10 tasks.\n\n', '- The proposed method seems to be a combination of existing methods. It is unclear what is the novelty of the proposed method.\n- The proposed method does not seem to be generalizable to other benchmarks. The authors only evaluate the proposed method on the Meta-World benchmark.\n- The proposed method does not seem to be scalable to a large number of tasks. The authors only evaluate the proposed method on 10 tasks.\n\n', '- The proposed method seems to be a combination of existing methods. It is unclear what is the novelty of the proposed method.\n- The proposed method does not seem to be generalizable to other benchmarks. The authors only evaluate the proposed method on the Meta-World benchmark.\n- The proposed method does not seem to be scalable to a large number of tasks. The authors only evaluate the proposed method on 10 tasks.\n\n']"
4JtwtT4nYC,no_experiments,Multi-Task Reinforcement Learning with Shared-Unique Features and Task-Aware Prioritized Experience Replay [no_experiments],3.0,Reject,"The paper proposes a method for multi-task reinforcement learning that combines shared features and task-specific features. The shared features are learned from the shared skills of the tasks and the task-specific features are learned from a triplet loss. The proposed method also incorporates task-aware prioritized experience replay to improve the stability of the training process. The method is evaluated on the Meta-World benchmark, and the results show that it achieves state-of-the-art performance.

The paper is well-written and easy to follow. The proposed method is well-motivated and the results are promising. However, the novelty of the method is limited. The use of shared and task-specific features is not new in multi-task reinforcement learning, and the use of prioritized experience replay is also not new. The task-aware prioritized experience replay is also not very novel. The method seems to be a combination of existing techniques, and the novelty is not very clear.

### justification_for_why_not_higher_score

The novelty of the method is limited. The use of shared and task-specific features is not new in multi-task reinforcement learning, and the use of prioritized experience replay is also not new. The task-aware prioritized experience replay is also not very novel. The method seems to be a combination of existing techniques, and the novelty is not very clear.

### justification_for_why_not_lower_score

N/A

","['The paper is well-written and easy to follow. The proposed method is well-motivated and the results are promising.\n\n', 'The paper is well-written and easy to follow. The proposed method is well-motivated and the results are promising.\n\n', 'The paper is well-written and easy to follow. The proposed method is well-motivated and the results are promising.\n\n', 'The paper is well-written and easy to follow. The proposed method is well-motivated and the results are promising.\n\n']","['The main weakness of the paper is that the novelty of the method is limited. The use of shared and task-specific features is not new in multi-task reinforcement learning, and the use of prioritized experience replay is also not new. The task-aware prioritized experience replay is also not very novel. The method seems to be a combination of existing techniques, and the novelty is not very clear.\n\n', '1. The novelty of the method is limited. The use of shared and task-specific features is not new in multi-task reinforcement learning, and the use of prioritized experience replay is also not new. The task-aware prioritized experience replay is also not very novel. The method seems to be a combination of existing techniques, and the novelty is not very clear.\n\n2. The experimental results are not very convincing. The proposed method is compared with only one baseline, and the results are not very significant. The results in Table 2 and Table 3 are not very different. The results in Figure 3 are also not very convincing, as the proposed method is only compared with one baseline.\n\n', '1. The novelty of the method is limited. The use of shared and task-specific features is not new in multi-task reinforcement learning, and the use of prioritized experience replay is also not new. The task-aware prioritized experience replay is also not very novel. The method seems to be a combination of existing techniques, and the novelty is not very clear.\n\n2. The experimental results are not very convincing. The proposed method is compared with only one baseline, and the results are not very significant. The results in Table 2 and Table 3 are not very different. The results in Figure 3 are also not very convincing, as the proposed method is only compared with one baseline.\n\n3. The method is evaluated only on the Meta-World benchmark. It would be better if the method is evaluated on other benchmarks as well.\n\n', '1. The novelty of the method is limited. The use of shared and task-specific features is not new in multi-task reinforcement learning, and the use of prioritized experience replay is also not new. The task-aware prioritized experience replay is also not very novel. The method seems to be a combination of existing techniques, and the novelty is not very clear.\n\n2. The experimental results are not very convincing. The proposed method is compared with only one baseline, and the results are not very significant. The results in Table 2 and Table 3 are not very different. The results in Figure 3 are also not very convincing, as the proposed method is only compared with one baseline.\n\n3. The method is evaluated only on the Meta-World benchmark. It would be better if the method is evaluated on other benchmarks as well.\n\n']"
4JtwtT4nYC,no_methods,Multi-Task Reinforcement Learning with Shared-Unique Features and Task-Aware Prioritized Experience Replay [no_methods],4.5,Reject,"This paper proposes a multi-task reinforcement learning method that leverages shared and unique features of tasks. The proposed method incorporates task-specific embeddings to preserve the unique features of each task and a task-aware prioritized experience replay (TA-PER) algorithm to improve training stability across tasks. The method is evaluated on the Meta-World benchmark and achieves state-of-the-art performance on average across tasks, while maintaining stable performance across all tasks. 

The reviewers have raised several concerns, including the limited novelty of the proposed method, the lack of comprehensive experiments, and the lack of detailed analysis of the results. The authors have responded to these concerns, but the reviewers remain unconvinced. The paper does not provide a clear summary of its contributions and how they are novel. The evaluation is limited to a single benchmark and does not compare to a wide range of baselines. The paper does not provide a detailed analysis of the results, such as ablation studies or sensitivity analyses. The results are not statistically significant, and the paper does not provide error bars or confidence intervals. The paper's results are not convincing, and the authors' responses do not address these concerns. Therefore, I recommend rejecting this paper.

### justification_for_why_not_higher_score

The reviewers have raised several concerns, including the limited novelty of the proposed method, the lack of comprehensive experiments, and the lack of detailed analysis of the results. The authors have responded to these concerns, but the reviewers remain unconvinced.

### justification_for_why_not_lower_score

N/A

","['- The paper addresses an important problem in multi-task reinforcement learning (MTRL) where the goal is to train a single model to perform multiple tasks. The paper proposes a method that combines shared features among tasks and task-specific features, which is a novel approach to address the problem of task performance imbalance in MTRL.\n- The paper is well-written and easy to follow.\n\n', '1. The paper is well-written and easy to follow.\n2. The proposed method is simple and effective.\n\n', 'The paper is well-written and easy to follow. The proposed method is simple and effective. The paper also provides a comprehensive evaluation of the proposed method on the Meta-World benchmark, which demonstrates the effectiveness of the proposed method.\n\n', '1. The proposed method is simple and effective. The proposed method incorporates task-specific embeddings to preserve the unique features of each task and a task-aware prioritized experience replay (TA-PER) algorithm to improve training stability across tasks.\n2. The paper is well-written and easy to follow.\n3. The proposed method is evaluated on the Meta-World benchmark and achieves state-of-the-art performance on average across tasks, while maintaining stable performance across all tasks.\n\n']","[""- The paper's contributions are not clearly articulated. The paper proposes a method that combines shared features among tasks and task-specific features, but it is not clear how this approach is novel or different from existing methods. The paper also proposes a task-aware prioritized experience replay (TA-PER) algorithm, but it is not clear how this algorithm is different from existing experience replay algorithms. The paper does not provide a clear summary of its contributions and how they are novel.\n- The paper's evaluation is not comprehensive. The paper evaluates its method on a single benchmark, Meta-World, and does not compare it to a wide range of baselines. The paper also does not provide a detailed analysis of the results, such as ablation studies or sensitivity analyses.\n- The paper's results are not convincing. The paper reports that its method achieves state-of-the-art performance on the Meta-World benchmark, but the results are not statistically significant. The paper also does not provide error bars or confidence intervals for its results, making it difficult to assess the significance of the results.\n\n"", '1. The novelty of the proposed method is limited. The idea of using shared and task-specific features is not new. The proposed method is also similar to the existing method CARE [1] in terms of using task-specific embeddings to preserve the unique features of each task.\n2. The experiments are not comprehensive. The proposed method is only evaluated on the Meta-World benchmark. It is unclear whether the proposed method can be generalized to other tasks.\n\n[1] Sodhani, S., Gal, Y., Azizzadenesheli, K., Bhattacharya, K., Anandkumar, A., & Bragg-Cashin, J. (2021, July). Contextual attention-based representation for multi-task reinforcement learning. In International Conference on Machine Learning (pp. 9444-9453). PMLR.\n\n', 'The paper does not provide a detailed analysis of the results, such as ablation studies or sensitivity analyses. It would be helpful to see how the results change when different hyperparameters are used or when different tasks are included in the training.\n\n', '1. The proposed method is only evaluated on the Meta-World benchmark. It is unclear whether the proposed method can be generalized to other tasks.\n2. The paper does not provide a detailed analysis of the results, such as ablation studies or sensitivity analyses. It would be helpful to see how the results change when different hyperparameters are used or when different tasks are included in the training.\n\n']"
