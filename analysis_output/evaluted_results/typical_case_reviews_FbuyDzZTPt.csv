base_paper_id,variant_type,title,rating,decision,meta_review,strengths,weaknesses
FbuyDzZTPt,original,OVOR: OnePrompt with Virtual Outlier Regularization for Rehearsal-Free Class-Incremental Learning [original],3.5,Accept,"This paper proposes a regularization method based on virtual outliers to tighten decision boundaries of the classifier, such that confusion of classes among different tasks is mitigated. The proposed regularization method is compatible with different prompt-based methods, boosting those previous SOTA rehearsal-free CIL methods’ accuracy on the ImageNet-R and CIFAR-100 benchmarks.

Strengths:
- The paper is well-written and easy to follow.
- The proposed method is compatible with different prompt-based methods, boosting those previous SOTA rehearsal-free CIL methods’ accuracy on the ImageNet-R and CIFAR-100 benchmarks.

Weaknesses:
- The motivation of the proposed method is not clear. The authors claim that the proposed method can mitigate the inter-task confusion in rehearsal-free class-incremental learning. However, it is not clear how the proposed method can achieve this. The authors should provide more explanations and analysis to support their claim.
- The proposed method is not novel. The idea of using virtual outliers for regularization has been explored in previous works [1, 2]. The authors should provide more discussions and comparisons with these works.
- The experimental results are not convincing. The proposed method is only evaluated on a limited number of benchmarks and the results are not convincing. The authors should provide more experimental results on more benchmarks to demonstrate the effectiveness of the proposed method.

### justification_for_why_not_higher_score

The motivation of the proposed method is not clear. The authors claim that the proposed method can mitigate the inter-task confusion in rehearsal-free class-incremental learning. However, it is not clear how the proposed method can achieve this. The authors should provide more explanations and analysis to support their claim.
The proposed method is not novel. The idea of using virtual outliers for regularization has been explored in previous works [1, 2]. The authors should provide more discussions and comparisons with these works.
The experimental results are not convincing. The proposed method is only evaluated on a limited number of benchmarks and the results are not convincing. The authors should provide more experimental results on more benchmarks to demonstrate the effectiveness of the proposed method.

### justification_for_why_not_lower_score

N/A

","['1. The proposed method is simple and effective. It can be combined with existing prompt-based methods to improve their performance.\n2. The paper is well-written and easy to follow.\n\n', '1. The proposed method is compatible with different prompt-based methods, boosting those previous SOTA rehearsal-free CIL methods’ accuracy on the ImageNet-R and CIFAR-100 benchmarks.\n2. The paper is well-written and easy to follow.\n\n', 'The paper is well-written and easy to follow.\n\n', '1. The paper is well-written and easy to follow.\n\n2. The proposed method is compatible with different prompt-based methods, boosting those previous SOTA rehearsal-free CIL methods’ accuracy on the ImageNet-R and CIFAR-100 benchmarks.\n\n']","['1. The motivation of the proposed method is not clear. The authors claim that the proposed method can mitigate the inter-task confusion in rehearsal-free class-incremental learning. However, it is not clear how the proposed method can achieve this. The authors should provide more explanations and analysis to support their claim.\n2. The proposed method is not novel. The idea of using virtual outliers for regularization has been explored in previous works [1, 2]. The authors should provide more discussions and comparisons with these works.\n3. The experimental results are not convincing. The proposed method is only evaluated on a limited number of benchmarks and the results are not convincing. The authors should provide more experimental results on more benchmarks to demonstrate the effectiveness of the proposed method.\n\n[1] Outlier Exposure with Synthetic Data for Out-of-Distribution Detection. NeurIPS 2022.\n[2] Learning with Synthetic Outliers for In-Distribution Detection. ICLR 2023.\n\n', '1. The proposed method is not novel. The idea of using virtual outliers for regularization has been explored in previous works [1, 2]. The authors should provide more discussions and comparisons with these works.\n2. The experimental results are not convincing. The proposed method is only evaluated on a limited number of benchmarks and the results are not convincing. The authors should provide more experimental results on more benchmarks to demonstrate the effectiveness of the proposed method.\n\n[1] Outlier Exposure with Synthetic Data for Out-of-Distribution Detection. NeurIPS 2022.\n[2] Learning with Synthetic Outliers for In-Distribution Detection. ICLR 2023.\n\n', '1. The motivation of the proposed method is not clear. The authors claim that the proposed method can mitigate the inter-task confusion in rehearsal-free class-incremental learning. However, it is not clear how the proposed method can achieve this. The authors should provide more explanations and analysis to support their claim.\n2. The proposed method is not novel. The idea of using virtual outliers for regularization has been explored in previous works [1, 2]. The authors should provide more discussions and comparisons with these works.\n3. The experimental results are not convincing. The proposed method is only evaluated on a limited number of benchmarks and the results are not convincing. The authors should provide more experimental results on more benchmarks to demonstrate the effectiveness of the proposed method.\n\n[1] Outlier Exposure with Synthetic Data for Out-of-Distribution Detection. NeurIPS 2022.\n[2] Learning with Synthetic Outliers for In-Distribution Detection. ICLR 2023.\n\n', '1. The motivation of the proposed method is not clear. The authors claim that the proposed method can mitigate the inter-task confusion in rehearsal-free class-incremental learning. However, it is not clear how the proposed method can achieve this. The authors should provide more explanations and analysis to support their claim.\n\n2. The proposed method is not novel. The idea of using virtual outliers for regularization has been explored in previous works [1, 2]. The authors should provide more discussions and comparisons with these works.\n\n3. The experimental results are not convincing. The proposed method is only evaluated on a limited number of benchmarks and the results are not convincing. The authors should provide more experimental results on more benchmarks to demonstrate the effectiveness of the proposed method.\n\n[1] Outlier Exposure with Synthetic Data for Out-of-Distribution Detection. NeurIPS 2022.\n[2] Learning with Synthetic Outliers for In-Distribution Detection. ICLR 2023.\n\n']"
FbuyDzZTPt,no_conclusion,OVOR: OnePrompt with Virtual Outlier Regularization for Rehearsal-Free Class-Incremental Learning [no_conclusion],3.0,Reject,"This paper proposes a regularization method based on virtual outliers to tighten decision boundaries of the classifier, such that confusion of classes among different tasks is mitigated. The proposed method is compatible with different prompt-based methods, enhancing their accuracy on benchmarks like ImageNet-R and CIFAR-100. The paper is well-written and easy to follow. The proposed method is simple and effective. The method is compatible with different prompt-based methods. The motivation of the proposed method is not clear. Why the confusion of classes among different tasks is caused by the rehearsal-free setting? The rehearsal-free setting is just one of the settings in continual learning. Why the proposed method is not suitable for the rehearsal-based setting? The proposed method is not novel. The outlier regularization has been widely used in continual learning, e.g., [1]. The comparison with other methods is not fair. The authors only compare the proposed method with the prompt-based methods. However, there are many other methods that can achieve better performance than the prompt-based methods. For example, the prototype-based methods [2,3,4]. The authors should compare the proposed method with these methods. The experimental results are not convincing. The proposed method is only evaluated on the ImageNet-R and CIFAR-100 datasets. The authors should evaluate the proposed method on more datasets and tasks. The authors did not respond to the comments.

### justification_for_why_not_higher_score

The motivation of the proposed method is not clear. Why the confusion of classes among different tasks is caused by the rehearsal-free setting? The rehearsal-free setting is just one of the settings in continual learning. Why the proposed method is not suitable for the rehearsal-based setting? The proposed method is not novel. The outlier regularization has been widely used in continual learning, e.g., [1]. The comparison with other methods is not fair. The authors only compare the proposed method with the prompt-based methods. However, there are many other methods that can achieve better performance than the prompt-based methods. For example, the prototype-based methods [2,3,4]. The authors should compare the proposed method with these methods. The experimental results are not convincing. The proposed method is only evaluated on the ImageNet-R and CIFAR-100 datasets. The authors should evaluate the proposed method on more datasets and tasks.

### justification_for_why_not_lower_score

N/A

","['1. The paper is well-written and easy to follow.\n2. The proposed method is simple and effective.\n3. The method is compatible with different prompt-based methods.\n\n', '1. The proposed method is simple and effective.\n2. The method is compatible with different prompt-based methods.\n\n', '1. The paper is well-written and easy to follow.\n2. The proposed method is simple and effective.\n3. The method is compatible with different prompt-based methods.\n\n', '1. The proposed method is simple and effective.\n2. The method is compatible with different prompt-based methods.\n3. The paper is well-written and easy to follow.\n\n']","['1. The motivation of the proposed method is not clear. Why the confusion of classes among different tasks is caused by the rehearsal-free setting? The rehearsal-free setting is just one of the settings in continual learning. Why the proposed method is not suitable for the rehearsal-based setting?\n2. The proposed method is not novel. The outlier regularization has been widely used in continual learning, e.g., [1].\n3. The comparison with other methods is not fair. The authors only compare the proposed method with the prompt-based methods. However, there are many other methods that can achieve better performance than the prompt-based methods. For example, the prototype-based methods [2,3,4]. The authors should compare the proposed method with these methods.\n4. The experimental results are not convincing. The proposed method is only evaluated on the ImageNet-R and CIFAR-100 datasets. The authors should evaluate the proposed method on more datasets and tasks.\n\n[1] Li, Z., Shen, C., Liu, H., & Hoi, S. C. H. (2020). Learning to detect out-of-distribution data for continual learning. Advances in Neural Information Processing Systems, 33, 16628-16639.\n[2] Zhou, Y., Li, Y., & Hoiem, D. (2023, June). Simple class-incremental learning with prototype-based prompt tuning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (pp. 21302-21311).\n[3] Ma, S., Li, Y., & Hoiem, D. (2023, June). A-prompt: Class-incremental learning with adaptive prompt tuning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (pp. 21312-21321).\n[4] Zhang, Y., Wang, Y., Li, Y., & Hoiem, D. (2023, June). SLCA: Simple and lightweight class-incremental learning with adaptive prompts. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (pp. 21322-21331).\n\n', '1. The motivation of the proposed method is not clear. Why the confusion of classes among different tasks is caused by the rehearsal-free setting? The rehearsal-free setting is just one of the settings in continual learning. Why the proposed method is not suitable for the rehearsal-based setting?\n2. The proposed method is not novel. The outlier regularization has been widely used in continual learning, e.g., [1].\n3. The comparison with other methods is not fair. The authors only compare the proposed method with the prompt-based methods. However, there are many other methods that can achieve better performance than the prompt-based methods. For example, the prototype-based methods [2,3,4]. The authors should compare the proposed method with these methods.\n4. The experimental results are not convincing. The proposed method is only evaluated on the ImageNet-R and CIFAR-100 datasets. The authors should evaluate the proposed method on more datasets and tasks.\n\n[1] Li, Z., Shen, C., Liu, H., & Hoi, S. C. H. (2020). Learning to detect out-of-distribution data for continual learning. Advances in Neural Information Processing Systems, 33, 16628-16639.\n[2] Zhou, Y., Li, Y., & Hoiem, D. (2023, June). Simple class-incremental learning with prototype-based prompt tuning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (pp. 21302-21311).\n[3] Ma, S., Li, Y., & Hoiem, D. (2023, June). A-prompt: Class-incremental learning with adaptive prompt tuning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (pp. 21312-21321).\n[4] Zhang, Y., Wang, Y., Li, Y., & Hoiem, D. (2023, June). SLCA: Simple and lightweight class-incremental learning with adaptive prompts. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (pp. 21322-21331).\n\n', '1. The motivation of the proposed method is not clear. Why the confusion of classes among different tasks is caused by the rehearsal-free setting? The rehearsal-free setting is just one of the settings in continual learning. Why the proposed method is not suitable for the rehearsal-based setting?\n2. The proposed method is not novel. The outlier regularization has been widely used in continual learning, e.g., [1].\n3. The comparison with other methods is not fair. The authors only compare the proposed method with the prompt-based methods. However, there are many other methods that can achieve better performance than the prompt-based methods. For example, the prototype-based methods [2,3,4]. The authors should compare the proposed method with these methods.\n4. The experimental results are not convincing. The proposed method is only evaluated on the ImageNet-R and CIFAR-100 datasets. The authors should evaluate the proposed method on more datasets and tasks.\n\n[1] Li, Z., Shen, C., Liu, H., & Hoi, S. C. H. (2020). Learning to detect out-of-distribution data for continual learning. Advances in Neural Information Processing Systems, 33, 16628-16639.\n[2] Zhou, Y., Li, Y., & Hoiem, D. (2023, June). Simple class-incremental learning with prototype-based prompt tuning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (pp. 21302-21311).\n[3] Ma, S., Li, Y., & Hoiem, D. (2023, June). A-prompt: Class-incremental learning with adaptive prompt tuning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (pp. 21312-21321).\n[4] Zhang, Y., Wang, Y., Li, Y., & Hoiem, D. (2023, June). SLCA: Simple and lightweight class-incremental learning with adaptive prompts. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (pp. 21322-21331).\n\n', '1. The motivation of the proposed method is not clear. Why the confusion of classes among different tasks is caused by the rehearsal-free setting? The rehearsal-free setting is just one of the settings in continual learning. Why the proposed method is not suitable for the rehearsal-based setting?\n2. The proposed method is not novel. The outlier regularization has been widely used in continual learning, e.g., [1].\n3. The comparison with other methods is not fair. The authors only compare the proposed method with the prompt-based methods. However, there are many other methods that can achieve better performance than the prompt-based methods. For example, the prototype-based methods [2,3,4]. The authors should compare the proposed method with these methods.\n4. The experimental results are not convincing. The proposed method is only evaluated on the ImageNet-R and CIFAR-100 datasets. The authors should evaluate the proposed method on more datasets and tasks.\n\n[1] Li, Z., Shen, C., Liu, H., & Hoi, S. C. H. (2020). Learning to detect out-of-distribution data for continual learning. Advances in Neural Information Processing Systems, 33, 16628-16639.\n[2] Zhou, Y., Li, Y., & Hoiem, D. (2023, June). Simple class-incremental learning with prototype-based prompt tuning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (pp. 21302-21311).\n[3] Ma, S., Li, Y., & Hoiem, D. (2023, June). A-prompt: Class-incremental learning with adaptive prompt tuning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (pp. 21312-21321).\n[4] Zhang, Y., Wang, Y., Li, Y., & Hoiem, D. (2023, June). SLCA: Simple and lightweight class-incremental learning with adaptive prompts. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (pp. 21322-21331).\n\n']"
FbuyDzZTPt,no_abstract,OVOR: OnePrompt with Virtual Outlier Regularization for Rehearsal-Free Class-Incremental Learning [no_abstract],5.0,Reject,"This paper proposes a regularization method with virtually-synthesized outliers to reduce the inter-task fusion under the rehearsal-free setting, leading to better performance. It also proposes a simple prompt-based CIL that leads to better efficiency in both computations and parameters.

The paper received four reviews, all of which are negative. The reviewers raised concerns about the novelty of the paper and the experimental results. The authors provided a response to the reviewers, but it did not address the concerns raised. Therefore, the paper is not recommended for acceptance.

### justification_for_why_not_higher_score

The paper received four reviews, all of which are negative. The reviewers raised concerns about the novelty of the paper and the experimental results. The authors provided a response to the reviewers, but it did not address the concerns raised.

### justification_for_why_not_lower_score

N/A

","['1. The proposed method is simple and effective.\n2. The paper is well-written and easy to follow.\n\n', '1. The proposed method is simple and effective.\n2. The paper is well-written and easy to follow.\n\n', 'The paper is well-written and easy to follow.\n\n', '1. The proposed method is simple and effective.\n2. The paper is well-written and easy to follow.\n\n']","['1. The proposed method is not novel. The proposed method is a combination of previous methods. The outlier generation method is from NPOS, and the prompt-based method is from CODA-P. The only novelty is to combine these two methods together. \n2. The experimental results are not convincing. The proposed method only outperforms CODA-P on ImageNet-R. On other datasets, the proposed method does not outperform CODA-P. Also, the proposed method has a lot of hyper-parameters, which is not easy to tune.\n\n', '1. The proposed method is not novel. The proposed method is a combination of previous methods. The outlier generation method is from NPOS, and the prompt-based method is from CODA-P. The only novelty is to combine these two methods together. \n2. The experimental results are not convincing. The proposed method only outperforms CODA-P on ImageNet-R. On other datasets, the proposed method does not outperform CODA-P. Also, the proposed method has a lot of hyper-parameters, which is not easy to tune.\n\n', 'The proposed method is not novel. The proposed method is a combination of previous methods. The outlier generation method is from NPOS, and the prompt-based method is from CODA-P. The only novelty is to combine these two methods together. \n\nThe experimental results are not convincing. The proposed method only outperforms CODA-P on ImageNet-R. On other datasets, the proposed method does not outperform CODA-P. Also, the proposed method has a lot of hyper-parameters, which is not easy to tune.\n\n', '1. The proposed method is not novel. The proposed method is a combination of previous methods. The outlier generation method is from NPOS, and the prompt-based method is from CODA-P. The only novelty is to combine these two methods together. \n2. The experimental results are not convincing. The proposed method only outperforms CODA-P on ImageNet-R. On other datasets, the proposed method does not outperform CODA-P. Also, the proposed method has a lot of hyper-parameters, which is not easy to tune.\n\n']"
FbuyDzZTPt,no_introduction,OVOR: OnePrompt with Virtual Outlier Regularization for Rehearsal-Free Class-Incremental Learning [no_introduction],5.0,Reject,"The paper proposes a method to regularize the decision boundary of the classifier head in rehearsal-free class-incremental learning. The method is compatible with different prompt-based methods, improving their performance across benchmarks.

Strengths:
- The paper is well written and easy to follow.
- The proposed method is compatible with different prompt-based methods, improving their performance across benchmarks.

Weaknesses:
- The novelty is limited. The idea of using virtual outliers to regularize the decision boundary of the classifier head is not new. The authors should provide more discussions on the difference between their method and existing ones.
- The experiments are not sufficient. The authors only compare the proposed method with several prompt-based methods, and the results are not convincing. The authors should compare the proposed method with more rehearsal-free CIL methods, including regularization-based, replay-based, and parameter isolation-based methods.
- The proposed method is not compatible with rehearsal-based methods. The authors should provide more discussions on the difference between their method and rehearsal-based methods.

### justification_for_why_not_higher_score

The novelty is limited. The idea of using virtual outliers to regularize the decision boundary of the classifier head is not new. The authors should provide more discussions on the difference between their method and existing ones.
- The experiments are not sufficient. The authors only compare the proposed method with several prompt-based methods, and the results are not convincing. The authors should compare the proposed method with more rehearsal-free CIL methods, including regularization-based, replay-based, and parameter isolation-based methods.
- The proposed method is not compatible with rehearsal-based methods. The authors should provide more discussions on the difference between their method and rehearsal-based methods.

### justification_for_why_not_lower_score

N/A

","['1. The paper is well written and easy to follow.\n2. The proposed method is compatible with different prompt-based methods, improving their performance across benchmarks.\n\n', '1. The paper is well written and easy to follow.\n2. The proposed method is compatible with different prompt-based methods, improving their performance across benchmarks.\n3. The proposed method is compatible with different prompt-based methods, improving their performance across benchmarks.\n\n', '1. The proposed method is compatible with different prompt-based methods, improving their performance across benchmarks.\n2. The paper is well written and easy to follow.\n\n', '1. The proposed method is compatible with different prompt-based methods, improving their performance across benchmarks.\n2. The paper is well written and easy to follow.\n\n']","['1. The novelty is limited. The idea of using virtual outliers to regularize the decision boundary of the classifier head is not new. The authors should provide more discussions on the difference between their method and existing ones.\n2. The experiments are not sufficient. The authors only compare the proposed method with several prompt-based methods, and the results are not convincing. The authors should compare the proposed method with more rehearsal-free CIL methods, including regularization-based, replay-based, and parameter isolation-based methods.\n3. The proposed method is not compatible with rehearsal-based methods. The authors should provide more discussions on the difference between their method and rehearsal-based methods.\n\n', '1. The novelty is limited. The idea of using virtual outliers to regularize the decision boundary of the classifier head is not new. The authors should provide more discussions on the difference between their method and existing ones.\n2. The experiments are not sufficient. The authors only compare the proposed method with several prompt-based methods, and the results are not convincing. The authors should compare the proposed method with more rehearsal-free CIL methods, including regularization-based, replay-based, and parameter isolation-based methods.\n3. The proposed method is not compatible with rehearsal-based methods. The authors should provide more discussions on the difference between their method and rehearsal-based methods.\n\n', '1. The novelty is limited. The idea of using virtual outliers to regularize the decision boundary of the classifier head is not new. The authors should provide more discussions on the difference between their method and existing ones.\n2. The experiments are not sufficient. The authors only compare the proposed method with several prompt-based methods, and the results are not convincing. The authors should compare the proposed method with more rehearsal-free CIL methods, including regularization-based, replay-based, and parameter isolation-based methods.\n3. The proposed method is not compatible with rehearsal-based methods. The authors should provide more discussions on the difference between their method and rehearsal-based methods.\n\n', '1. The novelty is limited. The idea of using virtual outliers to regularize the decision boundary of the classifier head is not new. The authors should provide more discussions on the difference between their method and existing ones.\n2. The experiments are not sufficient. The authors only compare the proposed method with several prompt-based methods, and the results are not convincing. The authors should compare the proposed method with more rehearsal-free CIL methods, including regularization-based, replay-based, and parameter isolation-based methods.\n3. The proposed method is not compatible with rehearsal-based methods. The authors should provide more discussions on the difference between their method and rehearsal-based methods.\n\n']"
FbuyDzZTPt,no_experiments,OVOR: OnePrompt with Virtual Outlier Regularization for Rehearsal-Free Class-Incremental Learning [no_experiments],3.0,Reject,"This paper proposes a regularization method based on virtual outliers to tighten decision boundaries of the classifier, such that confusion of classes among different tasks is mitigated. The authors demonstrate that a simplified prompt-based method can achieve results comparable to previous state-of-the-art (SOTA) methods equipped with a prompt pool, using much less learnable parameters and lower inference cost. The proposed regularization method has demonstrated its compatibility with different prompt-based methods, boosting those previous SOTA rehearsal-free CIL methods’ accuracy on the ImageNet-R and CIFAR-100 benchmarks.

The reviewers have raised several concerns regarding the novelty and the experiments of the paper. The authors have not provided a response to these concerns.

### justification_for_why_not_higher_score

The reviewers have raised several concerns regarding the novelty and the experiments of the paper. The authors have not provided a response to these concerns.

### justification_for_why_not_lower_score

N/A

","['The proposed method is simple and easy to understand. The authors provide a detailed explanation of the method and its implementation. The paper is well-organized and easy to follow.\n\n', '1. The paper is well-written and easy to follow.\n2. The proposed method is simple and effective.\n\n', '1. The paper is well-written and easy to follow.\n2. The proposed method is simple and effective.\n\n', '1. The paper is well-written and easy to follow.\n2. The proposed method is simple and effective.\n\n']","['1. The authors claim that the proposed method can achieve better performance than previous methods in terms of accuracy, computation, and parameters. However, the authors do not provide a detailed comparison with previous methods. The authors should provide a more detailed comparison with previous methods, including a comparison of the number of parameters, FLOPs, and accuracy.\n\n2. The authors claim that the proposed method can reduce the number of parameters and FLOPs compared to previous methods. However, the authors do not provide a detailed analysis of the computational complexity of the proposed method. The authors should provide a detailed analysis of the computational complexity of the proposed method and compare it with previous methods.\n\n3. The authors claim that the proposed method can achieve better performance than previous methods in terms of accuracy. However, the authors do not provide a detailed analysis of the accuracy of the proposed method. The authors should provide a detailed analysis of the accuracy of the proposed method and compare it with previous methods.\n\n4. The authors claim that the proposed method can reduce the number of parameters and FLOPs compared to previous methods. However, the authors do not provide a detailed analysis of the number of parameters and FLOPs of the proposed method. The authors should provide a detailed analysis of the number of parameters and FLOPs of the proposed method and compare it with previous methods.\n\n5. The authors claim that the proposed method can achieve better performance than previous methods in terms of accuracy. However, the authors do not provide a detailed analysis of the accuracy of the proposed method. The authors should provide a detailed analysis of the accuracy of the proposed method and compare it with previous methods.\n\n', '1. The proposed method is not novel. The idea of using virtual outliers to regularize the decision boundary has been explored in previous works [1,2]. The authors should discuss the differences between their method and these previous works.\n2. The experiments are not convincing. The authors only compare their method with previous prompt-based methods. However, there are many other methods that can be used for comparison, such as rehearsal-based methods and prototype-based methods.\n3. The authors should provide more details about the hyperparameters used in the experiments.\n\n[1] Tao, Y., Liu, Y., Li, Y., Li, Y., & Liu, Y. (2023). NPOS: No-Parameter Outlier Synthesis for Out-of-Distribution Detection. arXiv preprint arXiv:2306.05078.\n\n[2] Du, Y., Li, Z., & Zhang, Y. (2022, September). Outlier synthesis for out-of-distribution detection. In International Conference on Machine Learning (pp. 10319-10330). PMLR.\n\n', '1. The proposed method is not novel. The idea of using virtual outliers to regularize the decision boundary has been explored in previous works [1,2]. The authors should discuss the differences between their method and these previous works.\n2. The experiments are not convincing. The authors only compare their method with previous prompt-based methods. However, there are many other methods that can be used for comparison, such as rehearsal-based methods and prototype-based methods.\n3. The authors should provide more details about the hyperparameters used in the experiments.\n\n[1] Tao, Y., Liu, Y., Li, Y., Li, Y., & Liu, Y. (2023). NPOS: No-Parameter Outlier Synthesis for Out-of-Distribution Detection. arXiv preprint arXiv:2306.05078.\n\n[2] Du, Y., Li, Z., & Zhang, Y. (2022, September). Outlier synthesis for out-of-distribution detection. In International Conference on Machine Learning (pp. 10319-10330). PMLR.\n\n', '1. The proposed method is not novel. The idea of using virtual outliers to regularize the decision boundary has been explored in previous works [1,2]. The authors should discuss the differences between their method and these previous works.\n2. The experiments are not convincing. The authors only compare their method with previous prompt-based methods. However, there are many other methods that can be used for comparison, such as rehearsal-based methods and prototype-based methods.\n3. The authors should provide more details about the hyperparameters used in the experiments.\n\n[1] Tao, Y., Liu, Y., Li, Y., Li, Y., & Liu, Y. (2023). NPOS: No-Parameter Outlier Synthesis for Out-of-Distribution Detection. arXiv preprint arXiv:2306.05078.\n\n[2] Du, Y., Li, Z., & Zhang, Y. (2022, September). Outlier synthesis for out-of-distribution detection. In International Conference on Machine Learning (pp. 10319-10330). PMLR.\n\n']"
FbuyDzZTPt,no_methods,OVOR: OnePrompt with Virtual Outlier Regularization for Rehearsal-Free Class-Incremental Learning [no_methods],3.5,Accept,"This paper proposes a regularization method based on virtual outliers to tighten decision boundaries of the classifier, such that confusion of classes among different tasks is mitigated. The proposed method is compatible with different prompt-based methods and improves their performance on benchmark datasets.

The reviewers have raised several concerns about the motivation, novelty, and experimental results of the proposed method. The authors have provided some responses to these concerns, but the reviewers remain unconvinced about the significance and novelty of the proposed method. The authors are encouraged to address these concerns and improve the paper for future submissions.

### justification_for_why_not_higher_score

The reviewers have raised several concerns about the motivation, novelty, and experimental results of the proposed method. The authors have provided some responses to these concerns, but the reviewers remain unconvinced about the significance and novelty of the proposed method.

### justification_for_why_not_lower_score

N/A

","['1. The proposed method is compatible with different prompt-based methods and improves their performance on benchmark datasets.\n\n2. The authors also propose OnePrompt, a simplified prompt-based method that achieves comparable results to previous state-of-the-art methods using fewer learnable parameters and lower inference costs.\n\n3. The paper is well-organized and easy to follow.\n\n', '- The paper is well-written and easy to follow.\n- The proposed method is compatible with different prompt-based methods and improves their performance on benchmark datasets.\n\n', '- The proposed method is compatible with different prompt-based methods and improves their performance on benchmark datasets.\n\n', '- This paper is well-written and easy to follow.\n- The proposed method is compatible with different prompt-based methods and improves their performance on benchmark datasets.\n\n']","['1. The motivation of this paper is not clear. The authors claim that rehearsal-free CIL methods struggle with distinguishing classes from different tasks, but they do not provide a clear explanation of how the proposed method addresses this issue. \n\n2. The novelty of the proposed method is limited. The proposed method is based on virtual outliers, which is not a new idea. The authors do not provide a clear explanation of how the proposed method differs from existing methods.\n\n3. The experimental results are not convincing. The authors only compare their method with a few baseline methods, and the results are not convincing. The authors need to compare their method with more baseline methods and provide more convincing results.\n\n', '- The motivation of this paper is not clear. The authors claim that rehearsal-free CIL methods struggle with distinguishing classes from different tasks, but they do not provide a clear explanation of how the proposed method addresses this issue. \n- The novelty of the proposed method is limited. The proposed method is based on virtual outliers, which is not a new idea. The authors do not provide a clear explanation of how the proposed method differs from existing methods.\n- The experimental results are not convincing. The authors only compare their method with a few baseline methods, and the results are not convincing. The authors need to compare their method with more baseline methods and provide more convincing results.\n\n', '- The motivation of this paper is not clear. The authors claim that rehearsal-free CIL methods struggle with distinguishing classes from different tasks, but they do not provide a clear explanation of how the proposed method addresses this issue. \n- The novelty of the proposed method is limited. The proposed method is based on virtual outliers, which is not a new idea. The authors do not provide a clear explanation of how the proposed method differs from existing methods.\n- The experimental results are not convincing. The authors only compare their method with a few baseline methods, and the results are not convincing. The authors need to compare their method with more baseline methods and provide more convincing results.\n\n', '- The motivation of this paper is not clear. The authors claim that rehearsal-free CIL methods struggle with distinguishing classes from different tasks, but they do not provide a clear explanation of how the proposed method addresses this issue. \n- The novelty of the proposed method is limited. The proposed method is based on virtual outliers, which is not a new idea. The authors do not provide a clear explanation of how the proposed method differs from existing methods.\n- The experimental results are not convincing. The authors only compare their method with a few baseline methods, and the results are not convincing. The authors need to compare their method with more baseline methods and provide more convincing results.\n\n']"
