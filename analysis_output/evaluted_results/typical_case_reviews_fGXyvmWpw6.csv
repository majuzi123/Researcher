base_paper_id,variant_type,title,rating,decision,meta_review,strengths,weaknesses
fGXyvmWpw6,original,Federated Virtual Learning on Heterogeneous Data with Local-global Distillation [original],4.5,Reject,"The paper introduces a new approach to federated learning, termed federated virtual learning, which utilizes a combination of local and global dataset distillation to enhance the consistency of local training by aligning local and global features. The paper presents a comprehensive evaluation of the proposed method on benchmark and real-world datasets, demonstrating its effectiveness in addressing the heterogeneity issue in federated learning. However, the paper lacks a clear motivation for the proposed method, a thorough analysis of the proposed method, and a comparison with other methods that address the heterogeneity issue in FL. The paper also lacks a discussion of the limitations of the proposed method. These weaknesses are major concerns that need to be addressed before the paper can be accepted.

### justification_for_why_not_higher_score

The paper lacks a clear motivation for the proposed method, a thorough analysis of the proposed method, and a comparison with other methods that address the heterogeneity issue in FL. The paper also lacks a discussion of the limitations of the proposed method.

### justification_for_why_not_lower_score

N/A

","['1. The paper is well-written and easy to follow.\n2. The proposed method is novel and effective in addressing the heterogeneity issue in federated learning.\n3. The experimental results demonstrate the effectiveness of the proposed method in improving the performance of FL algorithms under various settings.\n\n', 'The paper is well-written and easy to follow. The authors provide a clear and concise explanation of the proposed method and its components. The paper is well-structured and logically organized, making it easy to understand the flow of the method and the experimental setup.\n\nThe proposed method, FEDLGD, is an innovative approach to addressing the heterogeneity issue in federated learning. The use of local and global dataset distillation is a novel idea that has not been explored in previous works. The iterative distribution matching and federated gradient matching components are well-designed and effective in aligning local and global features.\n\nThe experimental results demonstrate the effectiveness of the proposed method in improving the performance of FL algorithms under various settings. The paper provides a comprehensive evaluation of the proposed method on benchmark and real-world datasets, including DIGITS and CIFAR10C. The results show that FEDLGD outperforms state-of-the-art FL algorithms in terms of accuracy and convergence speed.\n\n', '1. The paper introduces a novel approach to federated learning, which they term federated virtual learning. The authors propose a method called Federated Virtual Learning on Heterogeneous Data with Local-Global Distillation (FEDLGD) that utilizes a combination of local and global dataset distillation to enhance the consistency of local training by aligning local and global features. \n2. The paper presents a comprehensive evaluation of the proposed method on benchmark and real-world datasets, demonstrating its effectiveness in addressing the heterogeneity issue in federated learning.\n\n', '1. The paper introduces a novel approach to federated learning, which they term federated virtual learning. The authors propose a method called Federated Virtual Learning on Heterogeneous Data with Local-Global Distillation (FEDLGD) that utilizes a combination of local and global dataset distillation to enhance the consistency of local training by aligning local and global features. \n2. The paper presents a comprehensive evaluation of the proposed method on benchmark and real-world datasets, demonstrating its effectiveness in addressing the heterogeneity issue in federated learning.\n\n']","[""1. The paper lacks a clear motivation for the proposed method. The authors mention that dataset distillation can amplify the heterogeneity issue in FL, but they do not provide a clear explanation of why this is the case. Additionally, they do not provide a clear explanation of how their method addresses this issue.\n2. The paper lacks a thorough analysis of the proposed method. The authors provide some ablation studies, but they do not provide a detailed analysis of the method's performance under different settings.\n3. The paper lacks a comparison with other methods that address the heterogeneity issue in FL. The authors compare their method with some FL algorithms, but they do not compare it with other methods that specifically address the heterogeneity issue.\n4. The paper lacks a discussion of the limitations of the proposed method. The authors mention that the method introduces additional communication and computation cost, but they do not discuss other potential limitations.\n\n"", ""The paper lacks a clear motivation for the proposed method. The authors mention that dataset distillation can amplify the heterogeneity issue in FL, but they do not provide a clear explanation of why this is the case. Additionally, they do not provide a clear explanation of how their method addresses this issue.\n\nThe paper lacks a thorough analysis of the proposed method. The authors provide some ablation studies, but they do not provide a detailed analysis of the method's performance under different settings. For example, the paper does not provide a detailed analysis of the effect of different hyperparameters on the performance of the method.\n\nThe paper lacks a comparison with other methods that address the heterogeneity issue in FL. The authors compare their method with some FL algorithms, but they do not compare it with other methods that specifically address the heterogeneity issue.\n\nThe paper lacks a discussion of the limitations of the proposed method. The authors mention that the method introduces additional communication and computation cost, but they do not discuss other potential limitations.\n\n"", ""1. The paper lacks a clear motivation for the proposed method. The authors mention that dataset distillation can amplify the heterogeneity issue in FL, but they do not provide a clear explanation of why this is the case. Additionally, they do not provide a clear explanation of how their method addresses this issue.\n2. The paper lacks a thorough analysis of the proposed method. The authors provide some ablation studies, but they do not provide a detailed analysis of the method's performance under different settings. For example, the paper does not provide a detailed analysis of the effect of different hyperparameters on the performance of the method.\n3. The paper lacks a comparison with other methods that address the heterogeneity issue in FL. The authors compare their method with some FL algorithms, but they do not compare it with other methods that specifically address the heterogeneity issue.\n4. The paper lacks a discussion of the limitations of the proposed method. The authors mention that the method introduces additional communication and computation cost, but they do not discuss other potential limitations.\n\n"", ""1. The paper lacks a clear motivation for the proposed method. The authors mention that dataset distillation can amplify the heterogeneity issue in FL, but they do not provide a clear explanation of why this is the case. Additionally, they do not provide a clear explanation of how their method addresses this issue.\n2. The paper lacks a thorough analysis of the proposed method. The authors provide some ablation studies, but they do not provide a detailed analysis of the method's performance under different settings. For example, the paper does not provide a detailed analysis of the effect of different hyperparameters on the performance of the method.\n3. The paper lacks a comparison with other methods that address the heterogeneity issue in FL. The authors compare their method with some FL algorithms, but they do not compare it with other methods that specifically address the heterogeneity issue.\n4. The paper lacks a discussion of the limitations of the proposed method. The authors mention that the method introduces additional communication and computation cost, but they do not discuss other potential limitations.\n\n""]"
fGXyvmWpw6,no_conclusion,Federated Virtual Learning on Heterogeneous Data with Local-global Distillation [no_conclusion],5.0,Reject,"The paper proposes a new method for federated learning on heterogeneous data. The authors propose a new method called Federated Virtual Learning on Heterogeneous Data with Local-Global Distillation (FEDLGD) that trains FL using virtual data created through a combination of local and global dataset distillation. The method consists of two main components: local data distillation and global data distillation. The local data distillation is used to distill local virtual data from the local datasets, while the global data distillation is used to distill global virtual data from the local virtual data. The authors evaluate their method on several benchmark and real-world datasets and show that it outperforms state-of-the-art heterogeneous FL algorithms under various settings. 

The paper is well-written and easy to follow. The proposed method is well-motivated and the results are promising. The experiments are extensive and the results are promising. 

However, the novelty of the proposed method is limited. The authors propose to use dataset distillation to create virtual datasets for FL, which is a well-known technique in the literature. The authors also propose to use federated gradient matching to distill global virtual data, which is similar to the method proposed in [1]. The authors do not provide any theoretical analysis of the proposed method. It is not clear how the proposed method works and why it is effective. The authors do not provide any error bars in the experimental results, which makes it difficult to evaluate the significance of the performance improvement. 

The authors did not provide a response to the review.

### justification_for_why_not_higher_score

The novelty of the proposed method is limited. The authors propose to use dataset distillation to create virtual datasets for FL, which is a well-known technique in the literature. The authors also propose to use federated gradient matching to distill global virtual data, which is similar to the method proposed in [1]. The authors do not provide any theoretical analysis of the proposed method. It is not clear how the proposed method works and why it is effective. The authors do not provide any error bars in the experimental results, which makes it difficult to evaluate the significance of the performance improvement.

### justification_for_why_not_lower_score

N/A

","['The authors provide a good motivation for the proposed method. The proposed method is well-motivated and the proposed method can improve the performance of FL.\n\n', '1. The paper is well-written and easy to follow.\n2. The proposed method is well-motivated and the results are promising.\n3. The experiments are extensive and the results are promising.\n\n', '1. The paper is well-written and easy to follow. \n2. The proposed method is well-motivated and the results are promising. \n3. The experiments are extensive and the results are promising.\n\n', '1. The paper is well-written and easy to follow. \n2. The proposed method is well-motivated and the results are promising. \n3. The experiments are extensive and the results are promising.\n\n']","['1. The proposed method is not well-motivated. The authors claim that the proposed method can improve the consistency of local training by enforcing similarity between local and global features. However, the authors do not provide any theoretical analysis or empirical evidence to support this claim. \n2. The proposed method is not novel. The authors propose to use dataset distillation to create virtual datasets for FL, which is a well-known technique in the literature. The authors also propose to use federated gradient matching to distill global virtual data, which is similar to the method proposed in [1]. \n3. The experimental results are not convincing. The authors only compare their method with a few baselines, and the performance improvement is not significant. The authors also do not provide any error bars in the experimental results, which makes it difficult to evaluate the significance of the performance improvement.\n\n[1] Zhao et al. Dataset Distillation: A New Paradigm for Scalable and Privacy-Preserving Federated Learning. ICLR 2021.\n\n', '1. The novelty of the proposed method is limited. The authors propose to use dataset distillation to create virtual datasets for FL, which is a well-known technique in the literature. The authors also propose to use federated gradient matching to distill global virtual data, which is similar to the method proposed in [1].\n2. The authors do not provide any theoretical analysis of the proposed method. It is not clear how the proposed method works and why it is effective.\n3. The authors do not provide any error bars in the experimental results, which makes it difficult to evaluate the significance of the performance improvement.\n\n[1] Zhao et al. Dataset Distillation: A New Paradigm for Scalable and Privacy-Preserving Federated Learning. ICLR 2021.\n\n', '1. The authors should provide a more detailed analysis of the computational complexity of the proposed method. \n2. The authors should provide more details about the initialization of the virtual data. \n3. The authors should provide more details about the hyperparameters of the proposed method. \n4. The authors should provide more details about the experimental setup, including the hardware and software used.\n\n', '1. The authors should provide a more detailed analysis of the computational complexity of the proposed method. \n2. The authors should provide more details about the initialization of the virtual data. \n3. The authors should provide more details about the hyperparameters of the proposed method. \n4. The authors should provide more details about the experimental setup, including the hardware and software used.\n\n']"
fGXyvmWpw6,no_abstract,Federated Virtual Learning on Heterogeneous Data with Local-global Distillation [no_abstract],5.0,Reject,"This paper proposes a new method for federated learning on heterogeneous data. The method consists of two steps: 1) local data distillation and 2) global data distillation. The local data distillation aims to create virtual data that is similar to the real data on each client, while the global data distillation aims to create a global virtual data that is similar to the local virtual data. The method is evaluated on several benchmark datasets and shows improvement over existing methods.

The reviewers agree that the paper is well-written and easy to follow. The proposed method is well-motivated and the results are promising. However, there are some concerns about the novelty of the proposed method and the evaluation of the proposed method. The authors are encouraged to address these concerns in the final version of the paper.

### justification_for_why_not_higher_score

The novelty of the proposed method is not very clear. The paper lacks a thorough evaluation of the proposed method.

### justification_for_why_not_lower_score

N/A

","['The paper is well-written and easy to follow. The proposed method is well-motivated and the results are promising.\n\n', 'The paper is well-written and easy to follow. The proposed method is well-motivated and the results are promising.\n\n', '1. This paper is well-written and easy to follow.\n2. The proposed method is well-motivated and the results are promising.\n\n', '1. The paper is well-written and easy to follow.\n2. The proposed method is well-motivated and the results are promising.\n3. The paper provides a comprehensive evaluation of the proposed method.\n\n']","['The paper lacks a thorough evaluation of the proposed method. The authors only report the accuracy of the model on the test set, but do not provide any information about the quality of the virtual data. For example, how similar are the virtual data to the real data? How well do the virtual data represent the distribution of the real data? These are important questions that need to be answered in order to understand the effectiveness of the proposed method.\n\n', 'The proposed method is a combination of two existing methods, dataset distillation and contrastive learning. The novelty of the proposed method is not very clear.\n\n', '1. The authors should provide more details about the data distillation process. For example, how to distill the local data, and how to distill the global data. \n2. The authors should provide more details about the experimental results. For example, what are the results of the ablation studies, and what are the results of the comparison with other methods.\n\n', ""1. The paper lacks a thorough evaluation of the proposed method. The authors only report the accuracy of the model on the test set, but do not provide any information about the quality of the virtual data. For example, how similar are the virtual data to the real data? How well do the virtual data represent the distribution of the real data?\n2. The paper does not provide any information about the computational cost of the proposed method. How much time and resources are required to train the model using FEDLGD?\n3. The paper does not provide any information about the privacy of the proposed method. How does FEDLGD protect the privacy of the clients' data?\n\n""]"
fGXyvmWpw6,no_introduction,Federated Virtual Learning on Heterogeneous Data with Local-global Distillation [no_introduction],4.75,Reject,"This paper introduces a new method for FL, called FEDLGD. It utilizes virtual data on both client and server sides to train FL models. The authors are the first to reveal that FL on local virtual data can increase heterogeneity. Furthermore, they propose iterative distribution matching and federated gradient matching to iteratively update local and global virtual data, and apply global virtual regularization to effectively harmonize domain shift. Their experiments on benchmark and real medical datasets show that FEDLGD outperforms current state-of-the-art methods in heterogeneous settings.

Strengths:
1. The paper is well-written and easy to follow. 
2. The proposed method is interesting and reasonable. 
3. The proposed method is novel.

Weaknesses:
1. The authors should compare the proposed method with more baselines, including FedGen [1], FedFTG [2], FedICT [3], FedGKT [4], and FedDKC [5].
2. The authors should evaluate the proposed method on more datasets to demonstrate its effectiveness.

### justification_for_why_not_higher_score

The authors should compare the proposed method with more baselines, including FedGen [1], FedFTG [2], FedICT [3], FedGKT [4], and FedDKC [5].
2. The authors should evaluate the proposed method on more datasets to demonstrate its effectiveness.

### justification_for_why_not_lower_score

N/A

","['1. The paper is well-written and easy to follow. \n2. The proposed method is interesting and reasonable.\n\n', 'The paper is well-written and easy to follow. The idea of using virtual data in federated learning is interesting and worth exploring.\n\n', '1. The paper is well-written and easy to follow. \n2. The proposed method is interesting and reasonable. \n3. The proposed method is novel.\n\n', '1. This paper proposes a novel approach to address the heterogeneity issue in FL by using a combination of local and global dataset distillation.\n2. The proposed method is well-motivated and the authors provide a thorough analysis of the experiments.\n\n']","['1. The proposed method is not novel enough. The idea of using local-global distillation to mitigate the heterogeneity issue in FL has been studied in [1]. \n2. The experiments are not sufficient. The authors only consider two datasets (digits and CIFAR10-C) in the experiments. The authors should consider more datasets to evaluate the proposed method. \n3. The authors should compare the proposed method with more baselines, including FedGen [2], FedFTG [3], FedICT [4], FedGKT [5], and FedDKC [6].\n\n[1] Zhu, Zeyuan, et al. ""FedGen: A federated learning framework with virtual data generation."" Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 36. No. 9. 2022.\n\n[2] Lin, Zhenyu, et al. ""Ensemble distillation for robust model fusion in federated learning."" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2020.\n\n[3] Zhang, Jiaxi, et al. ""Federated transfer learning with knowledge distillation for heterogeneous clients."" Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 36. No. 2. 2022.\n\n[4] Wu, Shuwen, et al. ""Improving Federated Learning on Non-IID Data with Knowledge Distillation."" arXiv preprint arXiv:2301.12869 (2023).\n\n[5] He, Jie, et al. ""FedGKT: Improving Federated Learning on Non-IID Data with Knowledge Transfer."" Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 35. No. 12. 2021.\n\n[6] Wu, Shuwen, et al. ""Federated knowledge distillation with curriculum learning for non-iid data."" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2022.\n\n', '1. The proposed method is not novel enough. The idea of using local and global distillation to create virtual data has been studied in previous works. The proposed method is a combination of these two ideas.\n\n2. The evaluation of the proposed method is not sufficient. The proposed method is only evaluated on several small datasets. The authors should evaluate the proposed method on larger datasets to demonstrate its effectiveness.\n\n3. The proposed method has high computation cost. The authors should provide more details on the computation cost of the proposed method.\n\n', '1. The authors should compare the proposed method with more baselines, including FedGen [1], FedFTG [2], FedICT [3], FedGKT [4], and FedDKC [5].\n\n[1] Lin, Zhenyu, et al. ""Ensemble distillation for robust model fusion in federated learning."" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2020.\n\n[2] Zhang, Jiaxi, et al. ""Federated transfer learning with knowledge distillation for heterogeneous clients."" Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 36. No. 2. 2022.\n\n[3] Wu, Shuwen, et al. ""Improving Federated Learning on Non-IID Data with Knowledge Distillation."" arXiv preprint arXiv:2301.12869 (2023).\n\n[4] He, Jie, et al. ""FedGKT: Improving Federated Learning on Non-IID Data with Knowledge Transfer."" Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 35. No. 12. 2021.\n\n[5] Wu, Shuwen, et al. ""Federated knowledge distillation with curriculum learning for non-iid data."" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2022.\n\n', '1. The authors should compare the proposed method with more baselines, including FedGen [1], FedFTG [2], FedICT [3], FedGKT [4], and FedDKC [5].\n2. The authors should evaluate the proposed method on more datasets to demonstrate its effectiveness.\n\n[1] Lin, Zhenyu, et al. ""Ensemble distillation for robust model fusion in federated learning."" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2020.\n\n[2] Zhang, Jiaxi, et al. ""Federated transfer learning with knowledge distillation for heterogeneous clients."" Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 36. No. 2. 2022.\n\n[3] Wu, Shuwen, et al. ""Improving Federated Learning on Non-IID Data with Knowledge Distillation."" arXiv preprint arXiv:2301.12869 (2023).\n\n[4] He, Jie, et al. ""FedGKT: Improving Federated Learning on Non-IID Data with Knowledge Transfer."" Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 35. No. 12. 2021.\n\n[5] Wu, Shuwen, et al. ""Federated knowledge distillation with curriculum learning for non-iid data."" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2022.\n\n']"
fGXyvmWpw6,no_experiments,Federated Virtual Learning on Heterogeneous Data with Local-global Distillation [no_experiments],6.0,Accept,"The paper proposes a new method, called Federated Virtual Learning on Heterogeneous Data with Local-Global Distillation (FEDLGD), which trains FL using virtual data created through a combination of local and global dataset distillation. The authors propose iterative distribution matching to inpaint global information to local virtual data and use federated gradient matching to distill global virtual data that are shared with clients without compromising data privacy. The authors experiment on both benchmark and real-world datasets that contain heterogeneous data from different sources, and further scale up to an FL scenario that contains large number of clients with heterogeneous and class imbalance data. The method outperforms state-of-the-art heterogeneous FL algorithms under various settings.

The paper is well-written and easy to follow. The proposed method is well-motivated and easy to understand. The experimental results show that the proposed method outperforms state-of-the-art heterogeneous FL algorithms under various settings.

### justification_for_why_not_higher_score

The paper proposes a new method, called Federated Virtual Learning on Heterogeneous Data with Local-Global Distillation (FEDLGD), which trains FL using virtual data created through a combination of local and global dataset distillation. The authors propose iterative distribution matching to inpaint global information to local virtual data and use federated gradient matching to distill global virtual data that are shared with clients without compromising data privacy. The authors experiment on both benchmark and real-world datasets that contain heterogeneous data from different sources, and further scale up to an FL scenario that contains large number of clients with heterogeneous and class imbalance data. The method outperforms state-of-the-art heterogeneous FL algorithms under various settings.

The paper is well-written and easy to follow. The proposed method is well-motivated and easy to understand. The experimental results show that the proposed method outperforms state-of-the-art heterogeneous FL algorithms under various settings.

### justification_for_why_not_lower_score

The paper proposes a new method, called Federated Virtual Learning on Heterogeneous Data with Local-Global Distillation (FEDLGD), which trains FL using virtual data created through a combination of local and global dataset distillation. The authors propose iterative distribution matching to inpaint global information to local virtual data and use federated gradient matching to distill global virtual data that are shared with clients without compromising data privacy. The authors experiment on both benchmark and real-world datasets that contain heterogeneous data from different sources, and further scale up to an FL scenario that contains large number of clients with heterogeneous and class imbalance data. The method outperforms state-of-the-art heterogeneous FL algorithms under various settings.

The paper is well-written and easy to follow. The proposed method is well-motivated and easy to understand. The experimental results show that the proposed method outperforms state-of-the-art heterogeneous FL algorithms under various settings.

","['1. This paper proposes a new method, called Federated Virtual Learning on Heterogeneous Data with Local-Global Distillation (FEDLGD), which trains FL using virtual data created through a combination of local and global dataset distillation. The authors propose iterative distribution matching to inpaint global information to local virtual data and use federated gradient matching to distill global virtual data that are shared with clients without compromising data privacy.\n\n2. The authors experiment on both benchmark and real-world datasets that contain heterogeneous data from different sources, and further scale up to an FL scenario that contains large number of clients with heterogeneous and class imbalance data. The method outperforms state-of-the-art heterogeneous FL algorithms under various settings.\n\n', '- The paper is well-written and easy to follow.\n- The proposed method is well-motivated and easy to understand.\n- The experimental results show that the proposed method outperforms state-of-the-art heterogeneous FL algorithms under various settings.\n\n', 'The paper introduces a new approach to federated learning called Federated Virtual Learning on Heterogeneous Data with Local-Global Distillation (FEDLGD). The authors propose a method that utilizes virtual data on both client and server sides to train FL models. The paper reveals that FL on local virtual data can increase heterogeneity. Furthermore, the authors propose iterative distribution matching and federated gradient matching to iteratively update local and global virtual data, and apply global virtual regularization to effectively harmonize domain shift. The paper shows that FEDLGD outperforms current state-of-the-art methods in heterogeneous settings.\n\n', '1. The proposed method is novel and effective in addressing the heterogeneity issue in FL.\n2. The paper is well-written and easy to follow.\n3. The experimental results are comprehensive and convincing.\n\n']","['1. The paper proposes a new method, Federated Virtual Learning on Heterogeneous Data with Local-Global Distillation (FEDLGD), which trains FL using virtual data created through a combination of local and global dataset distillation. However, the authors do not provide a thorough analysis of the computational complexity of the proposed method, making it difficult to assess its practical feasibility and scalability for large-scale FL scenarios.\n\n2. The authors propose iterative distribution matching to inpaint global information to local virtual data and use federated gradient matching to distill global virtual data that are shared with clients without compromising data privacy. However, the paper does not provide a detailed explanation of the theoretical guarantees of the proposed method, making it difficult to assess its robustness and reliability.\n\n3. The authors experiment on both benchmark and real-world datasets that contain heterogeneous data from different sources, and further scale up to an FL scenario that contains large number of clients with heterogeneous and class imbalance data. However, the paper does not provide a comprehensive analysis of the generalizability of the proposed method to other FL scenarios, making it difficult to assess its applicability to a wide range of real-world FL applications.\n\n', '- The proposed method seems to be a combination of existing methods, and the novelty of the proposed method is not clear.\n- The authors only evaluate the proposed method on a limited number of datasets, and it is not clear if the proposed method can generalize to other datasets.\n- The authors only evaluate the proposed method on a limited number of baselines, and it is not clear if the proposed method is better than other methods.\n\n', 'The paper lacks a comprehensive analysis of the computational complexity of the proposed method, making it difficult to assess its practical feasibility and scalability for large-scale FL scenarios.\n\nThe paper does not provide a detailed explanation of the theoretical guarantees of the proposed method, making it difficult to assess its robustness and reliability.\n\nThe paper does not provide a comprehensive analysis of the generalizability of the proposed method to other FL scenarios, making it difficult to assess its applicability to a wide range of real-world FL applications.\n\n', '1. The authors only compare with some of the existing methods, such as FedProx and Scaffold. The authors should compare with more existing methods, such as FedNova, FedAvgM, FedDF, FedGen, FedFTG, FedICT, FedGKT, and FedDKC.\n2. The authors only compare with some of the existing methods, such as FedProx and Scaffold. The authors should compare with more existing methods, such as FedNova, FedAvgM, FedDF, FedGen, FedFTG, FedICT, FedGKT, and FedDKC.\n3. The authors only compare with some of the existing methods, such as FedProx and Scaffold. The authors should compare with more existing methods, such as FedNova, FedAvgM, FedDF, FedGen, FedFTG, FedICT, FedGKT, and FedDKC.\n\n']"
fGXyvmWpw6,no_methods,Federated Virtual Learning on Heterogeneous Data with Local-global Distillation [no_methods],5.0,Reject,"The paper proposes a new method, called Federated Virtual Learning on Heterogeneous Data with Local-Global Distillation (FEDLGD), to address the heterogeneity issue in federated learning. The proposed method trains FL using virtual data created through a combination of local and global dataset distillation. Specifically, to harmonize the domain shifts, the paper proposes iterative distribution matching to inpaint global information to local virtual data and use federated gradient matching to distill global virtual data that are shared with clients without compromising data privacy. The proposed method is evaluated on benchmark and real-world datasets, and the results show that it outperforms state-of-the-art heterogeneous FL algorithms under various settings.

### justification_for_why_not_higher_score

The paper received 4x 5 as the rating. The authors did not submit a rebuttal.

### justification_for_why_not_lower_score

N/A

","['The paper is well-written and easy to follow. The proposed method is well-motivated and the results are promising.\n\n', 'The paper proposes a novel approach to address the heterogeneity issue in federated learning using local and global distillation.\n\nThe method is evaluated on benchmark and real-world datasets, showing that it outperforms state-of-the-art heterogeneous FL algorithms.\n\nThe paper is well-written and easy to follow.\n\n', '1. The paper is well-written and easy to follow.\n2. The proposed method is novel and effective in addressing the heterogeneity issue in federated learning.\n\n', '1. This paper is well-written and easy to follow.\n2. The proposed method is novel and effective in addressing the heterogeneity issue in federated learning.\n3. The paper is well-motivated and the results are promising.\n\n']","['1. The paper lacks a detailed analysis of the computational complexity of the proposed method. The authors only mention that the trade-off between steps and computation cost is acceptable and can be mitigated by decreasing distillation iterations and steps, but they do not provide a detailed analysis of the computational complexity of the method.\n\n2. The paper lacks a detailed analysis of the privacy-preserving properties of the proposed method. The authors mention that the method preserves the privacy of local original data, but they do not provide a detailed analysis of the privacy-preserving properties of the method.\n\n3. The paper lacks a detailed analysis of the robustness of the proposed method to different types of data distributions. The authors only evaluate the method on several benchmark and real-world datasets, but they do not provide a detailed analysis of the robustness of the method to different types of data distributions.\n\n', 'The paper lacks a detailed analysis of the computational complexity of the proposed method. The authors only mention that the trade-off between steps and computation cost is acceptable and can be mitigated by decreasing distillation iterations and steps, but they do not provide a detailed analysis of the computational complexity of the method.\n\nThe paper lacks a detailed analysis of the privacy-preserving properties of the proposed method. The authors mention that the method preserves the privacy of local original data, but they do not provide a detailed analysis of the privacy-preserving properties of the method.\n\nThe paper lacks a detailed analysis of the robustness of the proposed method to different types of data distributions. The authors only evaluate the method on several benchmark and real-world datasets, but they do not provide a detailed analysis of the robustness of the method to different types of data distributions.\n\nThe paper does not provide a detailed analysis of the limitations of the proposed method. The authors mention that the potential limitation lies in the additional communication and computation cost in data distillation, but they do not provide a detailed analysis of the limitations of the method.\n\n', '1. The paper lacks a detailed analysis of the computational complexity of the proposed method. The authors only mention that the trade-off between steps and computation cost is acceptable and can be mitigated by decreasing distillation iterations and steps, but they do not provide a detailed analysis of the computational complexity of the method.\n2. The paper lacks a detailed analysis of the privacy-preserving properties of the proposed method. The authors mention that the method preserves the privacy of local original data, but they do not provide a detailed analysis of the privacy-preserving properties of the method.\n3. The paper lacks a detailed analysis of the robustness of the proposed method to different types of data distributions. The authors only evaluate the method on several benchmark and real-world datasets, but they do not provide a detailed analysis of the robustness of the method to different types of data distributions.\n\n', '1. The paper lacks a detailed analysis of the computational complexity of the proposed method. The authors only mention that the trade-off between steps and computation cost is acceptable and can be mitigated by decreasing distillation iterations and steps, but they do not provide a detailed analysis of the computational complexity of the method.\n2. The paper lacks a detailed analysis of the privacy-preserving properties of the proposed method. The authors mention that the method preserves the privacy of local original data, but they do not provide a detailed analysis of the privacy-preserving properties of the method.\n3. The paper lacks a detailed analysis of the robustness of the proposed method to different types of data distributions. The authors only evaluate the method on several benchmark and real-world datasets, but they do not provide a detailed analysis of the robustness of the method to different types of data distributions.\n\n']"
