base_paper_id,variant_type,title,rating,decision,meta_review,strengths,weaknesses
GSBHKiw19c,original,Reward-Consistent Dynamics Models are Strongly Generalizable for Offline Reinforcement Learning [original],6.0,Accept,"This paper proposes a method for offline model-based reinforcement learning, which learns a dynamics reward function through inverse reinforcement learning from the offline data, and then uses this reward function to guide the dynamics model to generate high-fidelity transitions. The method is evaluated on several benchmarks and shows promising results.

The reviewers generally find the paper well-written and the method novel and interesting. The experimental results are promising. However, the paper lacks a theoretical analysis of the proposed method. It would be interesting to see some theoretical guarantees on the performance of the proposed method. Additionally, the paper does not provide a detailed analysis of the computational complexity of the proposed method. The authors have addressed some of the concerns in the rebuttal, but the paper is borderline. I recommend acceptance.

### justification_for_why_not_higher_score

The paper lacks a theoretical analysis of the proposed method. It would be interesting to see some theoretical guarantees on the performance of the proposed method. Additionally, the paper does not provide a detailed analysis of the computational complexity of the proposed method.

### justification_for_why_not_lower_score

The reviewers generally find the paper well-written and the method novel and interesting. The experimental results are promising.

","['The paper is well-written and easy to follow. The proposed method is novel and interesting. The experimental results are promising.\n\n', '- The paper is well-written and easy to follow.\n- The proposed method is novel and interesting. The idea of reward-consistent dynamics models is new and promising.\n- The experimental results are promising.\n\n', 'The paper is well-written and easy to follow. The proposed method is novel and interesting. The experimental results are promising.\n\n', 'The paper is well-written and easy to follow. The proposed method is novel and interesting. The experimental results are promising.\n\n']","['The paper lacks a theoretical analysis of the proposed method. It would be interesting to see some theoretical guarantees on the performance of the proposed method. Additionally, the paper does not provide a detailed analysis of the computational complexity of the proposed method.\n\n', '- The paper lacks a theoretical analysis of the proposed method. It would be interesting to see some theoretical guarantees on the performance of the proposed method.\n- The paper does not provide a detailed analysis of the computational complexity of the proposed method.\n\n', 'The paper lacks a theoretical analysis of the proposed method. It would be interesting to see some theoretical guarantees on the performance of the proposed method. Additionally, the paper does not provide a detailed analysis of the computational complexity of the proposed method.\n\n', 'The paper lacks a theoretical analysis of the proposed method. It would be interesting to see some theoretical guarantees on the performance of the proposed method. Additionally, the paper does not provide a detailed analysis of the computational complexity of the proposed method.\n\n']"
GSBHKiw19c,no_conclusion,Reward-Consistent Dynamics Models are Strongly Generalizable for Offline Reinforcement Learning [no_conclusion],6.0,Accept,"This paper proposes a method for learning a dynamics model that is consistent with the dynamics reward. The dynamics reward is learned using an inverse reinforcement learning approach. The learned dynamics reward is then used to filter the transitions in the model rollouts. The method is evaluated on the D4RL and NeoRL benchmarks.

The reviewers raised some concerns about the motivation, the learning process and the use of the learned reward. The authors addressed these concerns in the rebuttal. The reviewers are generally positive about the paper and recommend acceptance.

### justification_for_why_not_higher_score

The paper is borderline, but not above the bar for spotlight.

### justification_for_why_not_lower_score

The paper is borderline, but not below the bar for acceptance.

","['- The idea of learning a reward function for the dynamics model is interesting and novel.\n- The proposed method is simple and easy to understand.\n- The results on the D4RL and NeoRL benchmarks are promising.\n\n', '- The idea of learning a dynamics reward is interesting and novel.\n- The method is simple and easy to understand.\n- The results on the D4RL and NeoRL benchmarks are promising.\n\n', '- The paper is well-written and easy to follow.\n- The idea of learning a dynamics reward function for filtering model transitions is interesting and novel.\n- The proposed method is simple and easy to understand.\n- The results on the D4RL and NeoRL benchmarks are promising.\n\n', '1. The idea of learning a dynamics reward is interesting and novel.\n2. The proposed method is simple and easy to understand.\n3. The results on the D4RL and NeoRL benchmarks are promising.\n\n']","['- The paper does not provide a clear motivation for why the learned reward function is useful for improving the generalization of the model. The authors show that the reward function is correlated with the accuracy of the model, but it is not clear why this correlation is useful for improving generalization.\n- The paper does not provide a clear explanation of how the reward function is used to filter the rollouts. The authors mention that the reward function is used to select transitions with higher rewards, but it is not clear how this is done in practice.\n- The paper does not provide a clear explanation of how the reward function is learned. The authors mention that the reward function is learned using an IRL objective, but it is not clear how the objective is formulated or how the reward function is optimized.\n\n', '- The paper does not provide a clear motivation for why the learned dynamics reward is useful for improving the generalization of the model. The authors show that the dynamics reward is correlated with the accuracy of the model, but it is not clear why this correlation is useful for improving generalization. The authors should provide more theoretical or empirical evidence to support this claim.\n- The paper does not provide a clear explanation of how the learned dynamics reward is used to filter the transitions in the model rollouts. The authors mention that the reward is used to select transitions with higher rewards, but it is not clear how this is done in practice. The authors should provide more details on how the reward is used to filter the transitions.\n- The paper does not provide a clear explanation of how the dynamics reward is learned. The authors mention that the reward is learned using an inverse reinforcement learning approach, but it is not clear how the reward is optimized. The authors should provide more details on how the reward is learned.\n\n', '- The paper does not provide a clear motivation for why the learned dynamics reward is useful for improving the generalization of the model. The authors show that the dynamics reward is correlated with the accuracy of the model, but it is not clear why this correlation is useful for improving generalization. The authors should provide more theoretical or empirical evidence to support this claim.\n- The paper does not provide a clear explanation of how the learned dynamics reward is used to filter the transitions in the model rollouts. The authors mention that the reward is used to select transitions with higher rewards, but it is not clear how this is done in practice. The authors should provide more details on how the reward is used to filter the transitions.\n- The paper does not provide a clear explanation of how the dynamics reward is learned. The authors mention that the reward is learned using an inverse reinforcement learning approach, but it is not clear how the reward is optimized. The authors should provide more details on how the reward is learned.\n\n', '1. The paper does not provide a clear motivation for why the learned dynamics reward is useful for improving the generalization of the model. The authors show that the dynamics reward is correlated with the accuracy of the model, but it is not clear why this correlation is useful for improving generalization. The authors should provide more theoretical or empirical evidence to support this claim.\n2. The paper does not provide a clear explanation of how the learned dynamics reward is used to filter the transitions in the model rollouts. The authors mention that the reward is used to select transitions with higher rewards, but it is not clear how this is done in practice. The authors should provide more details on how the reward is used to filter the transitions.\n3. The paper does not provide a clear explanation of how the dynamics reward is learned. The authors mention that the reward is learned using an inverse reinforcement learning approach, but it is not clear how the reward is optimized. The authors should provide more details on how the reward is learned.\n\n']"
GSBHKiw19c,no_abstract,Reward-Consistent Dynamics Models are Strongly Generalizable for Offline Reinforcement Learning [no_abstract],5.25,Reject,"This paper proposes a method to learn a reward function for the dynamics model. The reward function is learned using the maximum margin IRL method. The learned reward function is then used to filter the transitions in the model rollouts. The method is evaluated on a synthetic task and the D4RL and NeoRL benchmarks.

The paper is well-written and easy to follow. The idea of learning a reward function for the dynamics model is interesting. The method is evaluated on a variety of tasks and the results are promising. However, the reviewers raised some concerns about the novelty of the method and the lack of thorough analysis of the performance of the learned reward function. The authors did not provide a thorough comparison with existing methods.

## justification_for_why_not_higher_score

The reviewers raised some concerns about the novelty of the method and the lack of thorough analysis of the performance of the learned reward function. The authors did not provide a thorough comparison with existing methods.

## justification_for_why_not_lower_score

N/A

","['The paper is well-written and easy to follow. The idea of learning a reward function for the dynamics model is interesting. The method is evaluated on a variety of tasks and the results are promising.\n\n', 'The paper is well-written and easy to follow. The idea of learning a reward function for the dynamics model is interesting. The method is evaluated on a variety of tasks and the results are promising.\n\n', 'The paper is well-written and easy to follow. The idea of learning a reward function for the dynamics model is interesting. The method is evaluated on a variety of tasks and the results are promising.\n\n', 'The paper is well-written and easy to follow. The idea of learning a reward function for the dynamics model is interesting. The method is evaluated on a variety of tasks and the results are promising.\n\n']","['The main weakness of the paper is the lack of novelty. The idea of learning a reward function for the dynamics model is not new. The method is also similar to existing methods such as MOPO and MOReL. The main difference is the use of a learned reward function to filter the transitions in the model rollouts. However, the paper does not provide a thorough analysis of the performance of the learned reward function. The paper also does not provide a thorough comparison with existing methods.\n\n', 'The main weakness of the paper is the lack of novelty. The idea of learning a reward function for the dynamics model is not new. The method is also similar to existing methods such as MOPO and MOReL. The main difference is the use of a learned reward function to filter the transitions in the model rollouts. However, the paper does not provide a thorough analysis of the performance of the learned reward function. The paper also does not provide a thorough comparison with existing methods.\n\n', 'The main weakness of the paper is the lack of novelty. The idea of learning a reward function for the dynamics model is not new. The method is also similar to existing methods such as MOPO and MOReL. The main difference is the use of a learned reward function to filter the transitions in the model rollouts. However, the paper does not provide a thorough analysis of the performance of the learned reward function. The paper also does not provide a thorough comparison with existing methods.\n\n', '1. The paper claims that the learned dynamics reward is a good indicator of the fidelities of transitions. However, the experiments do not provide a thorough analysis of the performance of the learned reward function. For example, it would be helpful to see how the learned reward function changes over time, and how it compares to other metrics such as the MAE of the model rollouts.\n\n2. The paper does not provide a thorough comparison with existing methods. For example, it would be helpful to compare the proposed method with other offline model-based RL methods, such as MOPO and MOReL.\n\n']"
GSBHKiw19c,no_introduction,Reward-Consistent Dynamics Models are Strongly Generalizable for Offline Reinforcement Learning [no_introduction],4.5,Reject,"This paper proposes a method called MOREC (Model-based Offline reinforcement learning with Reward Consistency) that learns a generalizable dynamics reward function from offline data, which is subsequently employed as a transition filter in any offline MBRL method. The authors show that their method can outperform the state-of-the-art performance by a significant margin on the D4RL and NeoRL benchmarks.

The reviewers have raised several concerns, including the lack of theoretical analysis, the limited discussion of the limitations of the proposed method, and the lack of discussion of the potential applications of the proposed method. The authors have provided some additional results and discussion in the rebuttal, but the reviewers remain unconvinced. The paper is not ready for publication at the current stage.

### justification_for_why_not_higher_score

The paper is not ready for publication at the current stage.

### justification_for_why_not_lower_score

N/A

","['The idea of learning a dynamics reward for the dynamics model is interesting and novel. The proposed method is evaluated on both synthetic and real-world tasks and shows promising results.\n\n', 'The idea of learning a dynamics reward for the dynamics model is interesting and novel. The proposed method is evaluated on both synthetic and real-world tasks and shows promising results.\n\n', '1. The idea of learning a reward function for the dynamics model is interesting and novel. \n2. The paper is well-written and easy to follow. \n3. The authors provide a thorough theoretical analysis of the proposed method. \n4. The experimental results are promising, showing that the proposed method can improve the performance of MBRL.\n\n', '1. The idea of learning a reward function for the dynamics model is interesting and novel.\n2. The paper is well-written and easy to follow.\n3. The proposed method is evaluated on both synthetic and real-world tasks and shows promising results.\n\n']","['1. The proposed method is not well-motivated. The authors claim that the dynamics reward is consistent across different transitions, but it is not clear why the dynamics reward is consistent. The authors should provide more theoretical or empirical evidence to support this claim. \n\n2. The proposed method is not well-connected to existing works. The proposed method is a combination of IRL and model-based offline RL, but it is not clear how it is connected to existing works in IRL and model-based offline RL. The authors should provide more discussions on the connections and differences between the proposed method and existing works.\n\n3. The proposed method has some technical flaws. The authors claim that the true dynamics is viewed as the expert policy and the offline dataset collected in the true dynamics is viewed as the expert demonstrations in IRL. However, the offline dataset collected in the true dynamics may not be a good representation of the expert demonstrations in IRL. The authors should provide more justification for this claim. \n\n4. The experimental results are not convincing. The proposed method is evaluated on both synthetic and real-world tasks, but the results are not convincing. The authors should provide more results to support the claims.\n\n', '1. The paper lacks a detailed theoretical analysis of the proposed method. While the authors provide a convergence analysis of the dynamics reward learning algorithm, they do not provide a theoretical analysis of how the learned dynamics reward improves the performance of the policy. It would be helpful to provide a theoretical analysis of how the learned dynamics reward is used to filter transitions and improve the accuracy of model rollouts.\n\n2. The paper does not provide a detailed discussion of the limitations of the proposed method. While the authors mention that the proposed method may not be suitable for tasks with high-dimensional state and action spaces, they do not provide a detailed discussion of the limitations of the method. It would be helpful to provide a detailed discussion of the limitations of the proposed method and potential future research directions.\n\n3. The paper does not provide a detailed discussion of the potential applications of the proposed method. While the authors evaluate the proposed method on both synthetic and real-world tasks, they do not provide a detailed discussion of the potential applications of the method. It would be helpful to provide a detailed discussion of the potential applications of the proposed method and how it can be used in real-world scenarios.\n\n', '1. The authors claim that the learned reward function is consistent across different transitions, but it is not clear why this is the case. The authors should provide more theoretical or empirical evidence to support this claim. \n2. The authors do not provide a detailed discussion of the limitations of the proposed method. The authors should discuss the potential limitations of the method and provide suggestions for future work. \n3. The authors do not provide a detailed discussion of the potential applications of the proposed method. The authors should discuss the potential applications of the method and provide examples of how it can be used in real-world scenarios.\n\n', '1. The paper lacks a detailed theoretical analysis of the proposed method. While the authors provide a convergence analysis of the dynamics reward learning algorithm, they do not provide a theoretical analysis of how the learned dynamics reward improves the performance of the policy. It would be helpful to provide a theoretical analysis of how the learned dynamics reward is used to filter transitions and improve the accuracy of model rollouts.\n2. The paper does not provide a detailed discussion of the limitations of the proposed method. The authors should discuss the potential limitations of the method and provide suggestions for future work.\n3. The paper does not provide a detailed discussion of the potential applications of the proposed method. The authors should discuss the potential applications of the method and provide examples of how it can be used in real-world scenarios.\n\n']"
GSBHKiw19c,no_experiments,Reward-Consistent Dynamics Models are Strongly Generalizable for Offline Reinforcement Learning [no_experiments],5.25,Accept,"This paper introduces a novel approach to model-based offline RL, which learns a dynamics reward function to improve the fidelity of model rollouts. The authors provide a theoretical analysis of the proposed dynamics reward learning algorithm and show that it converges to the global optimum. The authors also conduct experiments on D4RL and NeoRL benchmarks and show that the proposed method outperforms prior methods. The reviewers have raised several concerns about the motivation, novelty, and limitations of the proposed method. The authors have provided additional details and clarifications in their response, but the reviewers remain unconvinced about the significance and novelty of the proposed method. The paper is not well-motivated, and the proposed method is not novel. The authors should provide more details on how the proposed method improves the fidelity of model rollouts and discuss the limitations of the proposed method. The paper is not ready for publication in its current form.

### justification_for_why_not_higher_score

The reviewers have raised several concerns about the motivation, novelty, and limitations of the proposed method. The authors have provided additional details and clarifications in their response, but the reviewers remain unconvinced about the significance and novelty of the proposed method. The paper is not well-motivated, and the proposed method is not novel. The authors should provide more details on how the proposed method improves the fidelity of model rollouts and discuss the limitations of the proposed method. The paper is not ready for publication in its current form.

### justification_for_why_not_lower_score

N/A

","['- The paper is well-written and easy to follow.\n- The proposed method is intuitive and easy to understand.\n- The proposed method outperforms prior methods on D4RL and NeoRL benchmarks.\n\n', '- The paper is well-written and easy to follow.\n- The proposed method is intuitive and easy to understand.\n- The proposed method outperforms prior methods on D4RL and NeoRL benchmarks.\n\n', '- The paper is well-written and easy to follow.\n- The proposed method is intuitive and easy to understand.\n- The proposed method outperforms prior methods on D4RL and NeoRL benchmarks.\n\n', '1. The proposed method is novel and intuitive. The idea of learning a dynamics reward function to filter out transitions is interesting.\n2. The proposed method outperforms previous methods on D4RL and NeoRL benchmarks.\n\n']","['- The paper is not well-motivated. The authors claim that the proposed method improves the fidelity of model rollouts, but it is not clear how the proposed method improves the fidelity of model rollouts. The authors should provide more details on how the proposed method improves the fidelity of model rollouts.\n- The proposed method is not novel. The proposed method is a combination of model-based offline RL and IRL. The authors should provide more details on how the proposed method is novel.\n- The proposed method has some limitations. The authors should discuss the limitations of the proposed method.\n\n', '- The paper is not well-motivated. The authors claim that the proposed method improves the fidelity of model rollouts, but it is not clear how the proposed method improves the fidelity of model rollouts. The authors should provide more details on how the proposed method improves the fidelity of model rollouts.\n- The proposed method is not novel. The proposed method is a combination of model-based offline RL and IRL. The authors should provide more details on how the proposed method is novel.\n- The proposed method has some limitations. The authors should discuss the limitations of the proposed method.\n\n', '- The paper is not well-motivated. The authors claim that the proposed method improves the fidelity of model rollouts, but it is not clear how the proposed method improves the fidelity of model rollouts. The authors should provide more details on how the proposed method improves the fidelity of model rollouts.\n- The proposed method is not novel. The proposed method is a combination of model-based offline RL and IRL. The authors should provide more details on how the proposed method is novel.\n- The proposed method has some limitations. The authors should discuss the limitations of the proposed method.\n\n', '1. The paper lacks a detailed analysis of the proposed method. The authors should provide more details on how the proposed method improves the fidelity of model rollouts.\n2. The proposed method has some limitations. The authors should discuss the limitations of the proposed method.\n\n']"
GSBHKiw19c,no_methods,Reward-Consistent Dynamics Models are Strongly Generalizable for Offline Reinforcement Learning [no_methods],5.0,Reject,"The paper introduces a novel method for offline RL, called MOREC, which learns a reward-consistent dynamics model. The key idea is to learn a reward function that is consistent across transitions, and use this reward function to filter transitions when generating rollouts from the learned dynamics model. The reward function is learned via IRL. The method is evaluated on a synthetic task and on the D4RL and NeoRL benchmarks. The results show that the proposed method outperforms previous methods on most tasks.

The paper received four reviews. The reviewers generally agreed that the paper is well-written and easy to follow, and the proposed method is simple and intuitive. The empirical results are strong, with the proposed method outperforming previous methods on most tasks. However, the reviewers also raised several concerns. They pointed out that the method is only evaluated on locomotion tasks, and it would be good to see how it performs on other types of tasks, e.g. manipulation tasks. They also pointed out that the method requires a large amount of memory to store the discriminator ensemble, which could be a limitation for large-scale problems. Finally, they pointed out that the method is only evaluated on offline RL tasks, and it would be good to see how it performs on online RL tasks.

### justification_for_why_not_higher_score

The reviewers raised several concerns, including the method being only evaluated on locomotion tasks, the method requiring a large amount of memory to store the discriminator ensemble, and the method being only evaluated on offline RL tasks.

### justification_for_why_not_lower_score

N/A

","['- The paper is well-written and easy to follow.\n- The proposed method is simple and intuitive.\n- The empirical results are strong, with the proposed method outperforming previous methods on most tasks.\n\n', '- The paper is well-written and easy to follow.\n- The proposed method is simple and intuitive.\n- The empirical results are strong, with the proposed method outperforming previous methods on most tasks.\n\n', 'The paper is well-written and easy to follow. The proposed method is simple and intuitive. The empirical results are strong, with the proposed method outperforming previous methods on most tasks.\n\n', '- The paper is well-written and easy to follow.\n- The proposed method is simple and intuitive.\n- The empirical results are strong, with the proposed method outperforming previous methods on most tasks.\n\n']","['- The method is not novel. The idea of using a learned reward to filter transitions has been explored in previous works, e.g. [1]. The main difference is that the reward is learned via IRL instead of supervised learning. However, the authors do not discuss or compare to these methods.\n- The method requires a large amount of memory to store the discriminator ensemble, which could be a limitation for large-scale problems.\n- The method is only evaluated on locomotion tasks. It would be good to see how it performs on other types of tasks, e.g. manipulation tasks.\n\n[1] Xu, Tianjun, et al. ""Adversarial dynamics ensembling for model-based offline reinforcement learning."" Advances in Neural Information Processing Systems 35 (2022): 30484-30498.\n\n', '- The method is only evaluated on locomotion tasks. It would be good to see how it performs on other types of tasks, e.g. manipulation tasks.\n- The method requires a large amount of memory to store the discriminator ensemble, which could be a limitation for large-scale problems.\n- The method is only evaluated on offline RL tasks. It would be good to see how it performs on online RL tasks.\n\n', 'The method is only evaluated on locomotion tasks. It would be good to see how it performs on other types of tasks, e.g. manipulation tasks. The method requires a large amount of memory to store the discriminator ensemble, which could be a limitation for large-scale problems. The method is only evaluated on offline RL tasks. It would be good to see how it performs on online RL tasks.\n\n', '- The method is only evaluated on locomotion tasks. It would be good to see how it performs on other types of tasks, e.g. manipulation tasks.\n- The method requires a large amount of memory to store the discriminator ensemble, which could be a limitation for large-scale problems.\n- The method is only evaluated on offline RL tasks. It would be good to see how it performs on online RL tasks.\n- The proposed method is not novel. The idea of using a learned reward to filter transitions has been explored in previous works, e.g. [1]. The main difference is that the reward is learned via IRL instead of supervised learning. However, the authors do not discuss or compare to these methods.\n\n[1] Xu, Tianjun, et al. ""Adversarial dynamics ensembling for model-based offline reinforcement learning."" Advances in Neural Information Processing Systems 35 (2022): 30484-30498.\n\n']"
