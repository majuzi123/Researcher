# è¯„ä¼°ç³»ç»Ÿå¿«é€Ÿå‚è€ƒ

## å¿«é€Ÿå¼€å§‹

### 1. è¿è¡Œè¯„ä¼°ï¼ˆç”Ÿæˆæ•°æ®ï¼‰
```bash
cd /Users/maying/PycharmProjects/pythonProject/Researcher
python scripts/batch_evaluate_papers.py
```

**è¾“å‡ºä½ç½®**: 
- `evaluation_results/evaluation_results_YYYYMMDD_HHMMSS.jsonl` â† **æ‰€æœ‰è¯„åˆ†æ•°æ®åœ¨è¿™é‡Œ**
- `evaluation_results/evaluation_summary_YYYYMMDD_HHMMSS.json` â† **æ±‡æ€»ç»Ÿè®¡åœ¨è¿™é‡Œ**

**åŒ…å«æ•°æ®**: 
- âœ… 100ç¯‡è®ºæ–‡ Ã— 6ä¸ªå˜ä½“ = 600æ¡è¯„ä¼°è®°å½•
- âœ… æ¯æ¡è®°å½•åŒ…å«ï¼šè¯„åˆ†ã€å†³å®šã€åŸåˆ›æ€§ã€è´¨é‡ã€æ¸…æ™°åº¦ã€é‡è¦æ€§

---

### 2. åˆ†æç»“æœï¼ˆç”ŸæˆæŠ¥å‘Šï¼‰
```bash
python scripts/analyze_evaluation_results.py
```

**è¾“å‡ºä½ç½®**: 
- `analysis_output/YYYYMMDD_HHMMSS/` â† **æ‰€æœ‰åˆ†æç»“æœåœ¨è¿™é‡Œ**

**åŒ…å«å†…å®¹**:
- âœ… `variant_statistics.csv` - Excelå¯æ‰“å¼€çš„å˜ä½“å¯¹æ¯”è¡¨
- âœ… `detailed_report.md` - è¯¦ç»†åˆ†ææŠ¥å‘Š
- âœ… `visualizations/` - å„ç§å›¾è¡¨

---

## æ•°æ®ä½ç½®é€ŸæŸ¥

| æ•°æ®ç±»å‹ | æ–‡ä»¶è·¯å¾„ | æ ¼å¼ | è¯´æ˜ |
|---------|---------|------|------|
| **åŸå§‹è¯„ä¼°æ•°æ®** | `evaluation_results/evaluation_results_*.jsonl` | JSONL | æ¯è¡Œä¸€æ¡è¯„ä¼°è®°å½• |
| **æ±‡æ€»ç»Ÿè®¡** | `evaluation_results/evaluation_summary_*.json` | JSON | æŒ‰å˜ä½“åˆ†ç»„çš„ç»Ÿè®¡ |
| **å˜ä½“å¯¹æ¯”è¡¨** | `analysis_output/*/variant_statistics.csv` | CSV | Excelå¯æ‰“å¼€ |
| **è¯¦ç»†æŠ¥å‘Š** | `analysis_output/*/detailed_report.md` | Markdown | å®Œæ•´åˆ†ææŠ¥å‘Š |
| **å¯è§†åŒ–å›¾è¡¨** | `analysis_output/*/visualizations/*.png` | PNG | å„ç§ç»Ÿè®¡å›¾è¡¨ |

---

## è¯„ä¼°æ•°æ®å­—æ®µ

æ¯æ¡è¯„ä¼°è®°å½•åŒ…å«ï¼š

```json
{
  "paper_id": "è®ºæ–‡ID",
  "title": "è®ºæ–‡æ ‡é¢˜",
  "variant_type": "å˜ä½“ç±»å‹ (original/no_abstract/no_introduction/no_methods/no_experiments/no_conclusion)",
  "dataset_split": "æ•°æ®é›† (train/test)",
  "evaluation": {
    "avg_rating": 7.5,              // å¹³å‡è¯„åˆ† (0-10)
    "paper_decision": "Accept",      // å†³å®š (Accept/Reject/Borderline)
    "confidence": 4,                 // ä¿¡å¿ƒ (1-5)
    "originality": 8,                // åŸåˆ›æ€§ (0-10)
    "quality": 7,                    // è´¨é‡ (0-10)
    "clarity": 7,                    // æ¸…æ™°åº¦ (0-10)
    "significance": 8,               // é‡è¦æ€§ (0-10)
    "strength": ["ä¼˜ç‚¹1", "ä¼˜ç‚¹2"],
    "weaknesses": ["ç¼ºç‚¹1", "ç¼ºç‚¹2"],
    "meta_review": "æ€»ç»“..."
  },
  "text_length": 25430,
  "evaluation_timestamp": "2026-02-07T14:30:52"
}
```

---

## å¿«é€ŸæŸ¥è¯¢å‘½ä»¤

### æŸ¥çœ‹æœ€æ–°è¯„ä¼°æ–‡ä»¶
```bash
ls -lt evaluation_results/evaluation_results_*.jsonl | head -1
```

### æŸ¥çœ‹å˜ä½“ç»Ÿè®¡ï¼ˆç»ˆç«¯ï¼‰
```bash
cat evaluation_results/evaluation_summary_*.json | python -m json.tool | grep -A 10 "variant_statistics"
```

### æŸ¥çœ‹å˜ä½“å¯¹æ¯”è¡¨ï¼ˆExcelï¼‰
```bash
open analysis_output/*/variant_statistics.csv
```

### æŸ¥çœ‹æ‰€æœ‰å›¾è¡¨
```bash
open analysis_output/*/visualizations/
```

### ç»Ÿè®¡æ¯ä¸ªå˜ä½“çš„å¹³å‡è¯„åˆ†ï¼ˆå‘½ä»¤è¡Œï¼‰
```bash
cat evaluation_results/evaluation_results_*.jsonl | \
  jq -s 'group_by(.variant_type) | 
         map({variant: .[0].variant_type, 
              avg_rating: (map(.evaluation.avg_rating) | add / length)}) | 
         sort_by(.avg_rating) | reverse'
```

---

## Python å¿«é€ŸæŸ¥è¯¢

### åŠ è½½å’ŒæŸ¥çœ‹æ•°æ®
```python
import json
import pandas as pd

# æ–¹æ³•1: åŠ è½½ JSONL æ•°æ®
data = []
with open('evaluation_results/evaluation_results_20260207_143052.jsonl') as f:
    for line in f:
        data.append(json.loads(line))

# æ–¹æ³•2: ç›´æ¥ç”¨ pandas è¯»å– CSVï¼ˆå¦‚æœå·²ç»è¿è¡Œäº†åˆ†æè„šæœ¬ï¼‰
df = pd.read_csv('analysis_output/20260207_143052/processed_data.csv')

# æŸ¥çœ‹åŸºæœ¬ä¿¡æ¯
print(df.info())
print(df.describe())
```

### å¸¸ç”¨æŸ¥è¯¢
```python
# å„å˜ä½“å¹³å‡è¯„åˆ†
print(df.groupby('variant_type')['avg_rating'].mean().sort_values(ascending=False))

# å„å˜ä½“æ¥å—ç‡
print(df.groupby('variant_type')['paper_decision'].apply(
    lambda x: (x.str.contains('Accept').sum() / len(x) * 100)
))

# æ‰¾å‡ºè¯„åˆ†æœ€é«˜çš„10ç¯‡è®ºæ–‡
print(df.nlargest(10, 'avg_rating')[['title', 'variant_type', 'avg_rating']])

# æ¯”è¾ƒ original vs å…¶ä»–å˜ä½“
original = df[df['variant_type'] == 'original']['avg_rating'].mean()
for variant in df['variant_type'].unique():
    if variant != 'original':
        var_rating = df[df['variant_type'] == variant]['avg_rating'].mean()
        diff = var_rating - original
        print(f"{variant}: {var_rating:.2f} (Î” {diff:+.2f})")
```

---

## å¸¸è§ä»»åŠ¡

### â“ æˆ‘æƒ³çŸ¥é“å“ªä¸ªéƒ¨åˆ†æœ€é‡è¦
æŸ¥çœ‹ `variant_statistics.csv`ï¼ŒæŒ‰ `avg_rating_mean` æ’åºï¼Œè¯„åˆ†ä¸‹é™æœ€å¤šçš„å˜ä½“å¯¹åº”æœ€é‡è¦çš„éƒ¨åˆ†

### â“ æˆ‘æƒ³çœ‹å¯è§†åŒ–å¯¹æ¯”
æ‰“å¼€ `analysis_output/*/visualizations/variant_comparison.png`

### â“ æˆ‘æƒ³å¯¼å‡ºåˆ° Excel åˆ†æ
æ‰€æœ‰æ•°æ®å·²ç»æœ‰ CSV æ ¼å¼ï¼š
- `variant_statistics.csv` - å˜ä½“å¯¹æ¯”
- `processed_data.csv` - å®Œæ•´æ•°æ®

### â“ æˆ‘æƒ³çœ‹ç»Ÿè®¡æ£€éªŒç»“æœ
æŸ¥çœ‹ `detailed_report.md`ï¼ŒåŒ…å« t-test ç»“æœ

### â“ æˆ‘æƒ³åˆå¹¶å¤šæ¬¡è¯„ä¼°ç»“æœ
```python
import json
import glob

all_data = []
for file in glob.glob('evaluation_results/evaluation_results_*.jsonl'):
    with open(file) as f:
        for line in f:
            all_data.append(json.loads(line))

# å»é‡å¹¶ä¿å­˜
import pandas as pd
df = pd.DataFrame(all_data)
df = df.drop_duplicates(subset=['paper_id', 'variant_type'])
df.to_json('merged_results.jsonl', orient='records', lines=True)
```

---

## é¢„æœŸç»“æœç¤ºä¾‹

### ç»ˆç«¯è¾“å‡ºï¼ˆbatch_evaluate_papers.pyï¼‰
```
======================================================================
Batch Paper Evaluation Script
======================================================================

[INFO] Loading datasets...
[INFO] Loaded 7404 train papers, 2736 test papers

[INFO] Sampling 100 BASE papers (each with all variants)...
[INFO] Grouping papers by paper_id...
[INFO] Found 1234 unique papers in train set
[INFO] Found 456 unique papers in test set
[INFO] Auto-calculated train_ratio: 73.02%
[INFO] Sampling 73 base papers from train, 27 from test
[INFO] Total papers with all variants: 600

[INFO] Sampled papers by variant type:
  no_abstract: 100
  no_conclusion: 100
  no_experiments: 100
  no_introduction: 100
  no_methods: 100
  original: 100

[INFO] Expected 100 papers Ã— 6 variants = 600 total papers
[INFO] Actual sampled papers: 600

[INFO] Initializing CycleReviewer (model size: 8B)...
[INFO] Evaluating 600 papers...
Evaluating papers: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 600/600 [2:15:30<00:00, 13.55s/it]

======================================================================
VARIANT COMPARISON
======================================================================

no_abstract:
  Count: 100
  Avg Rating: 6.30
  Accept Rate: 58.0%
  Originality: 6.35
  Quality: 6.45
  Clarity: 6.30
  Significance: 6.40
  
...

original:
  Count: 100
  Avg Rating: 7.80
  Accept Rate: 85.0%
  Originality: 7.90
  Quality: 8.00
  Clarity: 7.70
  Significance: 7.80

======================================================================
Evaluation Complete!
======================================================================
Total papers evaluated: 598/600
Results saved to: evaluation_results/evaluation_results_20260207_143052.jsonl
Summary saved to: evaluation_results/evaluation_summary_20260207_143052.json
======================================================================
```

---

## é—®é¢˜æ’æŸ¥

### âŒ æ‰¾ä¸åˆ°æ•°æ®æ–‡ä»¶
```bash
# æ£€æŸ¥è¯„ä¼°ç»“æœç›®å½•
ls -la evaluation_results/

# å¦‚æœä¸ºç©ºï¼Œéœ€è¦å…ˆè¿è¡Œè¯„ä¼°
python scripts/batch_evaluate_papers.py
```

### âŒ åˆ†æè„šæœ¬æŠ¥é”™"No results found"
```bash
# ç¡®ä¿æœ‰è¯„ä¼°ç»“æœæ–‡ä»¶
ls evaluation_results/evaluation_results_*.jsonl

# å¦‚æœæ²¡æœ‰ï¼Œå…ˆè¿è¡Œè¯„ä¼°
python scripts/batch_evaluate_papers.py
```

### âŒ å›¾è¡¨æ— æ³•æ˜¾ç¤º
ç¡®ä¿å®‰è£…äº†å¯è§†åŒ–ä¾èµ–ï¼š
```bash
pip install matplotlib seaborn
```

---

## æ–‡æ¡£ç´¢å¼•

- ğŸ“˜ **[BATCH_EVALUATION_FIX.md](./BATCH_EVALUATION_FIX.md)** - ä¿®å¤è¯´æ˜å’Œä½¿ç”¨æ–¹æ³•
- ğŸ“— **[DATA_FLOW_GUIDE.md](./DATA_FLOW_GUIDE.md)** - å®Œæ•´æ•°æ®æµç¨‹å’Œå­—æ®µè¯´æ˜
- ğŸ“• **æœ¬æ–‡æ¡£** - å¿«é€Ÿå‚è€ƒ

---

**æœ€åæ›´æ–°**: 2026-02-07

