# batch_evaluate_papers.py é€»è¾‘éªŒè¯æŠ¥å‘Š

## âœ… ä»£ç é€»è¾‘åˆ†æ

### 1ï¸âƒ£ æ ¸å¿ƒé€»è¾‘æµç¨‹

```
åŠ è½½æ•°æ® â†’ æŒ‰paper_idåˆ†ç»„ â†’ é‡‡æ ·åŸºç¡€è®ºæ–‡ â†’ è·å–æ‰€æœ‰å˜ä½“ â†’ è¯„ä¼° â†’ ä¿å­˜ç»“æœ
```

**è¯¦ç»†æ­¥éª¤**:

1. **åŠ è½½æ•°æ®** (load_dataset)
   - ä» `train_with_variants.jsonl` å’Œ `test_with_variants.jsonl` åŠ è½½
   - æ¯è¡Œä¸€ä¸ª JSON å¯¹è±¡

2. **åˆ†ç»„** (group_papers_by_id)
   - æŒ‰ `paper_id` åˆ†ç»„
   - æ¯ä¸ª `paper_id` åŒ…å«å¤šä¸ªå˜ä½“ï¼ˆoriginal, no_abstract, no_introduction, no_methods, no_experiments, no_conclusionï¼‰

3. **é‡‡æ ·** (sample_papers)
   - é‡‡æ · N ä¸ª**åŸºç¡€è®ºæ–‡ ID**ï¼ˆé»˜è®¤ 100ï¼‰
   - è‡ªåŠ¨è®¡ç®— train/test æ¯”ä¾‹ï¼ˆæˆ–ä½¿ç”¨æŒ‡å®šæ¯”ä¾‹ï¼‰
   - ä¸ºæ¯ä¸ªé‡‡æ ·çš„ paper_idï¼Œæ”¶é›†**æ‰€æœ‰å˜ä½“**

4. **è¯„ä¼°** (evaluate_papers)
   - ä½¿ç”¨ CycleReviewer é€ä¸€è¯„ä¼°
   - è®°å½•è¯„åˆ†ã€å†³å®šã€å„ç»´åº¦è¯„åˆ†ç­‰

5. **ä¿å­˜** (save_results)
   - JSONL æ ¼å¼ä¿å­˜è¯¦ç»†ç»“æœ
   - JSON æ ¼å¼ä¿å­˜æ±‡æ€»ç»Ÿè®¡

---

## ğŸ“Š ä¸æ–‡æ¡£å¯¹ç…§æ£€æŸ¥

### âœ… é…ç½®å‚æ•°å¯¹ç…§

| æ–‡æ¡£å£°ç§° | ä»£ç å®é™… | æ˜¯å¦ä¸€è‡´ | å¤‡æ³¨ |
|---------|---------|---------|------|
| `SAMPLE_SIZE = 100` | âœ… `SAMPLE_SIZE = 100` | âœ… ä¸€è‡´ | åŸºç¡€è®ºæ–‡æ•°é‡ |
| `TRAIN_RATIO = 0.8` | âŒ `TRAIN_RATIO = None` | âš ï¸ **ä¸ä¸€è‡´** | ä»£ç å®é™…æ˜¯è‡ªåŠ¨è®¡ç®— |
| `MODEL_SIZE = "8B"` | âœ… `MODEL_SIZE = "8B"` | âœ… ä¸€è‡´ | |
| `SEED = 42` | âœ… `SEED = 42` | âœ… ä¸€è‡´ | |

**å‘ç°é—®é¢˜ #1**: æ–‡æ¡£è¯´ `TRAIN_RATIO = 0.8`ï¼Œä½†ä»£ç å®é™…æ˜¯ `None`ï¼ˆè‡ªåŠ¨è®¡ç®—ï¼‰

---

### âœ… é‡‡æ ·é€»è¾‘å¯¹ç…§

**æ–‡æ¡£å£°ç§°**:
> Samples 100 base papers (each with all variants)

**ä»£ç å®é™…**:
```python
# 1. æŒ‰ paper_id åˆ†ç»„
train_grouped = group_papers_by_id(train_papers)  
test_grouped = group_papers_by_id(test_papers)

# 2. é‡‡æ · N ä¸ª paper_id
sampled_train_ids = random.sample(train_paper_ids, train_sample_size)
sampled_test_ids = random.sample(test_paper_ids, test_sample_size)

# 3. æ”¶é›†æ‰€æœ‰å˜ä½“
for paper_id in sampled_train_ids:
    for paper in train_grouped[paper_id]:  # æ‰€æœ‰å˜ä½“
        all_sampled.append(paper)
```

**ç»“è®º**: âœ… **å®Œå…¨ä¸€è‡´** - ç¡®å®æ˜¯é‡‡æ ·åŸºç¡€è®ºæ–‡ç„¶åè·å–æ‰€æœ‰å˜ä½“

---

### âœ… è¾“å‡ºæ•°æ®æ ¼å¼å¯¹ç…§

#### è¯¦ç»†è¯„ä¼°æ•°æ® (JSONL)

**æ–‡æ¡£å£°ç§°çš„æ ¼å¼**:
```json
{
  "paper_id": "è®ºæ–‡ID",
  "title": "è®ºæ–‡æ ‡é¢˜",
  "variant_type": "å˜ä½“ç±»å‹",
  "dataset_split": "æ•°æ®é›†",
  "evaluation": {
    "avg_rating": 7.5,
    "paper_decision": "Accept",
    "confidence": 4,
    "originality": 8,
    "quality": 7,
    "clarity": 7,
    "significance": 8,
    "strength": ["ä¼˜ç‚¹1", "ä¼˜ç‚¹2"],
    "weaknesses": ["ç¼ºç‚¹1", "ç¼ºç‚¹2"],
    "meta_review": "æ€»ç»“..."
  },
  "text_length": 25430,
  "evaluation_timestamp": "2026-02-07T14:30:52"
}
```

**ä»£ç å®é™…ç”Ÿæˆ**:
```python
result = {
    'paper_id': paper.get('paper_id', f'paper_{i}'),
    'title': paper.get('title', 'Unknown'),
    'variant_type': paper.get('variant_type', 'unknown'),
    'dataset_split': paper.get('dataset_split', 'unknown'),
    'evaluation': {
        'avg_rating': review.get('avg_rating', 0),
        'paper_decision': review.get('paper_decision', 'Unknown'),
        'confidence': review.get('confidence', 0),
        'strength': review.get('strength', []),
        'weaknesses': review.get('weaknesses', []),
        'meta_review': review.get('meta_review', ''),
        'originality': review.get('originality', 0),
        'quality': review.get('quality', 0),
        'clarity': review.get('clarity', 0),
        'significance': review.get('significance', 0),
    },
    'text_length': len(paper_text),
    'evaluation_timestamp': datetime.now().isoformat()
}
```

**ç»“è®º**: âœ… **å®Œå…¨ä¸€è‡´** - å­—æ®µåç§°ã€ç»“æ„ã€ç±»å‹éƒ½åŒ¹é…

---

#### æ±‡æ€»ç»Ÿè®¡æ•°æ® (JSON)

**æ–‡æ¡£å£°ç§°çš„æ ¼å¼**:
```json
{
  "total_papers": 600,
  "timestamp": "20260207_143052",
  "config": {...},
  "variant_distribution": {...},
  "decision_distribution": {...},
  "rating_statistics": {...},
  "variant_statistics": {
    "original": {
      "count": 100,
      "avg_rating": 7.8,
      "median_rating": 8.0,
      "std_rating": 1.1,
      "decision_distribution": {...},
      "accept_rate": 0.85,
      "avg_originality": 7.9,
      "avg_quality": 8.0,
      "avg_clarity": 7.7,
      "avg_significance": 7.8
    },
    ...
  }
}
```

**ä»£ç å®é™…ç”Ÿæˆ**:
```python
summary = {
    'total_papers': len(results),
    'timestamp': timestamp,
    'config': {
        'sample_size': SAMPLE_SIZE,
        'train_ratio': TRAIN_RATIO,
        'model_size': MODEL_SIZE,
        'seed': SEED
    },
    'variant_distribution': dict(variant_counts),
    'decision_distribution': dict(decision_counts),
    'rating_statistics': {
        'mean': statistics.mean(ratings),
        'median': statistics.median(ratings),
        'min': min(ratings),
        'max': max(ratings),
        'std': statistics.stdev(ratings)
    },
    'variant_statistics': {
        variant: {
            'count': len(variant_results),
            'avg_rating': statistics.mean(variant_ratings),
            'median_rating': statistics.median(variant_ratings),
            'std_rating': statistics.stdev(variant_ratings),
            'decision_distribution': dict(variant_decisions),
            'accept_rate': ...,
            'avg_originality': ...,
            'avg_quality': ...,
            'avg_clarity': ...,
            'avg_significance': ...
        }
    }
}
```

**ç»“è®º**: âœ… **å®Œå…¨ä¸€è‡´** - æ‰€æœ‰å­—æ®µéƒ½æŒ‰æ–‡æ¡£ç”Ÿæˆ

---

### âœ… è¾“å‡ºæ–‡ä»¶ä½ç½®å¯¹ç…§

| æ–‡æ¡£å£°ç§° | ä»£ç å®é™… | æ˜¯å¦ä¸€è‡´ |
|---------|---------|---------|
| `evaluation_results/evaluation_results_*.jsonl` | âœ… `evaluation_results/evaluation_results_{timestamp}.jsonl` | âœ… ä¸€è‡´ |
| `evaluation_results/evaluation_summary_*.json` | âœ… `evaluation_results/evaluation_summary_{timestamp}.json` | âœ… ä¸€è‡´ |

---

## ğŸ” å…³é”®é€»è¾‘éªŒè¯

### âœ… éªŒè¯ç‚¹ 1: æ¯ä¸ªå˜ä½“æ•°é‡ç›¸åŒ

**ä»£ç é€»è¾‘**:
```python
# é‡‡æ · 100 ä¸ª paper_id
sampled_train_ids = random.sample(train_paper_ids, 73)  # å‡è®¾ 73%
sampled_test_ids = random.sample(test_paper_ids, 27)    # å‡è®¾ 27%

# ä¸ºæ¯ä¸ª paper_id è·å–æ‰€æœ‰å˜ä½“
for paper_id in sampled_train_ids:
    for paper in train_grouped[paper_id]:  # 6ä¸ªå˜ä½“
        all_sampled.append(paper)
```

**ç»“æœ**:
- 100 ä¸ª paper_id Ã— 6 ä¸ªå˜ä½“ = 600 ç¯‡è®ºæ–‡
- æ¯ä¸ªå˜ä½“æ°å¥½ 100 ç¯‡

**ç»“è®º**: âœ… **é€»è¾‘æ­£ç¡®** - ç¡®ä¿æ¯ä¸ªå˜ä½“æ•°é‡ç›¸åŒ

---

### âœ… éªŒè¯ç‚¹ 2: æ•°æ®å®Œæ•´æ€§

**ä»£ç æ£€æŸ¥**:
```python
# è¯„ä¼°æ—¶è®°å½•æ‰€æœ‰å­—æ®µ
result = {
    'paper_id': ...,
    'title': ...,
    'variant_type': ...,
    'dataset_split': ...,
    'evaluation': {
        'avg_rating': ...,           # âœ…
        'paper_decision': ...,        # âœ…
        'confidence': ...,            # âœ…
        'strength': ...,              # âœ…
        'weaknesses': ...,            # âœ…
        'meta_review': ...,           # âœ…
        'originality': ...,           # âœ…
        'quality': ...,               # âœ…
        'clarity': ...,               # âœ…
        'significance': ...,          # âœ…
    },
    'text_length': ...,               # âœ…
    'evaluation_timestamp': ...       # âœ…
}
```

**ç»“è®º**: âœ… **æ•°æ®å®Œæ•´** - æ‰€æœ‰æ–‡æ¡£å£°ç§°çš„å­—æ®µéƒ½è¢«è®°å½•

---

### âœ… éªŒè¯ç‚¹ 3: ç»Ÿè®¡è®¡ç®—æ­£ç¡®æ€§

**ä»£ç æ£€æŸ¥**:
```python
# æŒ‰å˜ä½“åˆ†ç»„ç»Ÿè®¡
for variant in variants:
    variant_results = [r for r in results if r['variant_type'] == variant]
    variant_ratings = [r['evaluation']['avg_rating'] for r in variant_results]
    
    summary['variant_statistics'][variant] = {
        'count': len(variant_results),                    # âœ… è®¡æ•°
        'avg_rating': statistics.mean(variant_ratings),   # âœ… å¹³å‡å€¼
        'median_rating': statistics.median(variant_ratings), # âœ… ä¸­ä½æ•°
        'std_rating': statistics.stdev(variant_ratings),  # âœ… æ ‡å‡†å·®
        'accept_rate': ...,                               # âœ… æ¥å—ç‡
        'avg_originality': ...,                           # âœ… å„ç»´åº¦å¹³å‡å€¼
        ...
    }
```

**ç»“è®º**: âœ… **ç»Ÿè®¡æ­£ç¡®** - ä½¿ç”¨ Python statistics æ¨¡å—æ­£ç¡®è®¡ç®—

---

## âš ï¸ å‘ç°çš„ä¸ä¸€è‡´

### é—®é¢˜ 1: TRAIN_RATIO é…ç½®

**æ–‡æ¡£è¯´**:
```markdown
**Configuration** (in `batch_evaluate_papers.py`):
- `TRAIN_RATIO = 0.8` - Ratio of train vs test papers
```

**ä»£ç å®é™…**:
```python
TRAIN_RATIO = None  # Auto-calculate based on actual dataset ratio
```

**å½±å“**:
- æ–‡æ¡£è¯´æ˜¯å›ºå®š 80% train / 20% test
- å®é™…æ˜¯æ ¹æ®æ•°æ®é›†è‡ªåŠ¨è®¡ç®—æ¯”ä¾‹
- å¦‚æœ train æœ‰ 1234 ç¯‡ï¼Œtest æœ‰ 456 ç¯‡ï¼Œåˆ™æ¯”ä¾‹æ˜¯ 73% / 27%

**å»ºè®®**: âš ï¸ **éœ€è¦æ›´æ–°æ–‡æ¡£**

---

### é—®é¢˜ 2: æ–‡æ¡£è·¯å¾„

**æ–‡æ¡£ä¸­çš„è·¯å¾„**:
```bash
cd D:\Mike\PycharmProjects\Researcher  # Windows è·¯å¾„
```

**å®é™…ç¯å¢ƒ**:
```
/Users/maying/PycharmProjects/pythonProject/Researcher  # macOS è·¯å¾„
```

**å»ºè®®**: âš ï¸ **æ–‡æ¡£åº”ä½¿ç”¨é€šç”¨è·¯å¾„è¡¨ç¤º**

---

## âœ… æ€»ä½“ç»“è®º

### æ ¸å¿ƒé€»è¾‘: âœ… **å®Œå…¨ç¬¦åˆæ–‡æ¡£**
1. âœ… é‡‡æ · 100 ç¯‡åŸºç¡€è®ºæ–‡
2. âœ… æ¯ç¯‡è®ºæ–‡åŒ…å«æ‰€æœ‰ 6 ä¸ªå˜ä½“
3. âœ… æ€»å…± 600 ç¯‡è®ºæ–‡è¯„ä¼°
4. âœ… æ¯ä¸ªå˜ä½“æ°å¥½ 100 ç¯‡

### æ•°æ®æ ¼å¼: âœ… **å®Œå…¨ç¬¦åˆæ–‡æ¡£**
1. âœ… JSONL æ ¼å¼è¯¦ç»†æ•°æ®
2. âœ… JSON æ ¼å¼æ±‡æ€»ç»Ÿè®¡
3. âœ… æ‰€æœ‰å­—æ®µéƒ½æŒ‰æ–‡æ¡£ç”Ÿæˆ
4. âœ… æ–‡ä»¶å‘½åå’Œä½ç½®æ­£ç¡®

### ç»Ÿè®¡è®¡ç®—: âœ… **æ­£ç¡®ä¸”å®Œæ•´**
1. âœ… æ•´ä½“ç»Ÿè®¡ï¼ˆå‡å€¼ã€ä¸­ä½æ•°ã€æ ‡å‡†å·®ï¼‰
2. âœ… æŒ‰å˜ä½“åˆ†ç»„ç»Ÿè®¡
3. âœ… æ¥å—ç‡ã€å„ç»´åº¦è¯„åˆ†
4. âœ… ç»ˆç«¯æ‰“å°å˜ä½“å¯¹æ¯”

### éœ€è¦ä¿®æ­£çš„æ–‡æ¡£é—®é¢˜:
1. âš ï¸ `TRAIN_RATIO` å®é™…æ˜¯ `None`ï¼ˆè‡ªåŠ¨è®¡ç®—ï¼‰ï¼Œè€Œéå›ºå®š `0.8`
2. âš ï¸ è·¯å¾„åº”ä½¿ç”¨é€šç”¨è¡¨ç¤º
3. âš ï¸ æ–‡æ¡£åº”è¯´æ˜"è‡ªåŠ¨è®¡ç®—æ¯”ä¾‹"çš„è¡Œä¸º

---

## ğŸ“ æ¨èçš„æ–‡æ¡£æ›´æ–°

### README_EVALUATION.md åº”è¯¥æ”¹ä¸º:

```markdown
**Configuration** (in `batch_evaluate_papers.py`):
- `SAMPLE_SIZE = 100` - Number of BASE papers to sample
- `TRAIN_RATIO = None` - Auto-calculate ratio from dataset (or set to float like 0.8 for fixed ratio)
- `MODEL_SIZE = "8B"` - CycleReviewer model size
- `SEED = 42` - Random seed for reproducibility

**Note**: When `TRAIN_RATIO = None`, the script automatically calculates the ratio 
based on the number of unique papers in train vs test sets. For example, if train 
has 1234 papers and test has 456 papers, it will sample 73 from train and 27 from test.
```

---

## ğŸ¯ ä»£ç è¡Œä¸ºæ€»ç»“

å½“è¿è¡Œ `batch_evaluate_papers.py` æ—¶:

1. **è¾“å…¥**: 
   - `util/train_with_variants.jsonl` (ä¾‹å¦‚: 7404ç¯‡ï¼Œ1234ä¸ªå”¯ä¸€paper_id Ã— 6å˜ä½“)
   - `util/test_with_variants.jsonl` (ä¾‹å¦‚: 2736ç¯‡ï¼Œ456ä¸ªå”¯ä¸€paper_id Ã— 6å˜ä½“)

2. **å¤„ç†**:
   - æŒ‰ paper_id åˆ†ç»„
   - è‡ªåŠ¨è®¡ç®—æ¯”ä¾‹: 1234/(1234+456) = 73%
   - é‡‡æ · 73 ä¸ª train paper_id + 27 ä¸ª test paper_id
   - è·å–è¿™ 100 ä¸ª paper_id çš„æ‰€æœ‰ 6 ä¸ªå˜ä½“ = 600 ç¯‡è®ºæ–‡

3. **è¾“å‡º**:
   - `evaluation_results/evaluation_results_YYYYMMDD_HHMMSS.jsonl` (600è¡Œ)
   - `evaluation_results/evaluation_summary_YYYYMMDD_HHMMSS.json` (å«å˜ä½“ç»Ÿè®¡)

4. **ç»ˆç«¯è¾“å‡º**:
   ```
   [INFO] Sampled papers by variant type:
     no_abstract: 100
     no_conclusion: 100
     no_experiments: 100
     no_introduction: 100
     no_methods: 100
     original: 100
   
   VARIANT COMPARISON
   ======================================================================
   no_abstract:
     Count: 100
     Avg Rating: 6.30
     Accept Rate: 58.0%
     ...
   original:
     Count: 100
     Avg Rating: 7.80
     Accept Rate: 85.0%
     ...
   ```

---

**éªŒè¯æ—¥æœŸ**: 2026-02-07  
**éªŒè¯äºº**: AI Assistant  
**ç»“è®º**: âœ… ä»£ç é€»è¾‘**ä¸¥æ ¼æŒ‰ç…§è®¾è®¡æ–‡æ¡£**å®ç°ï¼Œä»…æœ‰é…ç½®å‚æ•°æ–‡æ¡£éœ€è¦æ›´æ–°

