"""
ç¤ºä¾‹è„šæœ¬ï¼šå¦‚ä½•æŸ¥è¯¢å’Œä½¿ç”¨è¯„ä¼°ç»“æœæ•°æ®

æ¼”ç¤ºï¼š
1. åŠ è½½è¯„ä¼°ç»“æœ
2. æŸ¥è¯¢ç‰¹å®šè®ºæ–‡çš„æ‰€æœ‰å˜ä½“
3. æ¯”è¾ƒå„å˜ä½“çš„æ€§èƒ½
4. ç”Ÿæˆç®€å•çš„å¯¹æ¯”æŠ¥å‘Š
"""

import json
import pandas as pd
from pathlib import Path
from collections import defaultdict

# é…ç½®
PROJECT_ROOT = Path(__file__).parent.parent
RESULTS_DIR = PROJECT_ROOT / "evaluation_results"


def load_evaluation_results(results_file=None):
    """åŠ è½½è¯„ä¼°ç»“æœ"""
    if results_file is None:
        # æ‰¾åˆ°æœ€æ–°çš„ç»“æœæ–‡ä»¶
        result_files = list(RESULTS_DIR.glob('evaluation_results_*.jsonl'))
        if not result_files:
            print("âŒ æœªæ‰¾åˆ°è¯„ä¼°ç»“æœæ–‡ä»¶")
            print(f"è¯·å…ˆè¿è¡Œ: python scripts/batch_evaluate_papers.py")
            return None
        results_file = max(result_files, key=lambda p: p.stat().st_mtime)

    print(f"ğŸ“ åŠ è½½æ–‡ä»¶: {results_file.name}")

    data = []
    with open(results_file, 'r', encoding='utf-8') as f:
        for line in f:
            if line.strip():
                data.append(json.loads(line))

    print(f"âœ… åŠ è½½äº† {len(data)} æ¡è¯„ä¼°è®°å½•\n")
    return data


def example_1_basic_stats(data):
    """ç¤ºä¾‹1: åŸºæœ¬ç»Ÿè®¡"""
    print("="*70)
    print("ç¤ºä¾‹ 1: åŸºæœ¬ç»Ÿè®¡")
    print("="*70)

    df = pd.DataFrame(data)

    # å±•å¼€ evaluation å­—å…¸
    for key in ['avg_rating', 'paper_decision', 'originality', 'quality', 'clarity', 'significance']:
        df[key] = df['evaluation'].apply(lambda x: x.get(key))

    print(f"æ€»è¯„ä¼°è®°å½•: {len(df)}")
    print(f"è®ºæ–‡æ•°é‡: {df['paper_id'].nunique()}")
    print(f"å˜ä½“ç±»å‹: {df['variant_type'].unique().tolist()}")
    print(f"\nè¯„åˆ†ç»Ÿè®¡:")
    print(f"  å¹³å‡åˆ†: {df['avg_rating'].mean():.2f}")
    print(f"  ä¸­ä½æ•°: {df['avg_rating'].median():.2f}")
    print(f"  æœ€é«˜åˆ†: {df['avg_rating'].max():.2f}")
    print(f"  æœ€ä½åˆ†: {df['avg_rating'].min():.2f}")
    print()


def example_2_variant_comparison(data):
    """ç¤ºä¾‹2: å˜ä½“å¯¹æ¯”"""
    print("="*70)
    print("ç¤ºä¾‹ 2: å˜ä½“å¯¹æ¯”")
    print("="*70)

    df = pd.DataFrame(data)
    for key in ['avg_rating', 'paper_decision', 'originality', 'quality', 'clarity', 'significance']:
        df[key] = df['evaluation'].apply(lambda x: x.get(key))

    print(f"{'å˜ä½“ç±»å‹':<20} {'æ•°é‡':>6} {'å¹³å‡è¯„åˆ†':>10} {'æ¥å—ç‡':>10}")
    print("-" * 70)

    for variant in sorted(df['variant_type'].unique()):
        variant_df = df[df['variant_type'] == variant]
        count = len(variant_df)
        avg_rating = variant_df['avg_rating'].mean()
        accept_rate = variant_df['paper_decision'].str.contains('Accept', case=False).sum() / count * 100

        print(f"{variant:<20} {count:>6} {avg_rating:>10.2f} {accept_rate:>9.1f}%")

    print()


def example_3_paper_variants(data, paper_id=None):
    """ç¤ºä¾‹3: æŸ¥çœ‹ç‰¹å®šè®ºæ–‡çš„æ‰€æœ‰å˜ä½“"""
    print("="*70)
    print("ç¤ºä¾‹ 3: ç‰¹å®šè®ºæ–‡çš„æ‰€æœ‰å˜ä½“")
    print("="*70)

    # å¦‚æœæ²¡æœ‰æŒ‡å®š paper_idï¼Œéšæœºé€‰ä¸€ä¸ª
    if paper_id is None:
        paper_ids = list(set(item['paper_id'] for item in data))
        if paper_ids:
            paper_id = paper_ids[0]

    print(f"è®ºæ–‡ID: {paper_id}\n")

    # æ‰¾åˆ°è¯¥è®ºæ–‡çš„æ‰€æœ‰å˜ä½“
    paper_variants = [item for item in data if item['paper_id'] == paper_id]

    if not paper_variants:
        print(f"âŒ æœªæ‰¾åˆ°è®ºæ–‡ {paper_id}")
        return

    # æ˜¾ç¤ºè®ºæ–‡ä¿¡æ¯
    print(f"æ ‡é¢˜: {paper_variants[0]['title']}")
    print(f"æ•°æ®é›†: {paper_variants[0]['dataset_split']}")
    print(f"\nå„å˜ä½“è¯„åˆ†å¯¹æ¯”:")
    print(f"{'å˜ä½“':<20} {'è¯„åˆ†':>8} {'å†³å®š':>15} {'åŸåˆ›æ€§':>8} {'è´¨é‡':>8} {'æ¸…æ™°åº¦':>8} {'é‡è¦æ€§':>8}")
    print("-" * 100)

    # æŒ‰å˜ä½“æ’åº
    variant_order = ['original', 'no_abstract', 'no_introduction', 'no_methods', 'no_experiments', 'no_conclusion']
    sorted_variants = sorted(paper_variants, key=lambda x: variant_order.index(x['variant_type']) if x['variant_type'] in variant_order else 999)

    for item in sorted_variants:
        eval_data = item['evaluation']
        print(f"{item['variant_type']:<20} "
              f"{eval_data['avg_rating']:>8.2f} "
              f"{eval_data['paper_decision']:>15} "
              f"{eval_data['originality']:>8.1f} "
              f"{eval_data['quality']:>8.1f} "
              f"{eval_data['clarity']:>8.1f} "
              f"{eval_data['significance']:>8.1f}")

    print()


def example_4_impact_analysis(data):
    """ç¤ºä¾‹4: å½±å“åˆ†æ - ç¼ºå°‘å“ªä¸ªéƒ¨åˆ†å½±å“æœ€å¤§"""
    print("="*70)
    print("ç¤ºä¾‹ 4: å½±å“åˆ†æ")
    print("="*70)

    df = pd.DataFrame(data)
    df['avg_rating'] = df['evaluation'].apply(lambda x: x.get('avg_rating'))

    # è®¡ç®— original çš„å¹³å‡è¯„åˆ†
    original_rating = df[df['variant_type'] == 'original']['avg_rating'].mean()

    print(f"Original å¹³å‡è¯„åˆ†: {original_rating:.2f}\n")
    print(f"{'å˜ä½“':<20} {'å¹³å‡è¯„åˆ†':>10} {'è¯„åˆ†ä¸‹é™':>10} {'å½±å“ç¨‹åº¦':>10}")
    print("-" * 70)

    variants = [v for v in df['variant_type'].unique() if v != 'original']
    impacts = []

    for variant in variants:
        variant_rating = df[df['variant_type'] == variant]['avg_rating'].mean()
        impact = original_rating - variant_rating
        impacts.append((variant, variant_rating, impact))

    # æŒ‰å½±å“ç¨‹åº¦æ’åº
    impacts.sort(key=lambda x: x[2], reverse=True)

    for variant, rating, impact in impacts:
        impact_pct = (impact / original_rating) * 100
        print(f"{variant:<20} {rating:>10.2f} {impact:>10.2f} {impact_pct:>9.1f}%")

    print(f"\nğŸ’¡ ç»“è®º: '{impacts[0][0]}' éƒ¨åˆ†å¯¹è¯„åˆ†å½±å“æœ€å¤§ (ä¸‹é™ {impacts[0][2]:.2f} åˆ†)")
    print()


def example_5_find_top_papers(data, n=5):
    """ç¤ºä¾‹5: æ‰¾å‡ºè¯„åˆ†æœ€é«˜çš„è®ºæ–‡"""
    print("="*70)
    print(f"ç¤ºä¾‹ 5: è¯„åˆ†æœ€é«˜çš„ {n} ç¯‡è®ºæ–‡")
    print("="*70)

    df = pd.DataFrame(data)
    df['avg_rating'] = df['evaluation'].apply(lambda x: x.get('avg_rating'))

    top_papers = df.nlargest(n, 'avg_rating')

    print(f"{'æ’å':>4} {'è¯„åˆ†':>8} {'å˜ä½“':>20} è®ºæ–‡æ ‡é¢˜")
    print("-" * 100)

    for i, (_, row) in enumerate(top_papers.iterrows(), 1):
        title = row['title'][:50] + '...' if len(row['title']) > 50 else row['title']
        print(f"{i:>4} {row['avg_rating']:>8.2f} {row['variant_type']:>20} {title}")

    print()


def example_6_export_to_csv(data):
    """ç¤ºä¾‹6: å¯¼å‡ºä¸º CSV ä¾¿äº Excel åˆ†æ"""
    print("="*70)
    print("ç¤ºä¾‹ 6: å¯¼å‡ºä¸º CSV")
    print("="*70)

    df = pd.DataFrame(data)

    # å±•å¼€ evaluation å­—å…¸
    for key in ['avg_rating', 'paper_decision', 'confidence', 'originality', 'quality', 'clarity', 'significance']:
        df[key] = df['evaluation'].apply(lambda x: x.get(key))

    # é€‰æ‹©éœ€è¦çš„åˆ—
    export_df = df[['paper_id', 'title', 'variant_type', 'dataset_split',
                    'avg_rating', 'paper_decision', 'originality', 'quality', 'clarity', 'significance']]

    output_file = PROJECT_ROOT / 'evaluation_data_export.csv'
    export_df.to_csv(output_file, index=False, encoding='utf-8-sig')  # utf-8-sig for Excel

    print(f"âœ… æ•°æ®å·²å¯¼å‡ºåˆ°: {output_file}")
    print(f"   å…± {len(export_df)} æ¡è®°å½•")
    print(f"   å¯ä»¥ç”¨ Excel æ‰“å¼€æŸ¥çœ‹")
    print()


def main():
    """è¿è¡Œæ‰€æœ‰ç¤ºä¾‹"""
    print("\n" + "="*70)
    print("è¯„ä¼°ç»“æœæ•°æ®æŸ¥è¯¢ç¤ºä¾‹")
    print("="*70 + "\n")

    # åŠ è½½æ•°æ®
    data = load_evaluation_results()
    if not data:
        return

    # è¿è¡Œç¤ºä¾‹
    example_1_basic_stats(data)
    example_2_variant_comparison(data)
    example_3_paper_variants(data)
    example_4_impact_analysis(data)
    example_5_find_top_papers(data, n=5)
    example_6_export_to_csv(data)

    print("="*70)
    print("âœ… æ‰€æœ‰ç¤ºä¾‹è¿è¡Œå®Œæˆ")
    print("="*70)
    print("\nğŸ’¡ æç¤º:")
    print("  - è¯„ä¼°æ•°æ®ä¿å­˜åœ¨: evaluation_results/")
    print("  - åˆ†ææŠ¥å‘Šä¿å­˜åœ¨: analysis_output/")
    print("  - æŸ¥çœ‹è¯¦ç»†æ–‡æ¡£: scripts/DATA_FLOW_GUIDE.md")
    print()


if __name__ == "__main__":
    main()

