==== 缺失组合（文件1） ====
('HKGQDDTuvZ', 'no_experiments')
('LWwYyxF3w9', 'no_abstract')
('bCvm9h0FmQ', 'no_abstract')
('rnHqwPH4TZ', 'no_abstract')
('RN2lIjrtSR', 'no_methods')
('HKGQDDTuvZ', 'no_methods')
('N0isTh3rml', 'original')
('L6crLU7MIE', 'no_methods')
('WLgbjzKJkk', 'original')
('N0isTh3rml', 'no_abstract')
共缺失: 10

==== 缺失组合（文件2） ====
('HKGQDDTuvZ', 'no_experiments')
('LWwYyxF3w9', 'no_abstract')
('bCvm9h0FmQ', 'no_abstract')
('rnHqwPH4TZ', 'no_abstract')
('RN2lIjrtSR', 'no_methods')
('HKGQDDTuvZ', 'no_methods')
('N0isTh3rml', 'original')
('L6crLU7MIE', 'no_methods')
('WLgbjzKJkk', 'original')
('N0isTh3rml', 'no_abstract')
共缺失: 10

==== 重复组合（文件1） ====
('iNtEAeVQE0', 'no_experiments'): 4 次
('mHYkcQzdae', 'original'): 3 次
('mHYkcQzdae', 'no_abstract'): 4 次
('mHYkcQzdae', 'no_introduction'): 4 次
('mHYkcQzdae', 'no_conclusion'): 4 次
('mHYkcQzdae', 'no_experiments'): 4 次
('mHYkcQzdae', 'no_methods'): 4 次
('zgHamUBuuO', 'original'): 4 次
('zgHamUBuuO', 'no_abstract'): 4 次
('zgHamUBuuO', 'no_introduction'): 4 次
('zgHamUBuuO', 'no_conclusion'): 4 次
('zgHamUBuuO', 'no_methods'): 4 次
('WNSjteBJd9', 'original'): 4 次
('WNSjteBJd9', 'no_abstract'): 4 次
('WNSjteBJd9', 'no_introduction'): 4 次
('WNSjteBJd9', 'no_conclusion'): 4 次
('WNSjteBJd9', 'no_experiments'): 4 次
('WNSjteBJd9', 'no_methods'): 4 次
('9WD9KwssyT', 'no_abstract'): 4 次
('9WD9KwssyT', 'no_introduction'): 4 次
('9WD9KwssyT', 'no_conclusion'): 4 次
('9WD9KwssyT', 'no_experiments'): 4 次
('9WD9KwssyT', 'no_methods'): 4 次
('BkRD6GsswM', 'original'): 4 次
('BkRD6GsswM', 'no_abstract'): 4 次
('BkRD6GsswM', 'no_introduction'): 4 次
('BkRD6GsswM', 'no_conclusion'): 4 次
('BkRD6GsswM', 'no_experiments'): 4 次
('SqMVI1GFnp', 'original'): 4 次
('SqMVI1GFnp', 'no_abstract'): 4 次
('SqMVI1GFnp', 'no_introduction'): 4 次
('SqMVI1GFnp', 'no_conclusion'): 4 次
('SqMVI1GFnp', 'no_experiments'): 4 次
('SqMVI1GFnp', 'no_methods'): 4 次
('rR03qFesqk', 'original'): 4 次
('rR03qFesqk', 'no_abstract'): 4 次
('rR03qFesqk', 'no_introduction'): 4 次
('rR03qFesqk', 'no_conclusion'): 4 次
('rR03qFesqk', 'no_experiments'): 4 次
('rR03qFesqk', 'no_methods'): 4 次
('zmJDzPh1Dm', 'original'): 4 次
('zmJDzPh1Dm', 'no_abstract'): 4 次
('zmJDzPh1Dm', 'no_introduction'): 4 次
('zmJDzPh1Dm', 'no_conclusion'): 4 次
('zmJDzPh1Dm', 'no_experiments'): 4 次
('zmJDzPh1Dm', 'no_methods'): 4 次
('TKnzPdyeJu', 'original'): 4 次
('TKnzPdyeJu', 'no_abstract'): 4 次
('TKnzPdyeJu', 'no_introduction'): 4 次
('TKnzPdyeJu', 'no_methods'): 4 次
('Gk75gOjtQh', 'original'): 4 次
('Gk75gOjtQh', 'no_abstract'): 4 次
('Gk75gOjtQh', 'no_introduction'): 4 次
('Gk75gOjtQh', 'no_conclusion'): 4 次
('Gk75gOjtQh', 'no_experiments'): 4 次
('Gk75gOjtQh', 'no_methods'): 4 次
('SXj1qjFEpQ', 'original'): 4 次
('SXj1qjFEpQ', 'no_abstract'): 4 次
('SXj1qjFEpQ', 'no_introduction'): 4 次
('SXj1qjFEpQ', 'no_conclusion'): 4 次
('BkRD6GsswM', 'no_methods'): 3 次
('SXj1qjFEpQ', 'no_experiments'): 4 次
('SXj1qjFEpQ', 'no_methods'): 4 次
('G3LOFL4jGp', 'no_abstract'): 4 次
('G3LOFL4jGp', 'no_introduction'): 4 次
('G3LOFL4jGp', 'no_experiments'): 4 次
('G3LOFL4jGp', 'no_methods'): 4 次
('4UP387Adir', 'original'): 4 次
('4UP387Adir', 'no_abstract'): 4 次
('4UP387Adir', 'no_introduction'): 4 次
('4UP387Adir', 'no_conclusion'): 4 次
('4UP387Adir', 'no_experiments'): 4 次
('4UP387Adir', 'no_methods'): 4 次
('KksPo0zXId', 'original'): 4 次
('KksPo0zXId', 'no_abstract'): 4 次
('KksPo0zXId', 'no_introduction'): 4 次
('KksPo0zXId', 'no_conclusion'): 4 次
('KksPo0zXId', 'no_experiments'): 4 次
('KksPo0zXId', 'no_methods'): 4 次
('YhwDw31DGI', 'no_abstract'): 4 次
('YhwDw31DGI', 'no_introduction'): 4 次
('YhwDw31DGI', 'no_experiments'): 4 次
('YhwDw31DGI', 'no_methods'): 4 次
('n7Sr8SW4bn', 'original'): 4 次
('n7Sr8SW4bn', 'no_abstract'): 4 次
('n7Sr8SW4bn', 'no_introduction'): 4 次
('n7Sr8SW4bn', 'no_conclusion'): 4 次
('n7Sr8SW4bn', 'no_experiments'): 4 次
('n7Sr8SW4bn', 'no_methods'): 4 次
('TKDwsJmrDJ', 'no_abstract'): 4 次
('TKDwsJmrDJ', 'no_introduction'): 4 次
('TKDwsJmrDJ', 'no_conclusion'): 4 次
('TKDwsJmrDJ', 'no_methods'): 4 次
('lr69PmF2Ov', 'original'): 4 次
('lr69PmF2Ov', 'no_abstract'): 4 次
('lr69PmF2Ov', 'no_introduction'): 4 次
('lr69PmF2Ov', 'no_conclusion'): 4 次
('lr69PmF2Ov', 'no_experiments'): 4 次
('lr69PmF2Ov', 'no_methods'): 4 次
('ecbRyZZmKG', 'original'): 4 次
('ecbRyZZmKG', 'no_abstract'): 4 次
('ecbRyZZmKG', 'no_introduction'): 4 次
('TKnzPdyeJu', 'no_conclusion'): 3 次
('ecbRyZZmKG', 'no_conclusion'): 4 次
('ecbRyZZmKG', 'no_experiments'): 4 次
('ecbRyZZmKG', 'no_methods'): 4 次
('TKnzPdyeJu', 'no_experiments'): 3 次
('QMkYEau02q', 'no_abstract'): 4 次
('QMkYEau02q', 'no_introduction'): 4 次
('QMkYEau02q', 'no_conclusion'): 4 次
('QMkYEau02q', 'no_experiments'): 4 次
('QMkYEau02q', 'no_methods'): 4 次
('KzMMv0OygD', 'original'): 4 次
('KzMMv0OygD', 'no_abstract'): 4 次
('KzMMv0OygD', 'no_introduction'): 4 次
('KzMMv0OygD', 'no_conclusion'): 4 次
('KzMMv0OygD', 'no_experiments'): 4 次
('KzMMv0OygD', 'no_methods'): 4 次
('CpgoO6j6W1', 'original'): 4 次
('CpgoO6j6W1', 'no_abstract'): 4 次
('CpgoO6j6W1', 'no_introduction'): 4 次
('CpgoO6j6W1', 'no_conclusion'): 7 次
('CpgoO6j6W1', 'no_experiments'): 7 次
('CpgoO6j6W1', 'no_methods'): 7 次
('GSBHKiw19c', 'original'): 7 次
('GSBHKiw19c', 'no_abstract'): 7 次
('GSBHKiw19c', 'no_introduction'): 7 次
('G3LOFL4jGp', 'original'): 6 次
('GSBHKiw19c', 'no_experiments'): 7 次
('G3LOFL4jGp', 'no_conclusion'): 3 次
('GSBHKiw19c', 'no_methods'): 7 次
('t8cBsT9mcg', 'original'): 7 次
('t8cBsT9mcg', 'no_abstract'): 4 次
('t8cBsT9mcg', 'no_introduction'): 7 次
('t8cBsT9mcg', 'no_conclusion'): 7 次
('t8cBsT9mcg', 'no_experiments'): 7 次
('t8cBsT9mcg', 'no_methods'): 7 次
('E296x0YpML', 'original'): 7 次
('E296x0YpML', 'no_abstract'): 7 次
('E296x0YpML', 'no_introduction'): 7 次
('E296x0YpML', 'no_conclusion'): 7 次
('E296x0YpML', 'no_experiments'): 7 次
('GSBHKiw19c', 'no_conclusion'): 6 次
('E296x0YpML', 'no_methods'): 7 次
('QcgvtqxRhI', 'original'): 7 次
('QcgvtqxRhI', 'no_abstract'): 7 次
('QcgvtqxRhI', 'no_introduction'): 7 次
('QcgvtqxRhI', 'no_conclusion'): 7 次
('QcgvtqxRhI', 'no_experiments'): 7 次
('QcgvtqxRhI', 'no_methods'): 7 次
('wRkfniZIBl', 'original'): 7 次
('wRkfniZIBl', 'no_abstract'): 4 次
('wRkfniZIBl', 'no_introduction'): 4 次
('wRkfniZIBl', 'no_conclusion'): 7 次
('wRkfniZIBl', 'no_experiments'): 7 次
('wRkfniZIBl', 'no_methods'): 4 次
('Koh0i2u8qX', 'original'): 4 次
('Koh0i2u8qX', 'no_abstract'): 7 次
('Koh0i2u8qX', 'no_conclusion'): 7 次
('Koh0i2u8qX', 'no_experiments'): 7 次
('Koh0i2u8qX', 'no_methods'): 7 次
('yJdj2QQCUB', 'original'): 4 次
('yJdj2QQCUB', 'no_abstract'): 7 次
('yJdj2QQCUB', 'no_introduction'): 7 次
('yJdj2QQCUB', 'no_conclusion'): 7 次
('yJdj2QQCUB', 'no_experiments'): 7 次
('yJdj2QQCUB', 'no_methods'): 7 次
('3rmpixOjPS', 'original'): 4 次
('3rmpixOjPS', 'no_abstract'): 7 次
('3rmpixOjPS', 'no_introduction'): 7 次
('3rmpixOjPS', 'no_conclusion'): 7 次
('3rmpixOjPS', 'no_experiments'): 7 次
('3rmpixOjPS', 'no_methods'): 7 次
('9g8h5HwZMy', 'original'): 7 次
('9g8h5HwZMy', 'no_abstract'): 4 次
('9g8h5HwZMy', 'no_introduction'): 7 次
('9g8h5HwZMy', 'no_conclusion'): 7 次
('9g8h5HwZMy', 'no_experiments'): 7 次
('9g8h5HwZMy', 'no_methods'): 7 次
('zIJFG7wW2d', 'original'): 7 次
('zIJFG7wW2d', 'no_abstract'): 7 次
('zIJFG7wW2d', 'no_introduction'): 7 次
('zIJFG7wW2d', 'no_conclusion'): 7 次
('zIJFG7wW2d', 'no_experiments'): 7 次
('zIJFG7wW2d', 'no_methods'): 7 次
('M8J0b9gNfG', 'original'): 7 次
('M8J0b9gNfG', 'no_abstract'): 7 次
('M8J0b9gNfG', 'no_introduction'): 7 次
('M8J0b9gNfG', 'no_conclusion'): 7 次
('M8J0b9gNfG', 'no_experiments'): 7 次
('M8J0b9gNfG', 'no_methods'): 7 次
('b3kDP3IytM', 'original'): 7 次
('b3kDP3IytM', 'no_abstract'): 7 次
('b3kDP3IytM', 'no_introduction'): 7 次
('b3kDP3IytM', 'no_conclusion'): 7 次
('b3kDP3IytM', 'no_experiments'): 7 次
('b3kDP3IytM', 'no_methods'): 7 次
('ug8wDSimNK', 'original'): 4 次
('ug8wDSimNK', 'no_abstract'): 7 次
('ug8wDSimNK', 'no_introduction'): 7 次
('ug8wDSimNK', 'no_conclusion'): 7 次
('ug8wDSimNK', 'no_experiments'): 7 次
('ug8wDSimNK', 'no_methods'): 7 次
('DwcV654WBP', 'original'): 7 次
('DwcV654WBP', 'no_abstract'): 7 次
('DwcV654WBP', 'no_introduction'): 7 次
('DwcV654WBP', 'no_conclusion'): 7 次
('DwcV654WBP', 'no_experiments'): 7 次
('DwcV654WBP', 'no_methods'): 7 次
('Koh0i2u8qX', 'no_introduction'): 6 次
('1JbsdayvhO', 'original'): 4 次
('1JbsdayvhO', 'no_abstract'): 7 次
('1JbsdayvhO', 'no_introduction'): 7 次
('1JbsdayvhO', 'no_conclusion'): 7 次
('1JbsdayvhO', 'no_experiments'): 7 次
('1JbsdayvhO', 'no_methods'): 7 次
('ZMjflI1aL0', 'original'): 7 次
('ZMjflI1aL0', 'no_abstract'): 7 次
('ZMjflI1aL0', 'no_introduction'): 7 次
('ZMjflI1aL0', 'no_conclusion'): 7 次
('ZMjflI1aL0', 'no_experiments'): 4 次
('ZMjflI1aL0', 'no_methods'): 7 次
('wDE3clrYWR', 'original'): 7 次
('wDE3clrYWR', 'no_abstract'): 7 次
('wDE3clrYWR', 'no_introduction'): 7 次
('wDE3clrYWR', 'no_conclusion'): 7 次
('wDE3clrYWR', 'no_experiments'): 7 次
('wDE3clrYWR', 'no_methods'): 7 次
('Mqukp3Lsnt', 'no_abstract'): 4 次
('Mqukp3Lsnt', 'no_introduction'): 4 次
('Mqukp3Lsnt', 'no_conclusion'): 4 次
('Mqukp3Lsnt', 'no_experiments'): 4 次
('Mqukp3Lsnt', 'no_methods'): 4 次
('mWf3RGc6HG', 'original'): 4 次
('mWf3RGc6HG', 'no_abstract'): 4 次
('mWf3RGc6HG', 'no_introduction'): 4 次
('mWf3RGc6HG', 'no_conclusion'): 4 次
('mWf3RGc6HG', 'no_experiments'): 4 次
('mWf3RGc6HG', 'no_methods'): 4 次
('QO3yH7X8JJ', 'original'): 4 次
('QO3yH7X8JJ', 'no_abstract'): 4 次
('QO3yH7X8JJ', 'no_introduction'): 4 次
('QO3yH7X8JJ', 'no_conclusion'): 4 次
('QO3yH7X8JJ', 'no_experiments'): 4 次
('QO3yH7X8JJ', 'no_methods'): 4 次
('eSr9iK1z8n', 'original'): 4 次
('eSr9iK1z8n', 'no_abstract'): 4 次
('eSr9iK1z8n', 'no_conclusion'): 4 次
('eSr9iK1z8n', 'no_experiments'): 4 次
('eSr9iK1z8n', 'no_methods'): 4 次
('9Wy6pLNQcG', 'original'): 4 次
('9Wy6pLNQcG', 'no_abstract'): 4 次
('9Wy6pLNQcG', 'no_introduction'): 4 次
('9Wy6pLNQcG', 'no_conclusion'): 4 次
('9Wy6pLNQcG', 'no_experiments'): 4 次
('9Wy6pLNQcG', 'no_methods'): 4 次
('Qox9rO0kN0', 'original'): 4 次
('Qox9rO0kN0', 'no_abstract'): 4 次
('Qox9rO0kN0', 'no_introduction'): 4 次
('Qox9rO0kN0', 'no_conclusion'): 4 次
('Qox9rO0kN0', 'no_experiments'): 4 次
('Qox9rO0kN0', 'no_methods'): 4 次
('9w3iw8wDuE', 'original'): 4 次
('9w3iw8wDuE', 'no_abstract'): 4 次
('9w3iw8wDuE', 'no_introduction'): 4 次
('9w3iw8wDuE', 'no_conclusion'): 4 次
('9w3iw8wDuE', 'no_experiments'): 4 次
('9w3iw8wDuE', 'no_methods'): 4 次
('pdJXYfJjz9', 'original'): 4 次
('pdJXYfJjz9', 'no_abstract'): 4 次
('pdJXYfJjz9', 'no_introduction'): 4 次
('pdJXYfJjz9', 'no_conclusion'): 4 次
('pdJXYfJjz9', 'no_experiments'): 4 次
('pdJXYfJjz9', 'no_methods'): 4 次
('dumkzmqTmS', 'original'): 4 次
('dumkzmqTmS', 'no_abstract'): 4 次
('dumkzmqTmS', 'no_introduction'): 4 次
('dumkzmqTmS', 'no_conclusion'): 4 次
('dumkzmqTmS', 'no_experiments'): 4 次
('dumkzmqTmS', 'no_methods'): 4 次
('Q9R10ZKd8z', 'original'): 4 次
('Q9R10ZKd8z', 'no_abstract'): 4 次
('Q9R10ZKd8z', 'no_introduction'): 4 次
('Q9R10ZKd8z', 'no_conclusion'): 4 次
('Q9R10ZKd8z', 'no_experiments'): 4 次
('Q9R10ZKd8z', 'no_methods'): 4 次
('NSVtmmzeRB', 'original'): 4 次
('NSVtmmzeRB', 'no_abstract'): 4 次
('NSVtmmzeRB', 'no_introduction'): 4 次
('NSVtmmzeRB', 'no_conclusion'): 4 次
('NSVtmmzeRB', 'no_experiments'): 4 次
('NSVtmmzeRB', 'no_methods'): 4 次
('Hk7yW3vmSq', 'original'): 4 次
('Hk7yW3vmSq', 'no_abstract'): 4 次
('Hk7yW3vmSq', 'no_conclusion'): 4 次
('Hk7yW3vmSq', 'no_experiments'): 4 次
('Hk7yW3vmSq', 'no_methods'): 3 次
('eSr9iK1z8n', 'no_introduction'): 3 次
共重复: 298

==== 重复组合（文件2） ====
共重复: 0

==== 每种 variant_type 实际数量（文件1） ====
original: 273
no_abstract: 291
no_introduction: 296
no_conclusion: 301
no_methods: 299
no_experiments: 299
==== 每种 variant_type 实际数量（文件2） ====
original: 98
no_abstract: 96
no_introduction: 100
no_conclusion: 100
no_methods: 97
no_experiments: 99

==== 仅在文件1存在的组合 ====
共: 0

==== 仅在文件2存在的组合 ====
共: 0

==== 两文件都存在但评估不同的组合 ====
('mHYkcQzdae', 'no_conclusion'):
  file1: {'avg_rating': 4.5, 'paper_decision': 'Reject', 'confidence': ['4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.\n\n', '4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.\n\n', '4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.\n\n', '4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.\n\n'], 'strength': ['- The proposed method achieves state-of-the-art performance on two datasets.\n- The proposed method is well-motivated and easy to understand.\n\n', '1. The paper is well written and easy to follow.\n2. The proposed method achieves state-of-the-art performance on the CASME II and SAMM datasets.\n3. The authors provide a comprehensive ablation study to validate the effectiveness of each component of the proposed method.\n\n', '1. The proposed method is well-motivated and easy to understand.\n2. The proposed method achieves state-of-the-art performance on the CASME II and SAMM datasets.\n\n', '1. The proposed method achieves state-of-the-art performance on two datasets.\n2. The proposed method is well-motivated and easy to understand.\n\n'], 'weaknesses': ['- The proposed method is not novel. The two-stream network has been widely used in various computer vision tasks. The facial position focalizer is similar to the position embedding in ViT.\n- The performance gain is not significant. The proposed method only achieves a 1.98% improvement on the SAMM dataset compared to the previous method.\n- The paper is not well-written. The paper is not well-organized, and some sections are not easy to understand.\n\n', '1. The novelty of this paper is limited. The idea of using a two-stream network for MER has been explored in previous works, such as [1]. The proposed CVA block is similar to the attention mechanism used in [2]. The FPF module is similar to the positional embedding in ViT. The AU embedding is also a common practice in MER.\n2. The experimental results are not convincing. The authors only report the results on two datasets, which are relatively small. The authors should also report the results on other datasets, such as CASME I and SMIC.\n3. The authors should provide more details about the training and testing procedures. For example, how do the authors handle the class imbalance problem in the datasets?\n\n[1] Li, Y., Li, Y., & Li, X. (2022, September). Micro-expression recognition based on motion and location information. In 2022 4th International Conference on Artificial Intelligence and Data Analytics (AIDA) (pp. 121-126). IEEE.\n\n[2] Li, Y., Li, Y., & Li, X. (2022, September). Micro-expression recognition based on motion and location information. In 2022 4th International Conference on Artificial Intelligence and Data Analytics (AIDA) (pp. 121-126). IEEE.\n\n', '1. The novelty of this paper is limited. The idea of using a two-stream network for MER has been explored in previous works, such as [1]. The proposed CVA block is similar to the attention mechanism used in [2]. The FPF module is similar to the positional embedding in ViT. The AU embedding is also a common practice in MER.\n2. The experimental results are not convincing. The authors only report the results on two datasets, which are relatively small. The authors should also report the results on other datasets, such as CASME I and SMIC.\n3. The authors should provide more details about the training and testing procedures. For example, how do the authors handle the class imbalance problem in the datasets?\n\n[1] Li, Y., Li, Y., & Li, X. (2022, September). Micro-expression recognition based on motion and location information. In 2022 4th International Conference on Artificial Intelligence and Data Analytics (AIDA) (pp. 121-126). IEEE.\n\n[2] Li, Y., Li, Y., & Li, X. (2022, September). Micro-expression recognition based on motion and location information. In 2022 4th International Conference on Artificial Intelligence and Data Analytics (AIDA) (pp. 121-126). IEEE.\n\n', '1. The novelty of this paper is limited. The two-stream network has been widely used in various computer vision tasks. The facial position focalizer is similar to the position embedding in ViT.\n2. The experimental results are not convincing. The authors only report the results on two datasets, which are relatively small. The authors should also report the results on other datasets, such as CASME I and SMIC.\n3. The paper is not well-written. The paper is not well-organized, and some sections are not easy to understand.\n\n'], 'meta_review': 'The paper proposes a new method for micro-expression recognition. The proposed method is based on a two-stream network, where the first stream extracts facial features from the onset frame and the second stream extracts motion features from the difference between the onset and apex frames. The proposed method is evaluated on two datasets, CASME II and SAMM, and achieves state-of-the-art performance.\n\nThe reviewers have raised several concerns about the paper, including the limited novelty of the proposed method, the lack of convincing experimental results, and the poor writing quality of the paper. The authors have not provided any rebuttal to address these concerns. Therefore, I recommend rejecting the paper.\n\n### justification_for_why_not_higher_score\n\nThe reviewers have raised several concerns about the paper, including the limited novelty of the proposed method, the lack of convincing experimental results, and the poor writing quality of the paper. The authors have not provided any rebuttal to address these concerns.\n\n### justification_for_why_not_lower_score\n\nN/A\n\n', 'originality': 0, 'quality': 0, 'clarity': 0, 'significance': 0}
  file2: {'avg_rating': 4.5, 'paper_decision': 'Reject', 'confidence': ['3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.\n\n', '4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.\n\n', '3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.\n\n', '3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.\n\n'], 'strength': ['- The proposed method achieves state-of-the-art results on the CASME II and SAMM micro-expression datasets.\n- The paper is well-written and easy to follow.\n\n', 'The paper proposes a novel neural network for micro-expression recognition (MER) that focuses on subtle changes in facial movements using a CVA (Continuously Vertical Attention) block. The paper also proposes a facial position localization module called FPF (Facial Position Focalizer) based on Swin Transformer, which incorporates spatial information into the facial muscle movement pattern features used for MER. The paper also proves that including AU (Action Units) can further enhance accuracy, and therefore incorporates AU information to assist in micro-expression recognition.\n\n', 'The paper proposes a novel neural network for micro-expression recognition (MER), focusing on subtle changes in facial movements using a CVA (Continuously Vertical Attention) block, which models the local muscle changes with minimal identity information. Additionally, the paper proposes a facial position localization module called FPF (Facial Position Focalizer) based on Swin Transformer, which incorporates spatial information into the facial muscle movement pattern features used for MER. The paper also proves that including AU (Action Units) can further enhance accuracy, and therefore incorporates AU information to assist in micro-expression recognition. The experimental results indicate that the model achieved an average recognition accuracy of 94.35% and 86.76% on the popular CASME II and SAMM micro-expression datasets, improved by 6% and 1.98% compared to state-of-the-art models, respectively.\n\n', '1. The paper proposes a novel neural network for micro-expression recognition (MER), focusing on subtle changes in facial movements using a CVA (Continuously Vertical Attention) block, which models the local muscle changes with minimal identity information. Additionally, the paper proposes a facial position localization module called FPF (Facial Position Focalizer) based on Swin Transformer, which incorporates spatial information into the facial muscle movement pattern features used for MER. The paper also proves that including AU (Action Units) can further enhance accuracy, and therefore incorporates AU information to assist in micro-expression recognition. The experimental results indicate that the model achieved an average recognition accuracy of 94.35% and 86.76% on the popular CASME II and SAMM micro-expression datasets, improved by 6% and 1.98% compared to state-of-the-art models, respectively.\n\n'], 'weaknesses': ['- The paper lacks a comprehensive comparison with existing methods. The authors only compare their method with a few recent methods, and it is unclear how their method performs compared to other existing methods.\n- The paper does not provide a thorough analysis of the experimental results. For example, the authors do not provide a detailed analysis of the performance of the CVA block and FPF module.\n- The paper does not provide a clear explanation of the intuition behind the proposed method. For example, it is not clear why the CVA block is effective for MER.\n\n', 'The paper lacks a detailed analysis of the experimental results. For example, the authors do not provide a detailed analysis of the performance of the CVA block and FPF module. The paper does not provide a clear explanation of the intuition behind the proposed method. For example, it is not clear why the CVA block is effective for MER. The paper does not provide a thorough comparison with existing methods. The authors only compare their method with a few recent methods, and it is unclear how their method performs compared to other existing methods.\n\n', '1. The paper lacks a detailed analysis of the experimental results. For example, the authors do not provide a detailed analysis of the performance of the CVA block and FPF module. The paper does not provide a clear explanation of the intuition behind the proposed method. For example, it is not clear why the CVA block is effective for MER. The paper does not provide a thorough comparison with existing methods. The authors only compare their method with a few recent methods, and it is unclear how their method performs compared to other existing methods.\n2. The paper does not provide a clear explanation of the motivation behind the proposed method. For example, it is not clear why the CVA block is effective for MER. The paper does not provide a thorough comparison with existing methods. The authors only compare their method with a few recent methods, and it is unclear how their method performs compared to other existing methods.\n3. The paper does not provide a clear explanation of the intuition behind the proposed method. For example, it is not clear why the CVA block is effective for MER. The paper does not provide a thorough comparison with existing methods. The authors only compare their method with a few recent methods, and it is unclear how their method performs compared to other existing methods.\n\n', '1. The paper lacks a detailed analysis of the experimental results. For example, the authors do not provide a detailed analysis of the performance of the CVA block and FPF module. The paper does not provide a clear explanation of the intuition behind the proposed method. For example, it is not clear why the CVA block is effective for MER. The paper does not provide a thorough comparison with existing methods. The authors only compare their method with a few recent methods, and it is unclear how their method performs compared to other existing methods.\n\n2. The paper lacks a clear explanation of the motivation behind the proposed method. For example, it is not clear why the CVA block is effective for MER. The paper does not provide a thorough comparison with existing methods. The authors only compare their method with a few recent methods, and it is unclear how their method performs compared to other existing methods.\n\n3. The paper lacks a clear explanation of the intuition behind the proposed method. For example, it is not clear why the CVA block is effective for MER. The paper does not provide a thorough comparison with existing methods. The authors only compare their method with a few recent methods, and it is unclear how their method performs compared to other existing methods.\n\n'], 'meta_review': "The paper presents a novel neural network for micro-expression recognition (MER) that incorporates a Continuously Vertical Attention (CVA) block and a Facial Position Focalizer (FPF) module. The CVA block focuses on subtle facial changes, and the FPF module incorporates spatial information into facial muscle movement features. The paper also includes Action Unit (AU) information to enhance accuracy. The proposed method achieves state-of-the-art results on the CASME II and SAMM micro-expression datasets.\n\nThe reviewers raised several concerns regarding the paper, including the lack of a detailed analysis of the experimental results, the need for a clearer explanation of the motivation and intuition behind the proposed method, and the absence of a thorough comparison with existing methods. The authors provided a rebuttal addressing some of these concerns, but the reviewers maintained their initial ratings. The AC agrees with the reviewers' concerns and recommends rejecting the paper.\n\n### justification_for_why_not_higher_score\n\nThe reviewers raised several concerns regarding the paper, including the lack of a detailed analysis of the experimental results, the need for a clearer explanation of the motivation and intuition behind the proposed method, and the absence of a thorough comparison with existing methods. The authors provided a rebuttal addressing some of these concerns, but the reviewers maintained their initial ratings. The AC agrees with the reviewers' concerns and recommends rejecting the paper.\n\n### justification_for_why_not_lower_score\n\nN/A\n\n", 'originality': 0, 'quality': 0, 'clarity': 0, 'significance': 0}
('mHYkcQzdae', 'no_abstract'):
  file1: {'avg_rating': 3.0, 'paper_decision': 'Reject', 'confidence': ['4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.\n\n', '4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.\n\n', '5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.\n\n', '5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.\n\n'], 'strength': ['The paper is well-written and easy to follow. The motivation is clear and the proposed method is well-motivated. The experiments are extensive and the results are promising.\n\n', '1. The paper is well-written and easy to follow.\n2. The proposed method is novel and effective in capturing muscle movement features and facial position information.\n\n', '1. The proposed method is novel and effective in capturing muscle movement features and facial position information.\n\n', 'The proposed method is novel and effective in capturing muscle movement features and facial position information.\n\n'], 'weaknesses': ['The main weakness is the novelty of the proposed method. The proposed CVA module is similar to the coordinate attention module (CA) in MMNet, and the FPF module is similar to the ViT used in MMNet. The only difference is the use of Swin Transformer instead of ViT. The use of Swin Transformer is not well-motivated and the experiments are not sufficient to support this choice.\n\n', '1. The authors claim that the proposed CVA block is superior to the coordinate attention module (CA) in MMNet, but they do not provide a direct comparison between the two. It would be more convincing if the authors could compare the CVA block with the CA block in the same architecture.\n2. The paper lacks a detailed discussion of the limitations of the proposed method. It would be helpful to discuss potential limitations and future directions for improvement.\n3. The paper does not provide a clear explanation of the experimental results. For example, why does the F1-score of the proposed method on the CASME II dataset exceed 1?\n4. The paper does not provide a clear explanation of the experimental results. For example, why does the F1-score of the proposed method on the CASME II dataset exceed 1?\n\n', '1. The novelty of the proposed method is limited. The proposed method is similar to the MMNet, which also extracts the motion information between the onset and apex frames. The only difference is that this paper uses Swin Transformer to extract facial features and focus on some important areas, and the proposed CVA module is similar to the coordinate attention module (CA) in MMNet. \n2. The motivation of the proposed method is not clear. The authors claim that the model might learn identity information instead of the feature itself during training, but it is not clear how the identity information affects the performance of micro-expression recognition. \n3. The experimental results are not convincing. The proposed method is compared with the SOTA methods on two datasets, but the improvements are not significant. The proposed method achieves 94.35% accuracy and 0.9402 F1-score on CASME II dataset, but the results are not significantly better than the state-of-the-art method, which achieves 88.35% accuracy and 0.8676 F1-score. \n4. The ablation study is not comprehensive. The authors only show the results of each module, but do not show the results of the combination of multiple modules. \n5. The experiments are not sufficient. The authors only conduct experiments on two datasets, but do not show the results on other datasets. \n6. The paper does not provide a clear explanation of the experimental results. For example, why does the F1-score of the proposed method on the CASME II dataset exceed 1?\n\n', '1. The novelty of the proposed method is limited. The proposed method is similar to the MMNet, which also extracts the motion information between the onset and apex frames. The only difference is that this paper uses Swin Transformer to extract facial features and focus on some important areas, and the proposed CVA module is similar to the coordinate attention module (CA) in MMNet. \n2. The motivation of the proposed method is not clear. The authors claim that the model might learn identity information instead of the feature itself during training, but it is not clear how the identity information affects the performance of micro-expression recognition. \n3. The experimental results are not convincing. The proposed method is compared with the SOTA methods on two datasets, but the improvements are not significant. The proposed method achieves 94.35% accuracy and 0.9402 F1-score on CASME II dataset, but the results are not significantly better than the state-of-the-art method, which achieves 88.35% accuracy and 0.8676 F1-score. \n4. The ablation study is not comprehensive. The authors only show the results of each module, but do not show the results of the combination of multiple modules. \n5. The experiments are not sufficient. The authors only conduct experiments on two datasets, but do not show the results on other datasets. \n6. The paper does not provide a clear explanation of the experimental results. For example, why does the F1-score of the proposed method on the CASME II dataset exceed 1?\n\n'], 'meta_review': 'This paper proposes a new micro-expression recognition model based on continuous spatio-temporal attention blocks. The model consists of two streams, the main stream and the auxiliary stream. The main stream uses the CVA block to extract the spatio-temporal information of muscle motion. The sub-stream takes the starting frame as input information, and through the FPF module, it locates the facial position information in space, pays attention to the local facial feature changes, and finally inputs the fusion information into the classifier to classify the micro-expressions. Experiments on CASME II and SAMM datasets show the effectiveness of the proposed method.\n\n### justification_for_why_not_higher_score\n\nThe reviewers all gave the score of 3, and no rebuttal was provided.\n\n### justification_for_why_not_lower_score\n\nN/A\n\n', 'originality': 0, 'quality': 0, 'clarity': 0, 'significance': 0}
  file2: {'avg_rating': 3.0, 'paper_decision': 'Reject', 'confidence': ['5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.\n\n', '5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.\n\n', '5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.\n\n', '5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.\n\n'], 'strength': ['The paper is well-written and easy to follow. The proposed method is well-motivated and the design of the method is clear.\n\n', 'The paper is well-written and easy to follow. The proposed method is well-motivated and the design of the method is clear.\n\n', 'The paper is well-written and easy to follow. The proposed method is well-motivated and the design of the method is clear.\n\n', '1. The proposed method is based on the existing method MMNet, and the authors have improved it, which is a good way to improve the existing method.\n2. The proposed method achieves good performance on the CASME II and SAMM datasets.\n\n'], 'weaknesses': ['1. The paper lacks novelty. The proposed method is a combination of existing methods and does not introduce any new techniques. The method uses the same Swin Transformer encoder as MMNet, and the CVA block is similar to the attention module in MMNet. The FPF module is also similar to the position embedding module in MMNet. The AU embeddings are also used in previous works. The method does not introduce any new techniques for micro-expression recognition.\n\n2. The paper lacks ablation studies. The paper does not provide ablation studies on the design choices of the method. For example, the paper does not provide ablation studies on the number of CVA blocks, the number of Swin Transformer encoder blocks, the depths and number of heads per stage, etc. The paper only provides ablation studies on the AU embeddings, CVA block, and FPF block.\n\n3. The paper lacks comparisons with other methods. The paper only compares the proposed method with MMNet and does not compare with other state-of-the-art methods. The paper should compare with more recent methods such as [1,2,3].\n\n[1] Micro-expression recognition using multi-scale convolutional neural networks, IEEE Transactions on Affective Computing, 2023.\n\n[2] Micro-expression recognition based on a novel attention mechanism and patch of interest, IEEE Transactions on Affective Computing, 2023.\n\n[3] Micro-expression recognition using a novel multi-modal fusion framework, IEEE Transactions on Affective Computing, 2023.\n\n', '1. The paper lacks novelty. The proposed method is a combination of existing methods and does not introduce any new techniques. The method uses the same Swin Transformer encoder as MMNet, and the CVA block is similar to the attention module in MMNet. The FPF module is also similar to the position embedding module in MMNet. The AU embeddings are also used in previous works. The method does not introduce any new techniques for micro-expression recognition.\n\n2. The paper lacks ablation studies. The paper does not provide ablation studies on the design choices of the method. For example, the paper does not provide ablation studies on the number of CVA blocks, the number of Swin Transformer encoder blocks, the depths and number of heads per stage, etc. The paper only provides ablation studies on the AU embeddings, CVA block, and FPF block.\n\n3. The paper lacks comparisons with other methods. The paper only compares the proposed method with MMNet and does not compare with other state-of-the-art methods. The paper should compare with more recent methods such as [1,2,3].\n\n[1] Micro-expression recognition using multi-scale convolutional neural networks, IEEE Transactions on Affective Computing, 2023.\n\n[2] Micro-expression recognition based on a novel attention mechanism and patch of interest, IEEE Transactions on Affective Computing, 2023.\n\n[3] Micro-expression recognition using a novel multi-modal fusion framework, IEEE Transactions to Affective Computing, 2023.\n\n', 'The paper lacks novelty. The proposed method is a combination of existing methods and does not introduce any new techniques. The method uses the same Swin Transformer encoder as MMNet, and the CVA block is similar to the attention module in MMNet. The FPF module is also similar to the position embedding module in MMNet. The AU embeddings are also used in previous works. The method does not introduce any new techniques for micro-expression recognition.\n\nThe paper lacks ablation studies. The paper does not provide ablation studies on the design choices of the method. For example, the paper does not provide ablation studies on the number of CVA blocks, the number of Swin Transformer encoder blocks, the depths and number of heads per stage, etc. The paper only provides ablation studies on the AU embeddings, CVA block, and FPF block.\n\nThe paper lacks comparisons with other methods. The paper only compares the proposed method with MMNet and does not compare with other state-of-the-art methods. The paper should compare with more recent methods such as [1,2,3].\n\n[1] Micro-expression recognition using multi-scale convolutional neural networks, IEEE Transactions on Affective Computing, 2023.\n\n[2] Micro-expression recognition based on a novel attention mechanism and patch of interest, IEEE Transactions on Affective Computing, 2023.\n\n[3] Micro-expression recognition using a novel multi-modal fusion framework, IEEE Transactions to Affective Computing, 2023.\n\n', '1. The paper lacks novelty. The proposed method is a combination of existing methods and does not introduce any new techniques. The method uses the same Swin Transformer encoder as MMNet, and the CVA block is similar to the attention module in MMNet. The FPF module is also similar to the position embedding module in MMNet. The AU embeddings are also used in previous works. The method does not introduce any new techniques for micro-expression recognition.\n2. The paper lacks ablation studies. The paper does not provide ablation studies on the design choices of the method. For example, the paper does not provide ablation studies on the number of CVA blocks, the number of Swin Transformer encoder blocks, the depths and number of heads per stage, etc. The paper only provides ablation studies on the AU embeddings, CVA block, and FPF block.\n3. The paper lacks comparisons with other methods. The paper only compares the proposed method with MMNet and does not compare with other state-of-the-art methods. The paper should compare with more recent methods such as [1,2,3].\n\n[1] Micro-expression recognition using multi-scale convolutional neural networks, IEEE Transactions on Affective Computing, 2023.\n\n[2] Micro-expression recognition based on a novel attention mechanism and patch of interest, IEEE Transactions on Affective Computing, 2023.\n\n[3] Micro-expression recognition using a novel multi-modal fusion framework, IEEE Transactions to Affective Computing, 2023.\n\n'], 'meta_review': 'This paper proposes a new method for micro-expression recognition. The method consists of three parts: preprocessing, feature extraction, and classification. In the feature extraction part, the method uses two streams: Facial Feature Extraction and Temporal Motion Feature Extraction. In Facial Feature Extraction, the method extracts information from the onset frame and the apex frame separately by a Swin Transformer encoder and then adds them together. In Temporal Motion Feature Extraction, the method subtracts the onset frame from the apex frame before inputting the result into the continuous vertical attention (CVA) block. The CVA block takes the motion difference information obtained by the above method as input and leverages the continuous spatio-temporal attention model to extract the spatio-temporal information of muscle motion. The method also uses a Facial Position Focalizer (FPF) module to incorporate facial position information using Swin Transformer. The method also uses AU embeddings to assist the network in focusing on active facial regions. The method is evaluated on the CASME II and SAMM datasets and achieves state-of-the-art performance.\n\nThe paper is well-written and easy to follow. The proposed method is well-motivated and the design of the method is clear. However, the paper lacks novelty. The proposed method is a combination of existing methods and does not introduce any new techniques. The method uses the same Swin Transformer encoder as MMNet, and the CVA block is similar to the attention module in MMNet. The FPF module is also similar to the position embedding module in MMNet. The AU embeddings are also used in previous works. The method does not introduce any new techniques for micro-expression recognition.\n\nThe paper lacks ablation studies. The paper does not provide ablation studies on the design choices of the method. For example, the paper does not provide ablation studies on the number of CVA blocks, the number of Swin Transformer encoder blocks, the depths and number of heads per stage, etc. The paper only provides ablation studies on the AU embeddings, CVA block, and FPF block.\n\nThe paper lacks comparisons with other methods. The paper only compares the proposed method with MMNet and does not compare with other state-of-the-art methods. The paper should compare with more recent methods such as [1,2,3].\n\n[1] Micro-expression recognition using multi-scale convolutional neural networks, IEEE Transactions on Affective Computing, 2023.\n\n[2] Micro-expression recognition based on a novel attention mechanism and patch of interest, IEEE Transactions on Affective Computing, 2023.\n\n[3] Micro-expression recognition using a novel multi-modal fusion framework, IEEE Transactions to Affective Computing, 2023.\n\n### justification_for_why_not_higher_score\n\nThe paper lacks novelty. The proposed method is a combination of existing methods and does not introduce any new techniques. The method uses the same Swin Transformer encoder as MMNet, and the CVA block is similar to the attention module in MMNet. The FPF module is also similar to the position embedding module in MMNet. The AU embeddings are also used in previous works. The method does not introduce any new techniques for micro-expression recognition.\n\nThe paper lacks ablation studies. The paper does not provide ablation studies on the design choices of the method. For example, the paper does not provide ablation studies on the number of CVA blocks, the number of Swin Transformer encoder blocks, the depths and number of heads per stage, etc. The paper only provides ablation studies on the AU embeddings, CVA block, and FPF block.\n\nThe paper lacks comparisons with other methods. The paper only compares the proposed method with MMNet and does not compare with other state-of-the-art methods. The paper should compare with more recent methods such as [1,2,3].\n\n[1] Micro-expression recognition using multi-scale convolutional neural networks, IEEE Transactions on Affective Computing, 2023.\n\n[2] Micro-expression recognition based on a novel attention mechanism and patch of interest, IEEE Transactions on Affective Computing, 2023.\n\n[3] Micro-expression recognition using a novel multi-modal fusion framework, IEEE Transactions to Affective Computing, 2023.\n\n### justification_for_why_not_lower_score\n\nN/A\n\n', 'originality': 0, 'quality': 0, 'clarity': 0, 'significance': 0}
('mHYkcQzdae', 'no_introduction'):
  file1: {'avg_rating': 4.5, 'paper_decision': 'Reject', 'confidence': ['4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.\n\n', '4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.\n\n', '4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.\n\n', '3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.\n\n'], 'strength': ['1. The proposed model is simple and effective.\n2. The model achieves state-of-the-art performance on the CASME II and SAMM micro-expression datasets.\n\n', '1. The paper is well-written and easy to follow.\n2. The proposed method is simple and effective, achieving state-of-the-art performance on the CASME II and SAMM micro-expression datasets.\n3. The proposed method is well-motivated and the technical details are clearly explained.\n\n', '1. The paper is well-written and easy to follow.\n2. The proposed method is simple and effective, achieving state-of-the-art performance on the CASME II and SAMM micro-expression datasets.\n3. The proposed method is well-motivated and the technical details are clearly explained.\n\n', '1. The proposed method is simple and effective, achieving state-of-the-art performance on the CASME II and SAMM micro-expression datasets.\n2. The proposed method is well-motivated and the technical details are clearly explained.\n3. The paper is well-written and easy to follow.\n\n'], 'weaknesses': ['1. The novelty of the proposed method is limited. The proposed CVA block and FPF module are both inspired by existing works. The proposed model is a combination of existing techniques.\n2. The proposed method is not compared with the latest SOTA methods. The latest method is published in 2023.\n3. The experiments are not sufficient. The proposed method is not compared with the latest SOTA methods. The latest method is published in 2023.\n4. The paper writing needs to be improved. The paper is hard to follow. The presentation is not clear.\n\n', '1. The paper does not provide a detailed analysis of the computational complexity of the proposed method.\n2. The paper does not provide a detailed analysis of the generalizability of the proposed method to other datasets.\n\n', '1. The paper does not provide a detailed analysis of the computational complexity of the proposed method.\n2. The paper does not provide a detailed analysis of the generalizability of the proposed method to other datasets.\n3. The proposed method is not compared with the latest SOTA methods.\n\n', '1. The novelty of the proposed method is limited. The proposed CVA block and FPF module are both inspired by existing works. The proposed model is a combination of existing techniques.\n2. The proposed method is not compared with the latest SOTA methods. The latest method is published in 2023.\n3. The experiments are not sufficient. The proposed method is not compared with the latest SOTA methods. The latest method is published in 2023.\n4. The paper writing needs to be improved. The paper is hard to follow. The presentation is not clear.\n\n'], 'meta_review': "The paper proposes a new method for micro-expression recognition (MER), which is a challenging task due to the short duration and partial muscle movements involved. The proposed method incorporates several techniques to address these challenges: a CVA (Continuously Vertical Attention) block to model local muscle changes, a Facial Position Focalizer (FPF) module based on Swin Transformer to incorporate spatial information, and AU (Action Units) information to enhance accuracy. The proposed model achieves state-of-the-art performance on the CASME II and SAMM micro-expression datasets.\n\nThe reviewers raised several concerns regarding the novelty of the proposed method, the lack of comparison with the latest SOTA methods, and the insufficient experiments. The authors provided a response to these concerns, but the reviewers still maintained their initial ratings. The AC agrees with the reviewers' concerns and recommends rejecting the paper.\n\n### justification_for_why_not_higher_score\n\nThe reviewers raised several concerns regarding the novelty of the proposed method, the lack of comparison with the latest SOTA methods, and the insufficient experiments. The authors provided a response to these concerns, but the reviewers still maintained their initial ratings. The AC agrees with the reviewers' concerns and recommends rejecting the paper.\n\n### justification_for_why_not_lower_score\n\nN/A\n\n", 'originality': 0, 'quality': 0, 'clarity': 0, 'significance': 0}
  file2: {'avg_rating': 4.5, 'paper_decision': 'Reject', 'confidence': ['5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.\n\n', '3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.\n\n', '4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.\n\n', '3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.\n\n'], 'strength': ['The paper is well-written and easy to follow. The proposed method is simple yet effective.\n\n', '1. The proposed method is novel and effective. The use of CVA block and FPF module is innovative and shows significant improvement over existing methods.\n2. The paper is well-written and easy to follow. The authors provide a clear explanation of the proposed method and its components.\n3. The experimental results are convincing. The proposed method achieves state-of-the-art performance on two popular micro-expression datasets, CASME II and SAMM.\n\n', '1. The paper is well-written and easy to follow.\n2. The proposed method achieves state-of-the-art performance on two popular micro-expression datasets.\n3. The paper provides a detailed analysis of the results, including ablation studies and comparisons with other methods.\n\n', '1.\tThe proposed method is novel and effective. The use of CVA block and FPF module is innovative and shows significant improvement over existing methods.\n2.\tThe paper is well-written and easy to follow. The authors provide a clear explanation of the proposed method and its components.\n3.\tThe experimental results are convincing. The proposed method achieves state-of-the-art performance on two popular micro-expression datasets, CASME II and SAMM.\n\n'], 'weaknesses': ['1. The novelty of the proposed method is limited. The proposed method is a combination of existing methods, which is not very novel. The proposed CVA block is similar to the coordinate attention module. The facial position focalizer is also similar to the ViT. The AU embedding is also widely used in the micro-expression recognition field. \n\n2. The experimental results are not convincing. The authors only compare the proposed method with some old methods. The authors should compare the proposed method with some recent methods, such as [1, 2, 3]. \n\n3. The proposed method is not compared with some baselines. The authors should compare the proposed method with some baselines, such as ResNet, ViT, Swin Transformer, etc.\n\n[1] Micro-Expression Recognition with Diagonal Micro-Attention and Patch of Interest. In ICCV, 2023.\n\n[2] Micro-Expression Recognition with Multi-Scale Feature Aggregation and Attention Mechanism. In ICME, 2022.\n\n[3] Micro-Expression Recognition with a Novel Attention-based Multi-scale Fusion Network. In ICME, 2023.\n\n', '1. The paper lacks a detailed comparison with other methods. The authors should compare the proposed method with more recent methods in the literature.\n2. The paper lacks a detailed analysis of the results. The authors should provide more analysis of the results, such as the effect of different components of the proposed method.\n\n', '1. The novelty of the proposed method is limited. The proposed CVA block is similar to the coordinate attention module. The facial position focalizer is also similar to the ViT. The AU embedding is also widely used in the micro-expression recognition field. \n2. The experimental results are not convincing. The authors only compare the proposed method with some old methods. The authors should compare the proposed method with some recent methods, such as [1, 2, 3]. \n3. The paper lacks a detailed comparison with other methods. The authors should compare the proposed method with more recent methods in the literature.\n4. The paper lacks a detailed analysis of the results. The authors should provide more analysis of the results, such as the effect of different components of the proposed method.\n\n[1] Micro-Expression Recognition with Diagonal Micro-Attention and Patch of Interest. In ICCV, 2023.\n\n[2] Micro-Expression Recognition with Multi-Scale Feature Aggregation and Attention Mechanism. In ICME, 2022.\n\n[3] Micro-Expression Recognition with a Novel Attention-based Multi-scale Fusion Network. In ICME, 2023.\n\n', '1.\tThe novelty of the proposed method is limited. The proposed CVA block is similar to the coordinate attention module. The facial position focalizer is also similar to the ViT. The AU embedding is also widely used in the micro-expression recognition field. \n2.\tThe experimental results are not convincing. The authors only compare the proposed method with some old methods. The authors should compare the proposed method with some recent methods, such as [1, 2, 3]. \n3.\tThe paper lacks a detailed comparison with other methods. The authors should compare the proposed method with more recent methods in the literature.\n4.\tThe paper lacks a detailed analysis of the results. The authors should provide more analysis of the results, such as the effect of different components of the proposed method.\n\n[1] Micro-Expression Recognition with Diagonal Micro-Attention and Patch of Interest. In ICCV, 2023.\n\n[2] Micro-Expression Recognition with Multi-Scale Feature Aggregation and Attention Mechanism. In ICME, 2022.\n\n[3] Micro-Expression Recognition with a Novel Attention-based Multi-scale Fusion Network. In ICME, 2023.\n\n'], 'meta_review': 'The paper proposes a new method for micro-expression recognition, which is a challenging task due to the short duration and partial muscle movements involved. The proposed method uses a CVA block to model local muscle changes and a facial position localization module called FPF based on Swin Transformer to incorporate spatial information. The authors also show that including Action Units (AUs) can further improve accuracy, and they incorporate AU information to assist in micro-expression recognition. The proposed method achieves state-of-the-art performance on two popular micro-expression datasets, CASME II and SAMM.\n\nThe reviewers have raised several concerns, including the limited novelty of the proposed method, the lack of comparison with recent methods, and the lack of detailed analysis of the results. The authors have provided some responses to these concerns, but the reviewers remain unconvinced about the significance and novelty of the proposed method. The paper is also not well written and lacks a detailed comparison with other methods. The experimental results are not convincing and the paper lacks a detailed analysis of the results.\n\n### justification_for_why_not_higher_score\n\nThe reviewers have raised several concerns, including the limited novelty of the proposed method, the lack of comparison with recent methods, and the lack of detailed analysis of the results. The authors have provided some responses to these concerns, but the reviewers remain unconvinced about the significance and novelty of the proposed method. The paper is also not well written and lacks a detailed comparison with other methods. The experimental results are not convincing and the paper lacks a detailed analysis of the results.\n\n### justification_for_why_not_lower_score\n\nN/A\n\n', 'originality': 0, 'quality': 0, 'clarity': 0, 'significance': 0}
('iNtEAeVQE0', 'no_experiments'):
  file1: {'avg_rating': 4.0, 'paper_decision': 'Reject', 'confidence': ['4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.\n\n', '4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.\n\n', '4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.\n\n', '3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.\n\n'], 'strength': ['The paper is well-written and easy to follow. The proposed method is simple yet effective.\n\n', '1. The paper is well-written and easy to follow.\n2. The proposed method is simple and easy to understand.\n3. The experimental results show that the proposed method can achieve good performance on some datasets.\n\n', '1. The proposed method is simple and effective.\n2. The proposed method is general and can be applied to different datasets.\n3. The paper is well-written and easy to follow.\n\n', '1. The paper is well-written and easy to follow.\n2. The proposed method is simple and easy to understand.\n3. The experimental results show that the proposed method can achieve good performance on some datasets.\n\n'], 'weaknesses': ['The proposed method is not novel. The method of using a spurious classifier to detect spurious features has been used in many previous works. The only difference is that the authors use KL divergence to maximize the discrepancy between the correlations in the training and validation set. However, the authors do not provide any theoretical justification for this objective function. \n\nThe experiments are not comprehensive. The authors only evaluate the method on a few datasets. More experiments are needed to show the effectiveness of the proposed method.\n\n', '1. The proposed method is not novel. The idea of using spurious features to infer domain information has been explored in previous works, such as ZIN and DISC.\n2. The experimental results are not comprehensive. The authors only evaluate the method on a few datasets and do not compare it with other state-of-the-art methods.\n3. The method requires a validation set to train the spurious classifier, which may not be practical in real-world scenarios.\n\n', '1. The proposed method is not novel. The idea of using a spurious classifier to predict the spurious labels has been explored in previous works, such as ZIN and DISC. The proposed method is similar to DISC, but with a different objective function.\n2. The experiments are not comprehensive. The authors only evaluate the method on a few datasets and do not compare it with other state-of-the-art methods.\n3. The proposed method requires a validation set to train the spurious classifier, which may not be practical in real-world scenarios.\n\n', '1. The proposed method is not novel. The idea of using spurious features to infer domain information has been explored in previous works, such as ZIN and DISC.\n2. The experimental results are not comprehensive. The authors only evaluate the method on a few datasets and do not compare it with other state-of-the-art methods.\n3. The method requires a validation set to train the spurious classifier, which may not be practical in real-world scenarios.\n\n'], 'meta_review': 'The paper proposes a method to infer domain labels from spurious features. The reviewers have raised several concerns about the novelty of the method, the experimental evaluation, and the practicality of the method. The authors have not provided a rebuttal. The AC agrees with the reviewers that the paper is not ready for publication at the current stage.\n\n### justification_for_why_not_higher_score\n\nThe paper is not ready for publication at the current stage.\n\n### justification_for_why_not_lower_score\n\nN/A\n\n', 'originality': 0, 'quality': 0, 'clarity': 0, 'significance': 0}
  file2: {'avg_rating': 4.0, 'paper_decision': 'Reject', 'confidence': ['3: You are fairly confident in your assessment.\n\n', '3: You are fairly confident in your assessment.\n\n', '3: You are fairly confident in your assessment.\n\n', '3: You are fairly confident in your assessment.\n\n'], 'strength': ['The paper is well-written and easy to follow. The authors provide a clear motivation for their proposed method and explain the intuition behind the design choices. The proposed method is simple and effective, and the authors provide a theoretical justification for their approach.\n\n', '1. The proposed method is simple and easy to understand.\n2. The method is effective on some datasets.\n\n', '1. The paper is well-written and easy to follow.\n2. The proposed method is novel and effective in addressing the subpopulation shift problem.\n\n', '1. The paper is well-written and easy to follow.\n2. The proposed method is novel and effective in addressing the subpopulation shift problem.\n\n'], 'weaknesses': ['1. The proposed method is not novel. The idea of using spurious features to infer domain information has been explored in previous works, such as [1, 2]. The proposed method seems to be a combination of these previous works, and the authors do not provide a clear explanation of the novelty of their approach.\n2. The experiments are not sufficient to support the claims made in the paper. The authors only conduct experiments on a limited number of datasets and do not compare their method with a wide range of baselines. Additionally, the authors do not provide a detailed analysis of the results, making it difficult to understand the effectiveness of their method.\n3. The authors do not provide a clear explanation of how the proposed method can be used to mitigate subpopulation shift. The authors only provide a high-level description of how the method can be used to improve generalization, but do not provide a detailed explanation of the underlying mechanisms.\n\n[1] Lin, Y., Liu, Y., Zhang, Y., & Li, Z. (2022). Zin: Zero-invariant learning for domain generalization. Advances in Neural Information Processing Systems, 35, 25129-25143.\n[2] Wu, C., Li, Y., Wu, Y., & Sun, S. (2023). DISC: Domain Inference for Spurious Correlation. arXiv preprint arXiv:2304.08419.\n\n', '1. The proposed method is not novel. The idea of using spurious features to infer domain information has been explored in previous works, such as [1, 2]. The proposed method seems to be a combination of these previous works, and the authors do not provide a clear explanation of the novelty of their approach.\n2. The experiments are not sufficient to support the claims made in the paper. The authors only conduct experiments on a limited number of datasets and do not compare their method with a wide range of baselines. Additionally, the authors do not provide a detailed analysis of the results, making it difficult to understand the effectiveness of their method.\n3. The authors do not provide a clear explanation of how the proposed method can be used to mitigate subpopulation shift. The authors only provide a high-level description of how the method can be used to improve generalization, but do not provide a detailed explanation of the underlying mechanisms.\n4. The authors do not provide a clear explanation of how the proposed method can be used to improve generalization. The authors only provide a high-level description of how the method can be used to improve generalization, but do not provide a detailed explanation of the underlying mechanisms.\n5. The authors do not provide a clear explanation of how the proposed method can be used to improve generalization in the test domain. The authors only provide a high-level description of how the method can be used to improve generalization, but do not provide a detailed explanation of the underlying mechanisms.\n\n[1] Lin, Y., Liu, Y., Zhang, Y., & Li, Z. (2022). Zin: Zero-invariant learning for domain generalization. Advances in Neural Information Processing Systems, 35, 25129-25143.\n[2] Wu, C., Li, Y., Wu, Y., & Sun,  S. (2023). DISC: Domain Inference for Spurious Correlation. arXiv preprint arXiv:2304.08419.\n\n', '1. The paper lacks a clear definition of the subpopulation shift problem and its importance in machine learning.\n2. The paper does not provide a detailed explanation of the experimental results and how they support the claims made.\n3. The paper does not compare the proposed method with other state-of-the-art methods for addressing the subpopulation shift problem.\n\n', '1. The paper lacks a clear definition of the subpopulation shift problem and its importance in machine learning.\n2. The paper does not provide a detailed explanation of the experimental results and how they support the claims made.\n3. The paper does not compare the proposed method with other state-of-the-art methods for addressing the subpopulation shift problem.\n\n'], 'meta_review': 'The paper proposes a method for inferring spurious correlations (or domain information) without the need for any domain information. The method is based on the idea of maximizing the correlation between the true label and spurious label in the training set, and minimizing the correlation between the true label and spurious label in the validation set. The authors provide a lower bound for the spurious term when the validation labels are unavailable. The authors conduct experiments on various datasets to demonstrate the effectiveness of DISK in domain inference and mitigating subpopulation shift.\n\nStrengths: The paper is well-written and easy to follow. The proposed method is simple and easy to understand.\n\nWeaknesses: The proposed method is not novel. The idea of using spurious features to infer domain information has been explored in previous works, such as [1, 2]. The proposed method seems to be a combination of these previous works, and the authors do not provide a clear explanation of the novelty of their approach. The experiments are not sufficient to support the claims made in the paper. The authors only conduct experiments on a limited number of datasets and do not compare their method with a wide range of baselines. Additionally, the authors do not provide a detailed analysis of the results, making it difficult to understand the effectiveness of their method. The authors do not provide a clear explanation of how the proposed method can be used to mitigate subpopulation shift. The authors only provide a high-level description of how the method can be used to improve generalization, but do not provide a detailed explanation of the underlying mechanisms.\n\n### justification_for_why_not_higher_score\n\nThe proposed method is not novel. The idea of using spurious features to infer domain information has been explored in previous works, such as [1, 2]. The proposed method seems to be a combination of these previous works, and the authors do not provide a clear explanation of the novelty of their approach. The experiments are not sufficient to support the claims made in the paper. The authors only conduct experiments on a limited number of datasets and do not compare their method with a wide range of baselines. Additionally, the authors do not provide a detailed analysis of the results, making it difficult to understand the effectiveness of their method. The authors do not provide a clear explanation of how the proposed method can be used to mitigate subpopulation shift. The authors only provide a high-level description of how the method can be used to improve generalization, but do not provide a detailed explanation of the underlying mechanisms.\n\n### justification_for_why_not_lower_score\n\nN/A\n\n', 'originality': 0, 'quality': 0, 'clarity': 0, 'significance': 0}
共: 4
